{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372a8b17-f985-4e88-a734-0d11b6a29312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-10 18:27:18.987214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 18:27:20.152692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import KFold,GroupKFold,StratifiedKFold,StratifiedGroupKFold\n",
    "import warnings\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7806a91-2936-4a9e-a7f2-c1257fbb41fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CATS = ['activations', 'text_change']\n",
    "NUMS = ['action_time', 'cursor_position', 'word_count', 'elapsed_time_diff_1', \"down_time\" ]\n",
    "\n",
    "NUMS_1 = [ \"elapsed_time_diff_2\",\n",
    "            \"elapsed_time_diff_3\",\n",
    "            \"elapsed_time_diff_5\",\n",
    "            \"elapsed_time_diff_10\",\n",
    "            \"elapsed_time_diff_20\",\n",
    "            \"elapsed_time_diff_50\",\n",
    "            \"cursor_position_diff_1\",\n",
    "            \"cursor_position_diff_2\",\n",
    "            \"cursor_position_diff_3\",\n",
    "            \"cursor_position_diff_5\",\n",
    "            \"cursor_position_diff_10\",\n",
    "            \"cursor_position_diff_20\",\n",
    "            \"cursor_position_diff_50\",\n",
    "            \"word_count_diff_1\",\n",
    "            \"word_count_diff_2\",\n",
    "            \"word_count_diff_3\",\n",
    "            \"word_count_diff_5\",\n",
    "            \"word_count_diff_10\",\n",
    "            \"word_count_diff_20\",\n",
    "            \"word_count_diff_50\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "NUMS += NUMS_1\n",
    "name_feature = ['Nonproduction',\n",
    "                 'Input',\n",
    "                 'Remove/Cut',\n",
    "                 'Replace',\n",
    "                 'Move',\n",
    "                 'Paste',\n",
    "               ]\n",
    "\n",
    "text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "\n",
    "down_event_feature = [\n",
    "            'Dead','F3','{','Meta','F15','ArrowDown','z','F6','Pause','S','\\x80',')','u','Cancel','Backspace','End','¡','MediaTrackNext','/','Tab','C','i','Process','F11','Home','~','%','1','q','F','s','5','Clear','l','OS','h','o','.','2','n','Control','*','Escape','#','`','â\\x80\\x93','Â´','MediaTrackPrevious','Alt','Ä±','a','@','PageUp','A','NumLock','ModeChange',',','^','PageDown','ContextMenu','F12','F2','Unknownclick','b','\\x9b','-','\"',\"'\",';','Delete','F1','$','T','0','CapsLock','M','ArrowRight','Ë\\x86','AudioVolumeDown','?','p','Insert','\\x96','y','w','AudioVolumeUp','Enter','Leftclick','V','¿','MediaPlayPause','}','AltGraph','_','I',':','AudioVolumeMute','Rightclick','>','ArrowLeft','c','Middleclick','(','ScrollLock','r','ArrowUp','Shift','Unidentified','&','|','g','!','v','F10','x','+','=','j','t','d','e','\\x97','Space','Å\\x9f','m','f','\\\\','ä'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "down_event_feature = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "pasue_gaps = [8]\n",
    "\n",
    "up_event_feature = [\n",
    "    'AudioVolumeDown','h','MediaTrackNext','ArrowRight','l','Ë\\x86','S',\"'\",'f','Â´','>','F12','~',']','Space','Escape','r','o','F10','Leftclick','[','g','e','=','¿','T','-','MediaPlayPause','k','Backspace','Enter','C','{','i','AudioVolumeUp','MediaTrackPrevious','<','`','F15','q','F1','Å\\x9f','Delete','AltGraph','(','^','Process','0','CapsLock',')','Alt','ArrowUp','ScrollLock','Clear','c','ModeChange','_','|','ArrowDown','Ä±','+','PageUp','\\x80','NumLock','\"','Middleclick','M','Cancel','#',':','â\\x80\\x93','a','ä','Home','\\x9b','1','V','Unidentified','b','\\\\','Insert','Shift','Rightclick','\\x97','?','F11','5','Unknownclick','2','n','\\x96','%','t','$','j','ContextMenu','p','y','End','d','@','m','v','Pause','.','!','OS','ArrowLeft','PageDown','s','Control','F3','}','F6','w','&','Dead','¡','u','x','A','Meta',',','z','/','*',';','Tab','AudioVolumeMute','F2'\n",
    "    ]\n",
    "\n",
    "up_event_feature = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373df3a2-f1a0-44f4-9162-f2402b1a965f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_act(act_name):\n",
    "    if act_name.startswith(\"Move\"):\n",
    "        return \"Move\"\n",
    "    else:\n",
    "        return act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d47112-0cc6-4257-80a3-df4a4ef12f6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    pl.col(\"action_time\").cast(pl.Float32),\n",
    "    pl.col(\"cursor_position\").cast(pl.Float32),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(1)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff\")),\n",
    "    pl.col(\"word_count\").cast(pl.Float32),\n",
    "    pl.col(\"activity\").apply(rename_act).alias(\"activations\"),\n",
    "    \n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(1)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_1\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(2)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_2\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(3)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_3\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(5)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_5\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(10)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_10\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(20)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_20\")),\n",
    "    ((pl.col(\"down_time\") - pl.col(\"up_time\").shift(50)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"elapsed_time_diff_50\")),\n",
    "    \n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(1)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_1\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(2)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_2\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(3)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_3\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(5)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_5\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(10)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_10\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(20)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_20\")),\n",
    "    ((pl.col(\"cursor_position\") - pl.col(\"cursor_position\").shift(50)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"cursor_position_diff_50\")),\n",
    "\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(1)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_1\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(2)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_2\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(3)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_3\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(5)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_5\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(10)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_10\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(20)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_20\")),\n",
    "    ((pl.col(\"word_count\") - pl.col(\"word_count\").shift(50)).fill_null(0).clip(-1e9, 1e9).over(\n",
    "        [\"id\"]).alias(\"word_count_diff_50\")),\n",
    "    \n",
    "    (pl.col('word_count').max()/pl.col('action_time').max()/1000).alias('word_count_pre_step'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14e3fa4-75a9-48ac-ba93-a15257965a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (pl.read_csv(\"train_logs.csv\")\n",
    "      .with_columns(columns)\n",
    "      .drop([\"activity\"])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089ce2d-5e00-4ee2-aaf0-5652c68a8f83",
   "metadata": {},
   "source": [
    "## build text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f05f837-a95b-4a54-a990-c252d0022f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./xgb_5fold/pipeline.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays = pd.read_csv('train_essays_02.csv')\n",
    "train_essays['text'] = train_essays['essay']\n",
    "train_essays.index = train_essays[\"Unnamed: 0\"]\n",
    "train_essays.index.name = None\n",
    "train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_essays.head()\n",
    "corpus = train_essays['text'].tolist()\n",
    "pipe = Pipeline([('count', CountVectorizer(ngram_range=(1,2), max_features=128)),\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "text_feature = pipe.transform(corpus).toarray()\n",
    "joblib.dump(pipe, './xgb_5fold/pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b9db49-cb94-462f-9818-3e811931ec2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a692de0-a7a7-48a0-ad9c-f9fee89f9a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "\n",
    "def split_essays_into_sentences(df):\n",
    "    essay_df = df\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(df):\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(df):\n",
    "    essay_df = df\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(df):\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cdf9c36-8d31-480f-b601-be1754036a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sent_df = split_essays_into_sentences(train_essays)\n",
    "train_sent_agg_df = compute_sentence_aggregations(train_sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cccc68-341a-43dc-96fe-6857501c869b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_paragraph_df = split_essays_into_paragraphs(train_essays)\n",
    "train_paragraph_agg_df = compute_paragraph_aggregations(train_paragraph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7565fb-0b4e-4e11-bfc4-817181554386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_result = pd.read_csv('bert_predict.csv').set_index('id')[['bert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff76234e-bf05-4adf-a062-df2136b2c63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = joblib.load('./xgb_5fold/pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f1f7334-568a-4d6d-aebc-afb86427ae73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "\n",
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        #feature = F.normalize(feature, p=2, dim=1)\n",
    "        return feature\n",
    "\n",
    "    \n",
    "# model_name = './pretrained_model'\n",
    "# device = 'cuda'\n",
    "# model = CustomModel(model_name)\n",
    "# model.to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "       \n",
    "        inputs = tokenizer(text,\n",
    "                           max_length=256,\n",
    "                           pad_to_max_length=True,\n",
    "                           add_special_tokens=True,\n",
    "                           return_offsets_mapping=False)\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "def get_model_feature(model, texts):\n",
    "    feature_outs_all = []\n",
    "    test_dataset = TestDataset(texts)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest'),\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tqdm(test_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature_outs = model(inputs)\n",
    "            feature_outs_all.append(feature_outs.cpu())\n",
    "\n",
    "    feature_outs_all_final = torch.cat(feature_outs_all, dim=0).numpy()\n",
    "    #print(feature_outs_all_final.shape)\n",
    "\n",
    "    return feature_outs_all_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce6c8169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "\n",
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        #feature = F.normalize(feature, p=2, dim=1)\n",
    "        return feature\n",
    "\n",
    "    \n",
    "# model_name = './pretrained_model'\n",
    "# device = 'cuda'\n",
    "# model = CustomModel(model_name)\n",
    "# model.to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "       \n",
    "        inputs = tokenizer(text,\n",
    "                           max_length=256,\n",
    "                           pad_to_max_length=True,\n",
    "                           add_special_tokens=True,\n",
    "                           return_offsets_mapping=False)\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "def get_model_feature(model, texts):\n",
    "    feature_outs_all = []\n",
    "    test_dataset = TestDataset(texts)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=32,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest'),\n",
    "                             num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tqdm(test_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature_outs = model(inputs)\n",
    "            feature_outs_all.append(feature_outs.cpu())\n",
    "\n",
    "    feature_outs_all_final = torch.cat(feature_outs_all, dim=0).numpy()\n",
    "    #print(feature_outs_all_final.shape)\n",
    "\n",
    "    return feature_outs_all_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f39b57e-25a5-457b-86ff-ec488d85a09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = 1e-3\n",
    "def feature_engineer_for_index(x, feature_suffix):\n",
    "\n",
    "    aggs = [\n",
    "        (pl.col('word_count').max()/ pl.col(\"up_event\").filter(pl.col(\"up_event\") == '.').count()).clip(0, 500).alias('every_sentence_word_count'),\n",
    "        (pl.col('cursor_position').max()/ pl.col(\"word_count\").max()).alias('every_char_word'),\n",
    "        (pl.col('activations').filter(pl.col(\"activations\").is_in(['Remove/Cut', 'Replace'])).count()/ pl.col(\"up_event\").filter(pl.col(\"up_event\") == '.').count()).clip(0, 500).alias('every_sentence_change'),\n",
    "        ( pl.col(\"cursor_position\").max() / (pl.col('down_time').max() - pl.col('down_time').min()) * 1000 ).alias('char_per_min'),\n",
    "        ( pl.col(\"word_count\").max() / (pl.col('down_time').max() - pl.col('down_time').min()) * 1000 ).alias('word_per_min'),\n",
    "        \n",
    "        (pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"elapsed_time_diff_1\") > 8000 ).sum() / pl.col(\"elapsed_time_diff_1\").sum()).alias('proportion_of_pause_time'),\n",
    "        (pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"elapsed_time_diff_1\") > 8000 ).count()).alias('count_of_pause_time'),\n",
    "        (pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"elapsed_time_diff_1\") > 8000 ).sum() / pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"elapsed_time_diff_1\") > 10000 ).count()).alias('pause_length'),\n",
    "        (pl.col(\"elapsed_time_diff_1\").filter((pl.col(\"elapsed_time_diff_1\") > 8000) & (pl.col(\"text_change\") == ' ' )).count()).alias('freq_of_pause_time'),\n",
    "        #(pl.col(\"activations\").filter(pl.col(\"elapsed_time_diff_1\") > 2000 ).count() / pl.col(\"activations\").count()).alias('proportion_of_pause'),\n",
    "        # (pl.col('down_event').filter(pl.col('down_event').is_in(punctuations)).count() ).alias('punct_cnt'),\n",
    "        #*[(pl.col('text_change').filter(pl.col('text_change') == c).count() ).alias(f'text_change_{c}_cnt') for c in text_changes],\n",
    "        #pl.col('event_id').count().alias('event_count'),\n",
    "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in CATS],\n",
    "        *[pl.col(c).quantile(0.1, \"nearest\").alias(f\"{c}_quantile1_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.2, \"nearest\").alias(f\"{c}_quantile2_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.3, \"nearest\").alias(f\"{c}_quantile3_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.4, \"nearest\").alias(f\"{c}_quantile4_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.5, \"nearest\").alias(f\"{c}_quantile5_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.6, \"nearest\").alias(f\"{c}_quantile6_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.7, \"nearest\").alias(f\"{c}_quantile7_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.8, \"nearest\").alias(f\"{c}_quantile8_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).quantile(0.9, \"nearest\").alias(f\"{c}_quantile9_{feature_suffix}\") for c in NUMS],\n",
    "\n",
    "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).median().alias(f\"{c}_median_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).sum().alias(f\"{c}_sum_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).skew().alias(f\"{c}_skew_{feature_suffix}\") for c in NUMS],\n",
    "        *[pl.col(c).kurtosis().alias(f\"{c}_kurtosis_{feature_suffix}\") for c in NUMS],\n",
    "        # adding rank features\n",
    "        *[pl.col(c).last().alias(f\"{c}_rank_{feature_suffix}\") for c in NUMS],\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.1, \"nearest\").alias(f\"{c}_ET_quantile1_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.2, \"nearest\").alias(f\"{c}_ET_quantile2_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.3, \"nearest\").alias(f\"{c}_ET_quantile3_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.4, \"nearest\").alias(f\"{c}_ET_quantile4_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.5, \"nearest\").alias(f\"{c}_ET_quantile5_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.6, \"nearest\").alias(f\"{c}_ET_quantile6_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.7, \"nearest\").alias(f\"{c}_ET_quantile7_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.8, \"nearest\").alias(f\"{c}_ET_quantile8_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).quantile(0.9, \"nearest\").alias(f\"{c}_ET_quantile9_{feature_suffix}\") for c in name_feature],\n",
    "        \n",
    "        *[pl.col(\"activations\").filter(pl.col(\"activations\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\")for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).min().alias(f\"{c}_ET_min_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"activations\")==c).last().alias(f\"{c}_ET_rank_{feature_suffix}\") for c in name_feature],\n",
    "        #*[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"name\")==c).mode().alias(f\"{c}_ET_mode_{feature_suffix}\") for c in name_feature],\n",
    "        \n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.1, \"nearest\").alias(f\"{c}_ETD_quantile1_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.2, \"nearest\").alias(f\"{c}_ETD_quantile2_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.3, \"nearest\").alias(f\"{c}_ETD_quantile3_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.4, \"nearest\").alias(f\"{c}_ETD_quantile4_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.5, \"nearest\").alias(f\"{c}_ETD_quantile5_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.6, \"nearest\").alias(f\"{c}_ETD_quantile6_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.7, \"nearest\").alias(f\"{c}_ETD_quantile7_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.8, \"nearest\").alias(f\"{c}_ETD_quantile8_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).quantile(0.9, \"nearest\").alias(f\"{c}_ETD_quantile9_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).mean().alias(f\"{c}_ETD_mean_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).max().alias(f\"{c}_ETD_max_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).min().alias(f\"{c}_ETD_min_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).std().alias(f\"{c}_ETD_std_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).median().alias(f\"{c}_ETD_median_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"activations\")==c).last().alias(f\"{c}_ETD_rank_{feature_suffix}\") for c in name_feature],\n",
    "        \n",
    "        \n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.1, \"nearest\").alias(f\"{c}_CP_quantile1_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.2, \"nearest\").alias(f\"{c}_CP_quantile2_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.3, \"nearest\").alias(f\"{c}_CP_quantile3_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.4, \"nearest\").alias(f\"{c}_CP_quantile4_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.5, \"nearest\").alias(f\"{c}_CP_quantile5_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.6, \"nearest\").alias(f\"{c}_CP_quantile6_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.7, \"nearest\").alias(f\"{c}_CP_quantile7_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.8, \"nearest\").alias(f\"{c}_CP_quantile8_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).quantile(0.9, \"nearest\").alias(f\"{c}_CP_quantile9_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).mean().alias(f\"{c}_CP_mean_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).max().alias(f\"{c}_CP_max_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).min().alias(f\"{c}_CP_min_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).std().alias(f\"{c}_CP_std_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).median().alias(f\"{c}_CP_median_{feature_suffix}\") for c in name_feature],\n",
    "        *[pl.col(\"cursor_position_diff\").filter(pl.col(\"activations\")==c).last().alias(f\"{c}_CP_rank_{feature_suffix}\") for c in name_feature],\n",
    "        \n",
    "        \n",
    "        *[pl.col(\"activations\").filter(pl.col(\"down_event\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.1, \"nearest\").alias(f\"DE_{c}_ET_quantile1_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.2, \"nearest\").alias(f\"DE_{c}_ET_quantile2_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.3, \"nearest\").alias(f\"DE_{c}_ET_quantile3_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.4, \"nearest\").alias(f\"DE_{c}_ET_quantile4_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.5, \"nearest\").alias(f\"DE_{c}_ET_quantile5_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.6, \"nearest\").alias(f\"DE_{c}_ET_quantile6_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.7, \"nearest\").alias(f\"DE_{c}_ET_quantile7_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.8, \"nearest\").alias(f\"DE_{c}_ET_quantile8_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).quantile(0.9, \"nearest\").alias(f\"DE_{c}_ET_quantile9_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).mean().alias(f\"DE_{c}_ET_mean_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).std().alias(f\"DE_{c}_ET_std_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).max().alias(f\"DE_{c}_ET_max_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).min().alias(f\"DE_{c}_ET_min_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).median().alias(f\"DE_{c}_ET_median_{feature_suffix}\") for c in down_event_feature],\n",
    "        # adding rank features\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"down_event\")==c).last().alias(f\"DE_{c}_ET_rank_{feature_suffix}\") for c in down_event_feature],\n",
    "        \n",
    "        \n",
    "        \n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.1, \"nearest\").alias(f\"DE_{c}_ETD_quantile1_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.2, \"nearest\").alias(f\"DE_{c}_ETD_quantile2_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.3, \"nearest\").alias(f\"DE_{c}_ETD_quantile3_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.4, \"nearest\").alias(f\"DE_{c}_ETD_quantile4_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.5, \"nearest\").alias(f\"DE_{c}_ETD_quantile5_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.6, \"nearest\").alias(f\"DE_{c}_ETD_quantile6_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.7, \"nearest\").alias(f\"DE_{c}_ETD_quantile7_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.8, \"nearest\").alias(f\"DE_{c}_ETD_quantile8_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).quantile(0.9, \"nearest\").alias(f\"DE_{c}_ETD_quantile9_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).mean().alias(f\"DE_{c}_ETD_mean_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).max().alias(f\"DE_{c}_ETD_max_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).min().alias(f\"DE_{c}_ETD_min_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).std().alias(f\"DE_{c}_ETD_std_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).median().alias(f\"DE_{c}_ETD_median_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"elapsed_time_diff_1\").filter(pl.col(\"down_event\")==c).last().alias(f\"DE_{c}_ETD_rank_{feature_suffix}\") for c in down_event_feature],\n",
    "        \n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.1, \"nearest\").alias(f\"DE_{c}_DT_quantile1_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.2, \"nearest\").alias(f\"DE_{c}_DT_quantile2_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.3, \"nearest\").alias(f\"DE_{c}_DT_quantile3_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.4, \"nearest\").alias(f\"DE_{c}_DT_quantile4_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.5, \"nearest\").alias(f\"DE_{c}_DT_quantile5_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.6, \"nearest\").alias(f\"DE_{c}_DT_quantile6_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.7, \"nearest\").alias(f\"DE_{c}_DT_quantile7_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.8, \"nearest\").alias(f\"DE_{c}_DT_quantile8_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).quantile(0.9, \"nearest\").alias(f\"DE_{c}_DT_quantile9_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).mean().alias(f\"DE_{c}_DT_mean_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).max().alias(f\"DE_{c}_DT_max_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).min().alias(f\"DE_{c}_DT_min_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).std().alias(f\"DE_{c}_DT_std_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).median().alias(f\"DE_{c}_DT_median_{feature_suffix}\") for c in down_event_feature],\n",
    "        *[pl.col(\"down_time\").filter(pl.col(\"down_event\")==c).last().alias(f\"DE_{c}_DT_rank_{feature_suffix}\") for c in down_event_feature],\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        *[pl.col(\"activations\").filter(pl.col(\"up_event\") == c).count().alias(f\"UE_{c}_event_name_counts{feature_suffix}\")for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.1, \"nearest\").alias(f\"UE_{c}_ET_quantile1_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.2, \"nearest\").alias(f\"UE_{c}_ET_quantile2_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.3, \"nearest\").alias(f\"UE_{c}_ET_quantile3_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.4, \"nearest\").alias(f\"UE_{c}_ET_quantile4_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.5, \"nearest\").alias(f\"UE_{c}_ET_quantile5_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.6, \"nearest\").alias(f\"UE_{c}_ET_quantile6_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.7, \"nearest\").alias(f\"UE_{c}_ET_quantile7_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.8, \"nearest\").alias(f\"UE_{c}_ET_quantile8_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).quantile(0.9, \"nearest\").alias(f\"UE_{c}_ET_quantile9_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).mean().alias(f\"UE_{c}_ET_mean_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).std().alias(f\"UE_{c}_ET_std_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).max().alias(f\"UE_{c}_ET_max_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).min().alias(f\"UE_{c}_ET_min_{feature_suffix}\") for c in up_event_feature],\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).median().alias(f\"UE_{c}_ET_median_{feature_suffix}\") for c in up_event_feature],\n",
    "        # adding rank features\n",
    "        *[pl.col(\"action_time\").filter(pl.col(\"up_event\")==c).last().alias(f\"UE_{c}_ET_rank_{feature_suffix}\") for c in up_event_feature],\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "    df = x.groupby([\"id\"], maintain_order=True).agg(aggs).sort(\"id\")\n",
    "    tmp_df = pl.from_pandas(train_essays)\n",
    "    corpus = train_essays['text'].tolist()\n",
    "    tmp_df = tmp_df.with_columns([pl.col(\"text\").apply(lambda x: re.findall(r'q+', x)).alias(\"text_change\")])\n",
    "    tmp_df = tmp_df.with_columns([pl.col(\"text\").apply(lambda x: len(x)).alias(\"input_word_count\")])\n",
    "    tmp_df = tmp_df.with_columns([pl.col(\"text\").apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0) ).alias(\"input_word_length_mean\")])\n",
    "    tmp_df = tmp_df.with_columns([pl.col(\"text\").apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0) ).alias(\"input_word_length_max\")])\n",
    "    tmp_df = tmp_df.with_columns([pl.col(\"text\").apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0) ).alias(\"input_word_length_std\")])\n",
    "    tmp_df = tmp_df.drop([\"text_change\"])\n",
    "    df = df.join(tmp_df, on='id')\n",
    "    df = df.to_pandas()\n",
    "    df['word_time_ratio'] = df[f\"word_count_max_{feature_suffix}\"] / df[f\"down_time_max_{feature_suffix}\"]\n",
    "    #df['word_event_ratio'] = df[f\"word_count_max_{feature_suffix}\"] / df['event_count']\n",
    "    #df['event_time_ratio'] = df['event_count']  / df[f\"down_time_max_{feature_suffix}\"]\n",
    "    df['idle_time_ratio'] = df[f\"elapsed_time_diff_1_sum_{feature_suffix}\"] / df[f\"down_time_max_{feature_suffix}\"]\n",
    "    \n",
    "    text_feature = pipe.transform(corpus).toarray() # pipe.transform(corpus).toarray() # get_model_feature(model, texts)\n",
    "    text_feature_df = pd.DataFrame(text_feature, columns=[f'text_features_{i}' for i in range(text_feature.shape[1])])\n",
    "    text_feature_df['id'] = train_essays.index\n",
    "    text_feature_df = text_feature_df.set_index('id')\n",
    "    \n",
    "    \n",
    "#     x.with_clumns([pl.col(\"up_time\").shift(1).fill_null(pl.col(\"down_time\")).clip(-1e9, 1e9).over(\n",
    "#         [\"id\"]).alias(\"up_time_lagged\"),\n",
    "#                   pl.abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000\n",
    "                  \n",
    "#                   ])\n",
    "    \n",
    "    \n",
    "    df = df.join(text_feature_df, on='id')\n",
    "    \n",
    "    df = df.merge(train_sent_agg_df, on='id', how='left')\n",
    "    df = df.merge(train_paragraph_agg_df, on='id', how='left')\n",
    "\n",
    "    #df = df.join(bert_result, on='id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a5e50b8-e26c-4519-9154-03a638130c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df.with_columns([pl.col(\"up_time\").shift(1).fill_null(pl.col(\"down_time\")).clip(-1e9, 1e9).over([\"id\"]).alias(\"up_time_lagged\"),  \n",
    "#                   ])\n",
    "# df = df.with_columns([\n",
    "#                  ((pl.col('down_time') - pl.col('up_time_lagged')) / 1000).abs().alias(\"time_diff\"), \n",
    "#                   ])\n",
    "# aggs = [pl.col('time_diff').max().alias(\"time_diff_max_\"),\n",
    "#         pl.col('time_diff').min().alias(\"time_diff_min_\"),\n",
    "#         pl.col('time_diff').median().alias(\"time_diff_median_\"),\n",
    "#        ]\n",
    "# df = x.groupby([\"id\"], maintain_order=True).agg(aggs).sort(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "107fca5c-1256-42be-a202-55c1341f3008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_logs=pd.read_csv(\"train_logs.csv\")\n",
    "data = []\n",
    "for logs in [train_logs]:\n",
    "    #up_time向后移动并且用down_time填充缺失的位置\n",
    "    logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "    #(down_time减上一个时刻的up_time) /1000是单位转换\n",
    "    logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "    #按照id打包time_diff\n",
    "    group = logs.groupby('id')['time_diff']\n",
    "    #延迟时间的max,min,median\n",
    "    largest_lantency = group.max()\n",
    "    smallest_lantency = group.min()\n",
    "    median_lantency = group.median()\n",
    "    #down_time的first /1000是做单位转换吧\n",
    "    initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "    #分层次求和\n",
    "    pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x <= 1)).sum())\n",
    "    pauses_1_sec = group.apply(lambda x: ((x > 1) & (x <= 1.5)).sum())\n",
    "    pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x <= 2)).sum())\n",
    "    pauses_2_sec = group.apply(lambda x: ((x > 2) & (x <= 3)).sum())\n",
    "    pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "\n",
    "    data.append(pd.DataFrame({\n",
    "        'id': logs['id'].unique(),\n",
    "        #延迟\n",
    "        'largest_lantency': largest_lantency,\n",
    "        'smallest_lantency': smallest_lantency,\n",
    "        'median_lantency': median_lantency,\n",
    "        'initial_pause': initial_pause,\n",
    "        'pauses_half_sec': pauses_half_sec,\n",
    "        'pauses_1_sec': pauses_1_sec,\n",
    "        'pauses_1_half_sec': pauses_1_half_sec,\n",
    "        'pauses_2_sec': pauses_2_sec,\n",
    "        'pauses_3_sec': pauses_3_sec,\n",
    "    }).reset_index(drop=True))\n",
    "\n",
    "train_eD592674 = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b700978-a98e-45fc-995a-b35886334eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = feature_engineer_for_index(df, 'index_')\n",
    "df = df.merge(train_eD592674, on='id', how='left')\n",
    "df.drop(columns=['essay', 'text', 'sent', 'paragraph'], inplace=True)\n",
    "invalid_columns = [c for c in df.columns if '[' in c or ']' in c or '<' in c]\n",
    "rename_dict = {}\n",
    "for name in invalid_columns:\n",
    "    rename_dict[name] = name.replace('[', 'left_brackets').replace(']', 'right_brackets').replace('<', 'less')\n",
    "df = df.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d8d4d2-e072-4dd0-90f9-a576880275bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "word_count_diff_1_quantile1_index_\n",
      "word_count_diff_1_quantile2_index_\n",
      "word_count_diff_2_quantile2_index_\n",
      "word_count_diff_3_quantile2_index_\n",
      "word_count_diff_1_quantile3_index_\n",
      "word_count_diff_2_quantile3_index_\n",
      "word_count_diff_3_quantile3_index_\n",
      "word_count_diff_1_quantile4_index_\n",
      "word_count_diff_2_quantile4_index_\n",
      "word_count_diff_3_quantile4_index_\n",
      "word_count_diff_1_quantile5_index_\n",
      "word_count_diff_2_quantile5_index_\n",
      "word_count_diff_1_quantile6_index_\n",
      "word_count_diff_1_quantile7_index_\n",
      "cursor_position_min_index_\n",
      "word_count_diff_1_median_index_\n",
      "word_count_diff_2_median_index_\n",
      "Input_CP_quantile1_index_\n",
      "Input_CP_quantile2_index_\n",
      "Input_CP_quantile3_index_\n",
      "Input_CP_quantile4_index_\n",
      "Input_CP_quantile5_index_\n",
      "Input_CP_quantile6_index_\n",
      "Input_CP_quantile7_index_\n",
      "Input_CP_quantile8_index_\n",
      "Input_CP_quantile9_index_\n",
      "Input_CP_median_index_\n",
      "Input_CP_rank_index_\n",
      "DE_Shift_ET_quantile1_index_\n",
      "DE_Shift_ET_quantile2_index_\n",
      "DE_Shift_ET_quantile3_index_\n",
      "DE_Shift_ET_quantile4_index_\n",
      "DE_Shift_ET_quantile5_index_\n",
      "DE_Shift_ET_quantile6_index_\n",
      "DE_Shift_ET_quantile7_index_\n",
      "DE_Shift_ET_quantile8_index_\n",
      "DE_Shift_ET_quantile9_index_\n",
      "DE_Shift_ET_mean_index_\n",
      "DE_Shift_ET_std_index_\n",
      "DE_Shift_ET_max_index_\n",
      "DE_Shift_ET_min_index_\n",
      "DE_Shift_ET_median_index_\n",
      "DE_Shift_ET_rank_index_\n",
      "UE_Shift_ET_quantile1_index_\n",
      "UE_Shift_ET_quantile2_index_\n",
      "UE_Shift_ET_quantile3_index_\n",
      "UE_Shift_ET_quantile4_index_\n",
      "UE_Shift_ET_quantile5_index_\n",
      "UE_Shift_ET_quantile6_index_\n",
      "UE_Shift_ET_quantile7_index_\n",
      "UE_Shift_ET_quantile8_index_\n",
      "UE_Shift_ET_quantile9_index_\n",
      "UE_Shift_ET_mean_index_\n",
      "UE_Shift_ET_std_index_\n",
      "UE_Shift_ET_max_index_\n",
      "UE_Shift_ET_min_index_\n",
      "UE_Shift_ET_median_index_\n",
      "UE_Shift_ET_rank_index_\n",
      "input_word_length_mean\n",
      "input_word_length_max\n",
      "input_word_length_std\n",
      "smallest_lantency\n",
      "*********df DONE*********\n"
     ]
    }
   ],
   "source": [
    "# some cleaning...\n",
    "null1 = df.isnull().sum().sort_values(ascending=False) / len(df)\n",
    "\n",
    "drop1 = list(null1[null1>0.9].index)\n",
    "\n",
    "print(len(drop1))\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].nunique()==1:\n",
    "        print(col)\n",
    "        drop1.append(col)\n",
    "print(\"*********df DONE*********\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d2c36b7-cfa9-4d73-9292-358e27e1918e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop1 += not_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "163b8fa0-0d91-4826-9d19-f4eae7606b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = [c for c in df.columns if c not in drop1 + ['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f09ab1-29cc-469d-8d25-83fca6ae85c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e7933f5-5c8c-4408-a631-c8c04bcfc275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>every_sentence_word_count</th>\n",
       "      <th>every_char_word</th>\n",
       "      <th>every_sentence_change</th>\n",
       "      <th>char_per_min</th>\n",
       "      <th>word_per_min</th>\n",
       "      <th>proportion_of_pause_time</th>\n",
       "      <th>count_of_pause_time</th>\n",
       "      <th>pause_length</th>\n",
       "      <th>freq_of_pause_time</th>\n",
       "      <th>activations_unique_index_</th>\n",
       "      <th>...</th>\n",
       "      <th>paragraph_word_count_kurt</th>\n",
       "      <th>paragraph_word_count_sum</th>\n",
       "      <th>largest_lantency</th>\n",
       "      <th>median_lantency</th>\n",
       "      <th>initial_pause</th>\n",
       "      <th>pauses_half_sec</th>\n",
       "      <th>pauses_1_sec</th>\n",
       "      <th>pauses_1_half_sec</th>\n",
       "      <th>pauses_2_sec</th>\n",
       "      <th>pauses_3_sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>001519c8</th>\n",
       "      <td>12.190476</td>\n",
       "      <td>6.011719</td>\n",
       "      <td>20.190476</td>\n",
       "      <td>0.856260</td>\n",
       "      <td>0.142432</td>\n",
       "      <td>0.527536</td>\n",
       "      <td>34</td>\n",
       "      <td>35973.181818</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269</td>\n",
       "      <td>154.136</td>\n",
       "      <td>0.062</td>\n",
       "      <td>4.526</td>\n",
       "      <td>116</td>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0022f953</th>\n",
       "      <td>21.533333</td>\n",
       "      <td>5.188855</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.183709</td>\n",
       "      <td>0.686187</td>\n",
       "      <td>38</td>\n",
       "      <td>35089.103448</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342703</td>\n",
       "      <td>355</td>\n",
       "      <td>145.899</td>\n",
       "      <td>0.061</td>\n",
       "      <td>30.623</td>\n",
       "      <td>141</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0042269b</th>\n",
       "      <td>19.238095</td>\n",
       "      <td>5.670792</td>\n",
       "      <td>21.238095</td>\n",
       "      <td>1.296711</td>\n",
       "      <td>0.228665</td>\n",
       "      <td>0.691334</td>\n",
       "      <td>26</td>\n",
       "      <td>46527.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.536764</td>\n",
       "      <td>410</td>\n",
       "      <td>153.886</td>\n",
       "      <td>0.040</td>\n",
       "      <td>4.441</td>\n",
       "      <td>83</td>\n",
       "      <td>46</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0059420b</th>\n",
       "      <td>15.846154</td>\n",
       "      <td>5.082524</td>\n",
       "      <td>11.692308</td>\n",
       "      <td>0.768159</td>\n",
       "      <td>0.151137</td>\n",
       "      <td>0.458725</td>\n",
       "      <td>21</td>\n",
       "      <td>31664.941176</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>208</td>\n",
       "      <td>101.690</td>\n",
       "      <td>0.131</td>\n",
       "      <td>41.395</td>\n",
       "      <td>178</td>\n",
       "      <td>81</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0075873a</th>\n",
       "      <td>10.956522</td>\n",
       "      <td>5.563492</td>\n",
       "      <td>22.478261</td>\n",
       "      <td>0.885146</td>\n",
       "      <td>0.159099</td>\n",
       "      <td>0.645042</td>\n",
       "      <td>34</td>\n",
       "      <td>29264.178571</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722916</td>\n",
       "      <td>256</td>\n",
       "      <td>110.688</td>\n",
       "      <td>0.059</td>\n",
       "      <td>78.470</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffb8c745</th>\n",
       "      <td>10.720930</td>\n",
       "      <td>3.544469</td>\n",
       "      <td>22.372093</td>\n",
       "      <td>0.923626</td>\n",
       "      <td>0.260582</td>\n",
       "      <td>0.694018</td>\n",
       "      <td>18</td>\n",
       "      <td>51827.294118</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.798053</td>\n",
       "      <td>308</td>\n",
       "      <td>128.570</td>\n",
       "      <td>0.034</td>\n",
       "      <td>22.467</td>\n",
       "      <td>117</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffbef7e5</th>\n",
       "      <td>14.129032</td>\n",
       "      <td>4.285388</td>\n",
       "      <td>1.967742</td>\n",
       "      <td>1.056042</td>\n",
       "      <td>0.246428</td>\n",
       "      <td>0.469328</td>\n",
       "      <td>29</td>\n",
       "      <td>36683.200000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942230</td>\n",
       "      <td>443</td>\n",
       "      <td>267.869</td>\n",
       "      <td>0.172</td>\n",
       "      <td>21.732</td>\n",
       "      <td>121</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffccd6fd</th>\n",
       "      <td>40.200000</td>\n",
       "      <td>13.736319</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>1.426290</td>\n",
       "      <td>0.103834</td>\n",
       "      <td>0.553205</td>\n",
       "      <td>28</td>\n",
       "      <td>40992.521739</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1846</td>\n",
       "      <td>229.804</td>\n",
       "      <td>0.116</td>\n",
       "      <td>23.482</td>\n",
       "      <td>168</td>\n",
       "      <td>83</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffec5b38</th>\n",
       "      <td>13.322581</td>\n",
       "      <td>5.164649</td>\n",
       "      <td>8.903226</td>\n",
       "      <td>1.433034</td>\n",
       "      <td>0.277470</td>\n",
       "      <td>0.461450</td>\n",
       "      <td>16</td>\n",
       "      <td>55336.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.192696</td>\n",
       "      <td>417</td>\n",
       "      <td>127.733</td>\n",
       "      <td>0.091</td>\n",
       "      <td>19.885</td>\n",
       "      <td>116</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff05981</th>\n",
       "      <td>16.066667</td>\n",
       "      <td>6.207469</td>\n",
       "      <td>21.466667</td>\n",
       "      <td>0.736823</td>\n",
       "      <td>0.118699</td>\n",
       "      <td>0.415042</td>\n",
       "      <td>38</td>\n",
       "      <td>29903.291667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.539664</td>\n",
       "      <td>245</td>\n",
       "      <td>137.607</td>\n",
       "      <td>0.097</td>\n",
       "      <td>39.727</td>\n",
       "      <td>266</td>\n",
       "      <td>94</td>\n",
       "      <td>46</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2471 rows × 1759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          every_sentence_word_count  every_char_word  every_sentence_change  \\\n",
       "id                                                                            \n",
       "001519c8                  12.190476         6.011719              20.190476   \n",
       "0022f953                  21.533333         5.188855              17.400000   \n",
       "0042269b                  19.238095         5.670792              21.238095   \n",
       "0059420b                  15.846154         5.082524              11.692308   \n",
       "0075873a                  10.956522         5.563492              22.478261   \n",
       "...                             ...              ...                    ...   \n",
       "ffb8c745                  10.720930         3.544469              22.372093   \n",
       "ffbef7e5                  14.129032         4.285388               1.967742   \n",
       "ffccd6fd                  40.200000        13.736319              17.600000   \n",
       "ffec5b38                  13.322581         5.164649               8.903226   \n",
       "fff05981                  16.066667         6.207469              21.466667   \n",
       "\n",
       "          char_per_min  word_per_min  proportion_of_pause_time  \\\n",
       "id                                                               \n",
       "001519c8      0.856260      0.142432                  0.527536   \n",
       "0022f953      0.953237      0.183709                  0.686187   \n",
       "0042269b      1.296711      0.228665                  0.691334   \n",
       "0059420b      0.768159      0.151137                  0.458725   \n",
       "0075873a      0.885146      0.159099                  0.645042   \n",
       "...                ...           ...                       ...   \n",
       "ffb8c745      0.923626      0.260582                  0.694018   \n",
       "ffbef7e5      1.056042      0.246428                  0.469328   \n",
       "ffccd6fd      1.426290      0.103834                  0.553205   \n",
       "ffec5b38      1.433034      0.277470                  0.461450   \n",
       "fff05981      0.736823      0.118699                  0.415042   \n",
       "\n",
       "          count_of_pause_time  pause_length  freq_of_pause_time  \\\n",
       "id                                                                \n",
       "001519c8                   34  35973.181818                   2   \n",
       "0022f953                   38  35089.103448                   0   \n",
       "0042269b                   26  46527.700000                   1   \n",
       "0059420b                   21  31664.941176                   5   \n",
       "0075873a                   34  29264.178571                   3   \n",
       "...                       ...           ...                 ...   \n",
       "ffb8c745                   18  51827.294118                   3   \n",
       "ffbef7e5                   29  36683.200000                   3   \n",
       "ffccd6fd                   28  40992.521739                  10   \n",
       "ffec5b38                   16  55336.200000                   1   \n",
       "fff05981                   38  29903.291667                   3   \n",
       "\n",
       "          activations_unique_index_  ...  paragraph_word_count_kurt  \\\n",
       "id                                   ...                              \n",
       "001519c8                          5  ...                        NaN   \n",
       "0022f953                          5  ...                   2.342703   \n",
       "0042269b                          4  ...                  -1.536764   \n",
       "0059420b                          5  ...                        NaN   \n",
       "0075873a                          3  ...                   0.722916   \n",
       "...                             ...  ...                        ...   \n",
       "ffb8c745                          4  ...                  -3.798053   \n",
       "ffbef7e5                          4  ...                  -0.942230   \n",
       "ffccd6fd                          3  ...                        NaN   \n",
       "ffec5b38                          3  ...                  -1.192696   \n",
       "fff05981                          6  ...                  -1.539664   \n",
       "\n",
       "          paragraph_word_count_sum  largest_lantency  median_lantency  \\\n",
       "id                                                                      \n",
       "001519c8                       269           154.136            0.062   \n",
       "0022f953                       355           145.899            0.061   \n",
       "0042269b                       410           153.886            0.040   \n",
       "0059420b                       208           101.690            0.131   \n",
       "0075873a                       256           110.688            0.059   \n",
       "...                            ...               ...              ...   \n",
       "ffb8c745                       308           128.570            0.034   \n",
       "ffbef7e5                       443           267.869            0.172   \n",
       "ffccd6fd                      1846           229.804            0.116   \n",
       "ffec5b38                       417           127.733            0.091   \n",
       "fff05981                       245           137.607            0.097   \n",
       "\n",
       "          initial_pause  pauses_half_sec  pauses_1_sec  pauses_1_half_sec  \\\n",
       "id                                                                          \n",
       "001519c8          4.526              116            51                 30   \n",
       "0022f953         30.623              141            37                 14   \n",
       "0042269b          4.441               83            46                 25   \n",
       "0059420b         41.395              178            81                 34   \n",
       "0075873a         78.470               65            24                 11   \n",
       "...                 ...              ...           ...                ...   \n",
       "ffb8c745         22.467              117            41                 18   \n",
       "ffbef7e5         21.732              121            43                 24   \n",
       "ffccd6fd         23.482              168            83                 37   \n",
       "ffec5b38         19.885              116            35                 27   \n",
       "fff05981         39.727              266            94                 46   \n",
       "\n",
       "          pauses_2_sec  pauses_3_sec  \n",
       "id                                    \n",
       "001519c8            21           103  \n",
       "0022f953            19            61  \n",
       "0042269b            25            52  \n",
       "0059420b            32            55  \n",
       "0075873a            17            71  \n",
       "...                ...           ...  \n",
       "ffb8c745            11            30  \n",
       "ffbef7e5            24            66  \n",
       "ffccd6fd            29            58  \n",
       "ffec5b38            15            48  \n",
       "fff05981            38            82  \n",
       "\n",
       "[2471 rows x 1759 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.set_index('id')\n",
    "df = df[FEATURES]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ff6d59-ccec-434e-8266-1577ea1918b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('train_scores.csv')\n",
    "target = target.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a22cb8b-c357-41e4-873b-9d86a23e468e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_xgb = pd.DataFrame(data=np.zeros((len(target),3)), index=target.index, columns=['xgb', 'lgbm', 'cat'])\n",
    "#models = {}\n",
    "best_iteration_xgb = defaultdict(list)\n",
    "importance_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric':'rmse',\n",
    "        'learning_rate': 0.02,\n",
    "        'alpha': 8,\n",
    "        'max_depth': 4,\n",
    "        'subsample':0.8,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'seed': 42\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "cat_params = {\n",
    "    \n",
    "    'learning_rate': 0.02,\n",
    "    'depth': 8,\n",
    "    'iterations': 500,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "df = df.join(target)\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88d2206-08e9-4836-9800-6da321c42c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convert_target = {0.5: 0,\n",
    "                    1.0: 1,\n",
    "                    1.5: 2,\n",
    "                    2.0: 3,\n",
    "                    2.5: 4,\n",
    "                    3.0: 5,\n",
    "                    3.5: 6,\n",
    "                    4.0: 7,\n",
    "                    4.5: 8,\n",
    "                    5.0: 9,\n",
    "                    5.5: 10,\n",
    "                    6.0: 11,}\n",
    "\n",
    "reverse_convert_target = {0:0.5,\n",
    "                            1:1.0,\n",
    "                            2:1.5,\n",
    "                            3:2.0,\n",
    "                            4:2.5,\n",
    "                            5:3.0,\n",
    "                            6:3.5,\n",
    "                            7:4.0,\n",
    "                            8:4.5,\n",
    "                            9:5.0,\n",
    "                            10:5.5,\n",
    "                            11:6.0,}\n",
    "df['new_label'] = df['score'].map(convert_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12750932-441f-460b-87fb-bf33f40fae67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0).clip(-1e9, 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f5c7125-0f30-44cd-87c1-395ac96df248",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model_name = 'lgbm'\n",
    "    target_name = 'score'\n",
    "    task_type = 'reg'\n",
    "    final_score = []\n",
    "    if model_name == 'lgbm':\n",
    "        params = {\n",
    "                'metric': 'rmse', \n",
    "                'random_state': 42,\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500, log=True),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1, log=True),\n",
    "                'num_leaves' : trial.suggest_int('num_leaves', 2, 32),\n",
    "                #'max_depth' : trial.suggest_int('max_depth', 4, 16),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n",
    "            }\n",
    "    elif model_name == 'xgb':\n",
    "        params = {\n",
    "                'booster': 'gbtree',\n",
    "                'tree_method': 'hist',\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric':'rmse',\n",
    "                'random_state': 42,\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500, log=True),\n",
    "                'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "                'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1, log=True),\n",
    "                'max_leaves' : trial.suggest_int('max_leaves', 2, 32),\n",
    "                'max_depth': trial.suggest_int('max_depth', 1, 32),\n",
    "            }\n",
    "    else:\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1, log=True),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "            'eval_metric':'RMSE',\n",
    "            'random_state': 42,\n",
    "            }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X=df, y=df.score*10,)):\n",
    "         # TRAIN DATA\n",
    "        train_x = df.iloc[train_index][FEATURES]\n",
    "        train_users = train_x.index.values\n",
    "        train_y = df[[target_name]].iloc[train_index]\n",
    "\n",
    "        # VALID DATA\n",
    "        valid_x = df.iloc[test_index][FEATURES]\n",
    "        valid_users = valid_x.index.values\n",
    "        valid_y = df[[target_name]].iloc[test_index]\n",
    "\n",
    "\n",
    "        if model_name == 'lgbm' and task_type != 'class':\n",
    "            clf = LGBMRegressor(**params)#XGBRegressor(**xgb_params)\n",
    "        elif model_name == 'lgbm' and task_type == 'class':\n",
    "            clf = LGBMClassifier(**params)#XGBRegressor(**xgb_params)\n",
    "        elif model_name == 'xgb':\n",
    "            clf = XGBRegressor(**params)\n",
    "        else:\n",
    "            clf = CatBoostRegressor(**params)\n",
    "        if model_name != 'cat':\n",
    "            clf.fit(train_x.astype('float32'), train_y[target_name],\n",
    "                    eval_set=[(valid_x.astype('float32'), valid_y[target_name])],\n",
    "                    verbose=0\n",
    "                    )\n",
    "        else:\n",
    "            train_pool = Pool(train_x.astype('float32'), train_y[target_name])\n",
    "            test_pool = Pool(valid_x.astype('float32'), valid_y[target_name])\n",
    "            clf.fit(train_pool,\n",
    "                    eval_set=test_pool,\n",
    "                    verbose=0)\n",
    "\n",
    "        oof_xgb.loc[valid_users, model_name] = clf.predict(valid_x.astype('float32')) \n",
    "\n",
    "    final_result = oof_xgb.join(target)\n",
    "    rmse = mean_squared_error(final_result['score'], final_result[model_name], squared=False)\n",
    "    final_score.append(rmse)\n",
    "    return np.mean(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bc7fa-5f31-4a5e-b3dc-28e8a69aa6c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-12-11 10:33:28,535]\u001b[0m A new study created in memory with name: Optimize boosting hyperparameters\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 10:54:18,182]\u001b[0m Trial 0 finished with value: 0.6239449600639966 and parameters: {'n_estimators': 111, 'reg_alpha': 8.470380606220221, 'reg_lambda': 6.60194071607205, 'colsample_bytree': 0.7701745056326487, 'subsample': 0.6910953127309167, 'learning_rate': 0.034669768734086205, 'num_leaves': 18, 'min_child_samples': 94}. Best is trial 0 with value: 0.6239449600639966.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 11:48:17,997]\u001b[0m Trial 1 finished with value: 0.6870895214546545 and parameters: {'n_estimators': 232, 'reg_alpha': 2.550060514101067, 'reg_lambda': 7.920996059263239, 'colsample_bytree': 0.7481698664485007, 'subsample': 0.9088073752387767, 'learning_rate': 0.005643788347624591, 'num_leaves': 23, 'min_child_samples': 75}. Best is trial 0 with value: 0.6239449600639966.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 12:45:12,069]\u001b[0m Trial 2 finished with value: 0.6228124590482226 and parameters: {'n_estimators': 388, 'reg_alpha': 9.745840129548489, 'reg_lambda': 5.421675070282226, 'colsample_bytree': 0.6282436668100648, 'subsample': 0.5670651647389366, 'learning_rate': 0.08519318745549923, 'num_leaves': 18, 'min_child_samples': 92}. Best is trial 2 with value: 0.6228124590482226.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 13:46:57,320]\u001b[0m Trial 3 finished with value: 0.9831786100162003 and parameters: {'n_estimators': 339, 'reg_alpha': 6.146865920860071, 'reg_lambda': 8.122549940861276, 'colsample_bytree': 0.8440999710105128, 'subsample': 0.6931846387535905, 'learning_rate': 0.0002404336063642614, 'num_leaves': 25, 'min_child_samples': 88}. Best is trial 2 with value: 0.6228124590482226.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 15:16:07,036]\u001b[0m Trial 4 finished with value: 0.6191764493149468 and parameters: {'n_estimators': 238, 'reg_alpha': 4.9489276959037465, 'reg_lambda': 5.817724139403509, 'colsample_bytree': 0.8358247375256095, 'subsample': 0.8648568622045278, 'learning_rate': 0.06795512396382833, 'num_leaves': 32, 'min_child_samples': 51}. Best is trial 4 with value: 0.6191764493149468.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 15:33:07,332]\u001b[0m Trial 5 finished with value: 0.6388710853148797 and parameters: {'n_estimators': 107, 'reg_alpha': 1.1630560132770091, 'reg_lambda': 0.26106471666703396, 'colsample_bytree': 0.7090468822241525, 'subsample': 0.9228426996386708, 'learning_rate': 0.021021222357426834, 'num_leaves': 10, 'min_child_samples': 45}. Best is trial 4 with value: 0.6191764493149468.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 16:09:18,112]\u001b[0m Trial 6 finished with value: 0.6222719753354056 and parameters: {'n_estimators': 157, 'reg_alpha': 7.002134886713734, 'reg_lambda': 9.892054117713302, 'colsample_bytree': 0.8010295204162285, 'subsample': 0.5225601613270117, 'learning_rate': 0.09151681930089972, 'num_leaves': 28, 'min_child_samples': 82}. Best is trial 4 with value: 0.6191764493149468.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 17:08:23,139]\u001b[0m Trial 7 finished with value: 0.9876002235728946 and parameters: {'n_estimators': 329, 'reg_alpha': 5.0968604729547975, 'reg_lambda': 5.77015368724089, 'colsample_bytree': 0.7218766996779553, 'subsample': 0.7009872332643778, 'learning_rate': 0.00021858054087119054, 'num_leaves': 16, 'min_child_samples': 96}. Best is trial 4 with value: 0.6191764493149468.\u001b[0m\n",
      "\u001b[32m[I 2023-12-11 18:16:38,172]\u001b[0m Trial 8 finished with value: 0.682176857483627 and parameters: {'n_estimators': 382, 'reg_alpha': 6.164381559776124, 'reg_lambda': 9.28673939810035, 'colsample_bytree': 0.8378364499332809, 'subsample': 0.516773643131303, 'learning_rate': 0.0038073748802936237, 'num_leaves': 14, 'min_child_samples': 77}. Best is trial 4 with value: 0.6191764493149468.\u001b[0m\n",
      "\u001b[32m[I 2023-12-12 07:41:04,136]\u001b[0m Trial 24 finished with value: 0.6155931200359408 and parameters: {'n_estimators': 158, 'reg_alpha': 0.2933055113555475, 'reg_lambda': 3.601438762111186, 'colsample_bytree': 0.9388830274893231, 'subsample': 0.9429511031396427, 'learning_rate': 0.047703246836100666, 'num_leaves': 24, 'min_child_samples': 23}. Best is trial 18 with value: 0.6127791646327606.\u001b[0m\n",
      "\u001b[32m[I 2023-12-12 09:12:56,242]\u001b[0m Trial 25 finished with value: 0.6137752921570989 and parameters: {'n_estimators': 275, 'reg_alpha': 1.1802488922816927, 'reg_lambda': 4.921113569822856, 'colsample_bytree': 0.887271375194003, 'subsample': 0.8788484031526941, 'learning_rate': 0.018290518104286373, 'num_leaves': 26, 'min_child_samples': 51}. Best is trial 18 with value: 0.6127791646327606.\u001b[0m\n",
      "\u001b[32m[I 2023-12-12 10:22:30,615]\u001b[0m Trial 26 finished with value: 0.6118896720214734 and parameters: {'n_estimators': 274, 'reg_alpha': 0.8332437261115078, 'reg_lambda': 6.296148736772044, 'colsample_bytree': 0.8981453618414851, 'subsample': 0.9629384376856098, 'learning_rate': 0.022507393572923525, 'num_leaves': 26, 'min_child_samples': 73}. Best is trial 26 with value: 0.6118896720214734.\u001b[0m\n",
      "\u001b[32m[I 2023-12-12 11:14:00,968]\u001b[0m Trial 27 finished with value: 0.6688910176092504 and parameters: {'n_estimators': 210, 'reg_alpha': 0.5497304036371078, 'reg_lambda': 6.635576965946125, 'colsample_bytree': 0.9538691453242356, 'subsample': 0.9635512731574056, 'learning_rate': 0.007183598199886711, 'num_leaves': 29, 'min_child_samples': 76}. Best is trial 26 with value: 0.6118896720214734.\u001b[0m\n",
      "\u001b[32m[I 2023-12-12 12:15:16,585]\u001b[0m Trial 28 finished with value: 0.6217283249482836 and parameters: {'n_estimators': 225, 'reg_alpha': 0.03624590729422761, 'reg_lambda': 5.994545151550325, 'colsample_bytree': 0.9957682813765539, 'subsample': 0.9607982309648698, 'learning_rate': 0.013745766847123572, 'num_leaves': 29, 'min_child_samples': 68}. Best is trial 26 with value: 0.6118896720214734.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize', study_name='Optimize boosting hyperparameters')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "207c257b-ac30-400c-b054-9d3a4c954652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'n_estimators': 230, 'reg_alpha': 0.011637601667639394, 'reg_lambda': 0.0040057167198726314, 'colsample_bytree': 0.6440699798660601, 'subsample': 0.6135016901133, 'learning_rate': 0.0213326106169659, 'num_leaves': 21, 'min_child_samples': 15}\n"
     ]
    }
   ],
   "source": [
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3759f4-765a-4361-bc78-3989823651c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'lgbm'\n",
    "target_name = 'score'\n",
    "task_type = 'reg'\n",
    "final_results = []\n",
    "if model_name == 'lgbm':\n",
    "    params = {\n",
    "            'metric': 'rmse', \n",
    "            'random_state': 42,\n",
    "        }\n",
    "    params.update(study.best_trial.params)\n",
    "elif model_name == 'xgb':\n",
    "    params = {\n",
    "                'booster': 'gbtree',\n",
    "                'tree_method': 'gpu_hist',\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric':'rmse',\n",
    "                'random_state': 42,}\n",
    "    params.update(study.best_trial.params)\n",
    "else:\n",
    "    params = {\n",
    "             'eval_metric':'RMSE',\n",
    "             'random_state': 42,\n",
    "             }\n",
    "    params.update(study.best_trial.params)\n",
    "for j in range(5): \n",
    "    skf = StratifiedKFold(n_splits=10, random_state=42 + j, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X=df, y=df.score*10,)):\n",
    "         # TRAIN DATA\n",
    "        train_x = df.iloc[train_index][FEATURES]\n",
    "        train_users = train_x.index.values\n",
    "        train_y = df[[target_name]].iloc[train_index]\n",
    "\n",
    "        # VALID DATA\n",
    "        valid_x = df.iloc[test_index][FEATURES]\n",
    "        valid_users = valid_x.index.values\n",
    "        valid_y = df[[target_name]].iloc[test_index]\n",
    "\n",
    "\n",
    "        if model_name == 'lgbm' and task_type != 'class':\n",
    "            clf = LGBMRegressor(**params)#XGBRegressor(**xgb_params)\n",
    "        elif model_name == 'lgbm' and task_type == 'class':\n",
    "            clf = LGBMClassifier(**params)#XGBRegressor(**xgb_params)\n",
    "        elif model_name == 'xgb':\n",
    "            clf = XGBRegressor(**params)\n",
    "        else:\n",
    "            clf = CatBoostRegressor(**params)\n",
    "        if model_name != 'cat':\n",
    "            clf.fit(train_x.astype('float32'), train_y[target_name],\n",
    "                    eval_set=[(valid_x.astype('float32'), valid_y[target_name])],\n",
    "                    verbose=0)\n",
    "        else:\n",
    "            train_pool = Pool(train_x.astype('float32'), train_y[target_name])\n",
    "            test_pool = Pool(valid_x.astype('float32'), valid_y[target_name])\n",
    "            clf.fit(train_pool,\n",
    "                    eval_set=test_pool,\n",
    "                    verbose=0)\n",
    "        #print(i+1, ', ', end='')\n",
    "        if model_name == 'lgbm':\n",
    "            clf.booster_.save_model(f'./lgbm_5fold/lgbm_question_{j}_{i}.xgb')\n",
    "        elif model_name == 'xgb':\n",
    "            clf.save_model(f'./xgb_5fold/XGB_question_{j}_{i}.xgb')\n",
    "        else:\n",
    "            clf.save_model(f'./cat_5fold/cat_question_{j}_{i}.xgb')\n",
    "\n",
    "        oof_xgb.loc[valid_users, model_name] = clf.predict(valid_x.astype('float32'))\n",
    "    \n",
    "    final_result = oof_xgb.join(target)\n",
    "    rmse = mean_squared_error(final_result['score'], final_result[model_name], squared=False)\n",
    "    final_results.append(rmse)\n",
    "print(np.mean(final_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9c3cc-4438-4039-b332-42867c827361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_squared_error(final_result['score'], final_result['lgbm'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2eb0be76-c198-47e9-81ab-fd8e104f41ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8501236400545635"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(final_result['score'], final_result['xgb'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "73b7a53e-b710-4b2e-aa03-e82fa5d7229d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108285895791181"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(final_result['score'], final_result['cat'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06b7e6a5-d7da-405c-80cd-aeac9ac24597",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6148173295521092\n",
      "(1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "best_score = 100\n",
    "best_weight = 0\n",
    "for w_0 in np.arange(0, 1.05, 0.05):\n",
    "    for w_1 in np.arange(0, 1.05-w_0, 0.05): \n",
    "        final_result['predict'] = w_0 * final_result['xgb'] + w_1*final_result['lgbm'] + (1-w_0-w_1) * final_result['cat']\n",
    "        score = mean_squared_error(final_result['score'], final_result['predict'], squared=False)\n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            best_weight = (w_0, w_1)\n",
    "print(best_score)\n",
    "print(best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11d71e28-bb6c-4549-90a2-76623288a070",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6056496849770823"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result['predict'] = (final_result['xgb'] + final_result['lgbm'] + final_result['cat']) / 3\n",
    "mean_squared_error(final_result['score'], final_result['predict'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ad1f937-28ee-4833-bc00-91c231f67557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "1.0\n",
      "1.0\n",
      "1.5\n",
      "1.5\n",
      "2.0\n",
      "2.0\n",
      "2.5\n",
      "2.5\n",
      "3.0\n",
      "3.0\n",
      "3.5\n",
      "3.5\n",
      "4.0\n",
      "4.0\n",
      "4.5\n",
      "4.5\n",
      "5.0\n",
      "5.0\n",
      "5.5\n",
      "5.5\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "for i in sorted([3.5, 6.0, 2.0, 4.0, 4.5, 2.5, 5.0, 3.0, 1.5, 5.5, 1.0, 0.5]):\n",
    "    sub = final_result[final_result['score'] == i]\n",
    "    score = mean_squared_error(sub['score'], sub['lgbm'], squared=False)\n",
    "    print(i)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e86933a3-0165-46d9-a0f5-3fcadebca88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./xgb_5fold/columns_names', FEATURES)\n",
    "np.save('./xgb_5fold/rename_dict', rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b704f26-38a0-41c7-a515-e5a118fe1fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: cat_5fold/ (stored 0%)\n",
      "  adding: cat_5fold/cat_question_3_8.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_9.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_2_4.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_8.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_0_0.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_3_7.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_2_6.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_3_5.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_4_6.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_3_9.xgb (deflated 59%)\n",
      "  adding: cat_5fold/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: cat_5fold/cat_question_3_1.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_2_7.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_3_0.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_2_5.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_3_2.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_2.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_5.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_4_8.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_2_8.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_2_3.xgb (deflated 63%)\n",
      "  adding: cat_5fold/cat_question_1_5.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_3.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_2_9.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_4_0.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_0_7.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_2_2.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_4_4.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_4_7.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_4_5.xgb (deflated 61%)\n",
      "  adding: cat_5fold/cat_question_1_4.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_0_1.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_8.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_2_0.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_6.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_3_4.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_1_0.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_2.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_4_2.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_4_9.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_9.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_6.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_1_7.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_4_3.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_3_6.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_0_3.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_0_4.xgb (deflated 58%)\n",
      "  adding: cat_5fold/cat_question_4_1.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_2_1.xgb (deflated 57%)\n",
      "  adding: cat_5fold/cat_question_1_1.xgb (deflated 59%)\n",
      "  adding: cat_5fold/cat_question_3_3.xgb (deflated 58%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r cat_5fold.zip cat_5fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c12a3f60-5e87-424b-8e84-2b38684664be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: xgb_5fold/ (stored 0%)\n",
      "updating: xgb_5fold/rename_dict.npy (deflated 30%)\n",
      "updating: xgb_5fold/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: xgb_5fold/pipeline.pkl (deflated 45%)\n",
      "updating: xgb_5fold/columns_names.npy (deflated 97%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r xgb_5fold.zip xgb_5fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7dfc920d-5a97-4f29-b3fc-e47c8ffd79da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: lgbm_5fold/ (stored 0%)\n",
      "updating: lgbm_5fold/lgbm_question_4_4.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_8.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_0.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_3.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_5.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_1.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_5.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_7.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_5.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_8.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_9.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_6.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_8.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_2.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_5.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_1.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: lgbm_5fold/lgbm_question_0_6.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_9.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_1.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_3.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_0.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_9.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_2.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_0.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_8.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_4.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_2.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_2.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_6.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_3.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_7.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_0.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_6.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_1.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_7.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_0.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_3.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_4.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_9.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_6.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_4.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_8.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_2_7.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_2.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_1_4.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_0_1.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_3.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_7.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_4_5.xgb (deflated 65%)\n",
      "updating: lgbm_5fold/lgbm_question_3_9.xgb (deflated 65%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r lgbm_5fold.zip lgbm_5fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b540a9b8-7707-4215-90b2-9085b725a2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0.6381 BASELINE - v1\n",
    "# 0.6372807396907196 remving up event\n",
    "# 0.6363905859568488 adding every_word_count with up event\n",
    "# 0.6388692848476841 adding every_word_count without up event\n",
    "# 0.6375898226286834 removing every_word_count with up event + word_count_pre_step\n",
    "# 0.634864097139824 -v4\n",
    "# 0.6324737799704642 adding every_char_word\n",
    "# 0.6312740919756227 adding every_char_word & every_sentence_change\n",
    "# 0.6301625913548855 - v9\n",
    "# 0.624126755429523 -v17\n",
    "# 0.6228860242274676 -v19\n",
    "# cv 0.616 lb 0.602 -v28\n",
    "# cv 0.6159972729277812 lb 0.601 -v29\n",
    "# cv 0.6139405 lb -v34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297641f8-07d1-4ae5-bbbf-0626c0cf19d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
