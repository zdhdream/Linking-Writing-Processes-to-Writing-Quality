{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa12181a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:07:59.959111Z",
     "iopub.status.busy": "2024-01-09T08:07:59.958232Z",
     "iopub.status.idle": "2024-01-09T08:08:48.055787Z",
     "shell.execute_reply": "2024-01-09T08:08:48.054845Z"
    },
    "papermill": {
     "duration": 48.120036,
     "end_time": "2024-01-09T08:08:48.058162",
     "exception": false,
     "start_time": "2024-01-09T08:07:59.938126",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lightautoml-038-dependecies\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/lightautoml-0.3.8-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/AutoWoE-1.3.2-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: catboost>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/cmaes-0.10.0-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: holidays in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.24)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1.2)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/joblib-1.2.0-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/json2html-1.3.0.tar.gz (from lightautoml==0.3.8)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hProcessing /kaggle/input/lightautoml-038-dependecies/lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1)\r\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.24.3)\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.4.0)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/poetry_core-1.8.1-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (6.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\r\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.12.2)\r\n",
      "Requirement already satisfied: statsmodels<=0.14.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.14.0)\r\n",
      "Requirement already satisfied: torch<=2.0.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (2.0.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (4.66.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/StrEnum-0.4.15-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (3.7.4)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (7.4.3)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (2023.3)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (1.11.4)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinx-7.2.6-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (0.2.4)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (0.20.1)\r\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (5.16.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (1.16.0)\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm<=3.2.1,>=2.3->lightautoml==0.3.8) (0.41.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0->lightautoml==0.3.8) (2.8.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->lightautoml==0.3.8) (3.2.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (21.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.12)\r\n",
      "Requirement already satisfied: hijri-converter in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (2.3.1)\r\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (0.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->lightautoml==0.3.8) (2.1.3)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (1.13.0)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (6.8.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (2.0.20)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->lightautoml==0.3.8) (1.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (4.42.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (10.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (3.0.9)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->lightautoml==0.3.8) (2.0.2)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost>=0.26.1->lightautoml==0.3.8) (8.2.3)\r\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.0)\r\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.2.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.1.3)\r\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_applehelp-1.0.7-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_devhelp-1.0.5-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_qthelp-1.0.6-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.16.1)\r\n",
      "Requirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (0.20.1)\r\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.2.0)\r\n",
      "Requirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.12.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/alabaster-0.7.13-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/imagesize-1.4.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.31.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (2023.11.17)\r\n",
      "Building wheels for collected packages: json2html\r\n",
      "  Building wheel for json2html (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7591 sha256=34fae2f9dba387db2b23c58a074c1c33416d47b740b5a75565ba4daeb95b2707\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/04/0d/34912ecabd9128a537a032c0fc15c6c46e734fb5fe3a14536c\r\n",
      "Successfully built json2html\r\n",
      "Installing collected packages: StrEnum, json2html, sphinxcontrib-jsmath, poetry-core, joblib, imagesize, cmaes, alabaster, pandas, lightgbm, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sphinx, autowoe, lightautoml\r\n",
      "  Attempting uninstall: joblib\r\n",
      "    Found existing installation: joblib 1.3.2\r\n",
      "    Uninstalling joblib-1.3.2:\r\n",
      "      Successfully uninstalled joblib-1.3.2\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.0.3\r\n",
      "    Uninstalling pandas-2.0.3:\r\n",
      "      Successfully uninstalled pandas-2.0.3\r\n",
      "  Attempting uninstall: lightgbm\r\n",
      "    Found existing installation: lightgbm 3.3.2\r\n",
      "    Uninstalling lightgbm-3.3.2:\r\n",
      "      Successfully uninstalled lightgbm-3.3.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "fitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\r\n",
      "fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed StrEnum-0.4.15 alabaster-0.7.13 autowoe-1.3.2 cmaes-0.10.0 imagesize-1.4.1 joblib-1.2.0 json2html-1.3.0 lightautoml-0.3.8 lightgbm-3.2.1 pandas-1.5.3 poetry-core-1.8.1 sphinx-7.2.6 sphinxcontrib-applehelp-1.0.7 sphinxcontrib-devhelp-1.0.5 sphinxcontrib-htmlhelp-2.0.4 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.6 sphinxcontrib-serializinghtml-1.1.9\r\n",
      "Looking in links: /kaggle/input/lightautoml-038-dependecies\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\r\n",
      "Installing collected packages: pandas\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 1.5.3\r\n",
      "    Uninstalling pandas-1.5.3:\r\n",
      "      Successfully uninstalled pandas-1.5.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "fitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "lightautoml 0.3.8 requires pandas<2.0.0, but you have pandas 2.0.3 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed pandas-2.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies lightautoml==0.3.8\n",
    "!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies pandas==2.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d6871",
   "metadata": {
    "papermill": {
     "duration": 0.021663,
     "end_time": "2024-01-09T08:08:48.102565",
     "exception": false,
     "start_time": "2024-01-09T08:08:48.080902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# My LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4758b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:08:48.148596Z",
     "iopub.status.busy": "2024-01-09T08:08:48.148208Z",
     "iopub.status.idle": "2024-01-09T08:09:23.292924Z",
     "shell.execute_reply": "2024-01-09T08:09:23.292088Z"
    },
    "papermill": {
     "duration": 35.170326,
     "end_time": "2024-01-09T08:09:23.295467",
     "exception": false,
     "start_time": "2024-01-09T08:08:48.125141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from itertools import combinations  \n",
    "from scipy.stats import skew\n",
    "import copy\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings  \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b267cbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:23.342633Z",
     "iopub.status.busy": "2024-01-09T08:09:23.341848Z",
     "iopub.status.idle": "2024-01-09T08:09:37.720857Z",
     "shell.execute_reply": "2024-01-09T08:09:37.719716Z"
    },
    "papermill": {
     "duration": 14.40523,
     "end_time": "2024-01-09T08:09:37.723505",
     "exception": false,
     "start_time": "2024-01-09T08:09:23.318275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "train_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\n",
    "train_essays.index = train_essays[\"Unnamed: 0\"]\n",
    "train_essays.index.name = None\n",
    "train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_essays_with_upper = pd.read_csv('/kaggle/input/essays-generator-with-upper/essays_with_upper.csv') \n",
    "train_essays_with_upper.index = train_essays_with_upper[\"Unnamed: 0\"]\n",
    "train_essays_with_upper.index.name = None\n",
    "train_essays_with_upper.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743e2007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:37.774786Z",
     "iopub.status.busy": "2024-01-09T08:09:37.774084Z",
     "iopub.status.idle": "2024-01-09T08:09:51.002198Z",
     "shell.execute_reply": "2024-01-09T08:09:51.001181Z"
    },
    "papermill": {
     "duration": 13.255986,
     "end_time": "2024-01-09T08:09:51.004639",
     "exception": false,
     "start_time": "2024-01-09T08:09:37.748653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/4222491666.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['event_id'] = df.groupby('id').cumcount()\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df,dataset='test'):\n",
    "    if dataset == 'train':\n",
    "        add_value = 66231-17831+500\n",
    "        df.loc[(df['id']=='a0c24719')&(df['event_id']>68),'down_time'] += add_value\n",
    "        df.loc[(df['id']=='a0c24719')&(df['event_id']>68),'up_time'] += add_value\n",
    "    for i in range(1,4):\n",
    "        df[f'down_event_shift{i}'] = df.groupby('id')['down_event'].shift(i)\n",
    "    df['need_drop'] = np.zeros(len(df))\n",
    "    df.loc[(df['down_event']=='Shift')&(df['down_event_shift1']=='Shift')&(df['down_event_shift2']=='Shift')&(df['down_event_shift3']=='Shift'),'need_drop'] = 1\n",
    "    df = df[df['need_drop']==0]\n",
    "    df['event_id'] = df.groupby('id').cumcount()\n",
    "    return df.drop(columns=['down_event_shift1','down_event_shift2','down_event_shift3','need_drop'])\n",
    "train_logs = preprocessing(train_logs,dataset='train')\n",
    "test_logs = preprocessing(test_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84077ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:51.051805Z",
     "iopub.status.busy": "2024-01-09T08:09:51.051239Z",
     "iopub.status.idle": "2024-01-09T08:09:51.082643Z",
     "shell.execute_reply": "2024-01-09T08:09:51.081733Z"
    },
    "papermill": {
     "duration": 0.057167,
     "end_time": "2024-01-09T08:09:51.084577",
     "exception": false,
     "start_time": "2024-01-09T08:09:51.027410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#essay生成：普通版本和带大写版本\n",
    "class EssayConstructor:\n",
    "    \n",
    "    def processingInputs(self,currTextInput):\n",
    "        # Where the essay content will be stored\n",
    "        essayText = \"\"\n",
    "        # Produces the essay\n",
    "        for Input in currTextInput.values:\n",
    "            # Input[0] = activity\n",
    "            # Input[1] = cursor_position\n",
    "            # Input[2] = text_change\n",
    "            # Input[3] = id\n",
    "            # If activity = Replace\n",
    "            if Input[0] == 'Replace':\n",
    "                # splits text_change at ' => '\n",
    "                replaceTxt = Input[2].split(' => ')\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Paste    \n",
    "            if Input[0] == 'Paste':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Remove/Cut\n",
    "            if Input[0] == 'Remove/Cut':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Move...\n",
    "            if \"M\" in Input[0]:\n",
    "                # Gets rid of the \"Move from to\" text\n",
    "                croppedTxt = Input[0][10:]              \n",
    "                # Splits cropped text by ' To '\n",
    "                splitTxt = croppedTxt.split(' To ')              \n",
    "                # Splits split text again by ', ' for each item\n",
    "                valueArr = [item.split(', ') for item in splitTxt]              \n",
    "                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "                # Skip if someone manages to activiate this by moving to same place\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    # Check if they move text forward in essay (they are different)\n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    else:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue                \n",
    "                \n",
    "            # If activity = input\n",
    "            # DONT TOUCH\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        return essayText\n",
    "            \n",
    "            \n",
    "    def getEssays(self,df):\n",
    "        # Copy required columns\n",
    "        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "        # Get rid of text inputs that make no change\n",
    "        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "        # construct essay, fast \n",
    "        tqdm.pandas()\n",
    "        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n",
    "        # to dataframe\n",
    "        essayFrame=essay.to_frame().reset_index()\n",
    "        essayFrame.columns=['id','essay']\n",
    "        # Returns the essay series\n",
    "        return essayFrame\n",
    "\n",
    "def getEssays_with_upper(df):\n",
    "    df['down_event_shift'] = df.groupby('id')['down_event'].shift(1)\n",
    "    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change','down_event','down_event_shift']]\n",
    "    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n",
    "    lastIndex = 0\n",
    "    essaySeries = pd.Series()\n",
    "    for index, valCount in enumerate(tqdm(valCountsArr)):\n",
    "        capital = False\n",
    "        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change','down_event','down_event_shift']].iloc[lastIndex : lastIndex + valCount]\n",
    "        lastIndex += valCount\n",
    "        essayText = \"\"\n",
    "        for Input in currTextInput.values:\n",
    "            if Input[3] == 'CapsLock':\n",
    "                capital = not capital\n",
    "            if Input[0] == 'Nonproduction':\n",
    "                continue\n",
    "            if Input[0] != 'Nonproduction':\n",
    "                if (Input[0] == 'Replace')&(Input[4] == 'Shift'):\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + (replaceTxt[1]).upper() +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Replace':\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Paste':\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                    continue\n",
    "                if Input[0] == 'Remove/Cut':\n",
    "                    essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                    continue\n",
    "                if \"M\" in Input[0]:\n",
    "                    croppedTxt = Input[0][10:]\n",
    "                    splitTxt = croppedTxt.split(' To ')\n",
    "                    valueArr = [item.split(', ') for item in splitTxt]\n",
    "                    moveData = (int(valueArr[0][0][1:]), \n",
    "                                int(valueArr[0][1][:-1]), \n",
    "                                int(valueArr[1][0][1:]), \n",
    "                                int(valueArr[1][1][:-1]))\n",
    "                    if moveData[0] != moveData[2]:\n",
    "                        if moveData[0] < moveData[2]:\n",
    "                            essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                            essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                        else:\n",
    "                            essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                            essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                    continue\n",
    "                if capital|((Input[4]=='Shift')&(Input[3]=='q')):\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2].upper() + essayText[Input[1] - len(Input[2]):]\n",
    "                else:\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        essaySeries[index] = essayText\n",
    "    essaySeries.index =  textInputDf['id'].unique()\n",
    "    return pd.DataFrame(essaySeries, columns=['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbb9a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:51.130976Z",
     "iopub.status.busy": "2024-01-09T08:09:51.130623Z",
     "iopub.status.idle": "2024-01-09T08:09:51.135414Z",
     "shell.execute_reply": "2024-01-09T08:09:51.134546Z"
    },
    "papermill": {
     "duration": 0.029889,
     "end_time": "2024-01-09T08:09:51.137279",
     "exception": false,
     "start_time": "2024-01-09T08:09:51.107390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#两个分位数\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b00410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:51.183527Z",
     "iopub.status.busy": "2024-01-09T08:09:51.183144Z",
     "iopub.status.idle": "2024-01-09T08:09:51.189175Z",
     "shell.execute_reply": "2024-01-09T08:09:51.188307Z"
    },
    "papermill": {
     "duration": 0.03161,
     "end_time": "2024-01-09T08:09:51.191044",
     "exception": false,
     "start_time": "2024-01-09T08:09:51.159434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(text):  \n",
    "    # 统计每个字符的出现次数  \n",
    "    char_count = {}  \n",
    "    for char in text:  \n",
    "        if char in char_count:  \n",
    "            char_count[char] += 1  \n",
    "        else:  \n",
    "            char_count[char] = 1  \n",
    "    probabilities = [float(char_count[char]) / len(text) for char in char_count]  \n",
    "    entropy = -sum([p * math.log2(p) for p in probabilities])  \n",
    "    return entropy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155dbab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:51.296035Z",
     "iopub.status.busy": "2024-01-09T08:09:51.295680Z",
     "iopub.status.idle": "2024-01-09T08:09:51.606580Z",
     "shell.execute_reply": "2024-01-09T08:09:51.605805Z"
    },
    "papermill": {
     "duration": 0.341994,
     "end_time": "2024-01-09T08:09:51.608865",
     "exception": false,
     "start_time": "2024-01-09T08:09:51.266871",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessor_v1:\n",
    "    \n",
    "    def __init__(self,seed,essays,essays_with_upper,train_scores=None,tokenizer=None,method='train',save_cols=None):\n",
    "        self.seed = seed\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_scores = train_scores\n",
    "        self.save_cols = save_cols\n",
    "        self.essays = essays\n",
    "        self.essays_with_upper = essays_with_upper\n",
    "        self.method =method \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/','@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        self.idf = defaultdict(float)\n",
    "        self.text_changes_dict = {\n",
    "            'q': 'q', \n",
    "            ' ': 'space', \n",
    "            'NoChange': 'NoChange', \n",
    "            '.': 'full_stop', \n",
    "            ',': 'comma', \n",
    "            '\\n': 'newline', \n",
    "            \"'\": 'single_quote', \n",
    "            '\"': 'double_quote', \n",
    "            '-': 'dash', \n",
    "            '?': 'question_mark', \n",
    "            ';': 'semicolon', \n",
    "            '=': 'equals', \n",
    "            '/': 'slash', \n",
    "            '\\\\': 'double_backslash', \n",
    "            ':': 'colon'\n",
    "        }\n",
    "        self.AGGREGATIONS =  ['nunique','count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "        self.AGGREGATIONS2 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS3 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS4 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS5 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "                \n",
    "            di[\"move_to\"] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "                else:\n",
    "                    di[\"move_to\"] += v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "    \n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def calculate_pauses(self, df, pause_threshold=2000):\n",
    "        # Compute IKI within each 'id' group\n",
    "        df['IKI'] = df.groupby('id')['down_time'].diff()\n",
    "\n",
    "        # Define pauses\n",
    "        df['is_pause'] = (df['IKI'] > pause_threshold)\n",
    "\n",
    "        # Compute statistics for IKI\n",
    "        iki_stats = df.groupby('id')['IKI'].agg(['mean', 'median', 'std', 'max']).reset_index().rename(columns={\n",
    "            'mean': 'iki_mean',\n",
    "            'median': 'iki_median',\n",
    "            'std': 'iki_std',\n",
    "            'max': 'iki_max'\n",
    "        })\n",
    "\n",
    "        # Compute pause counts\n",
    "        pause_counts = df.groupby('id')['is_pause'].sum().reset_index(name='pause_count')\n",
    "\n",
    "        # Compute average pause time excluding NaNs\n",
    "        pause_times = df[df['is_pause']].groupby('id')['IKI'].mean().reset_index(name='average_pause_time')\n",
    "\n",
    "        # Compute total pause time for paragraph\n",
    "        para_pause_duration = df.groupby('id').apply(lambda group: group['IKI'].where(group['text_change'] == '\\n').sum()).reset_index(name='para_pause_duration')\n",
    "\n",
    "        # Merge pause features\n",
    "        pause_features = pause_counts.merge(pause_times, on='id', how='left')\n",
    "        pause_features = pause_features.merge(para_pause_duration, on='id', how='left')\n",
    "        pause_features = pause_features.merge(iki_stats, on='id', how='left')\n",
    "\n",
    "        # Compute total IKI time and exclude NaNs\n",
    "        total_time = df.groupby('id')['IKI'].sum().reset_index(name='total_time')\n",
    "        \n",
    "        # Merge the total time into pause_features\n",
    "        pause_features = pause_features.merge(total_time, on='id', how='left')\n",
    "\n",
    "        # Calculate pause time ratio\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_count'] * pause_features['average_pause_time']\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_time_ratio'] / pause_features['total_time'].replace(0, np.nan)\n",
    "\n",
    "        # Calculate times between sentences within each 'id' group\n",
    "        df['sentence_end_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].isin(['.', '?', '!']))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for times between sentences\n",
    "        between_sentences_stats = df.groupby('id')['sentence_end_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_sentences_IKI', 'std': 'std_between_sentences_IKI'})\n",
    "\n",
    "        # Calculate within-word IKI for 'q' characters within each 'id'\n",
    "        df['within_word_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'] == 'q')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for within-word IKI\n",
    "        within_word_stats = df.groupby('id')['within_word_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_within_word_IKI', 'std': 'std_within_word_IKI'})\n",
    "\n",
    "        # Calculate between-words IKI for spaces or punctuation followed by 'q'\n",
    "        df['between_words_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].shift().isin([' '] + self.punctuations) & (group['text_change'] == 'q'))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for between-words IKI\n",
    "        between_words_stats = df.groupby('id')['between_words_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_words_IKI', 'std': 'std_between_words_IKI'})\n",
    "\n",
    "        # Combine all the IKI related features into one DataFrame\n",
    "        pause_features = pause_features.merge(between_sentences_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(within_word_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(between_words_stats, on='id', how='left')\n",
    "\n",
    "        return pause_features\n",
    "\n",
    "    def brute_force_agg(self,df):\n",
    "        #bruteforce agg\n",
    "        agg_fe_df = df.groupby(\"id\")[['down_time', 'cursor_position', 'word_count']].agg(\n",
    "            ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "        agg_fe_df.columns = ['_'.join(x) for x in agg_fe_df.columns]\n",
    "        agg_fe_df = agg_fe_df.add_prefix(\"tmp_\")\n",
    "        agg_fe_df.reset_index(inplace=True)\n",
    "        return agg_fe_df\n",
    "    \n",
    "    def duration_features(self,df):\n",
    "        logs = copy.deepcopy(df)\n",
    "        logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "        logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "        group = logs.groupby('id')['time_diff']\n",
    "        initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        data = pd.DataFrame({\n",
    "            'id': logs['id'].unique(),\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True)\n",
    "        return data\n",
    "    \n",
    "    def essay_CountVectorizer_and_tfidf(self):\n",
    "        if self.method=='train':\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            merged_data = essaysdf.merge(self.train_scores, on='id')\n",
    "            count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "            tokenizer = count_vectorizer.fit_transform(merged_data['essay'])\n",
    "            y = merged_data['score']\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]) : \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            \n",
    "            save_cols = []\n",
    "            for i in count_vector.columns:\n",
    "                if sum(count_vector[i]==0)/len(count_vector)<0.1:\n",
    "                    save_cols.append(i)\n",
    "\n",
    "            return count_vector[save_cols],count_vectorizer,save_cols\n",
    "\n",
    "        else:\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            tokenizer = self.tokenizer.transform(essaysdf['essay'])\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]): \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            return count_vector[self.save_cols]       \n",
    "        \n",
    "    def other_features(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        a['Input_all_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['activity']!='Input'))/df.groupby(['id']).apply(lambda x:sum(x['activity']=='Input'))\n",
    "        a['all_q_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['down_event']!='q'))/df.groupby(['id']).apply(lambda x:sum(x['down_event']=='q'))\n",
    "        activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        events_dict = {\n",
    "                        'q':'q', \n",
    "                        'Space':'Space', \n",
    "                        'Backspace':'Backspace', \n",
    "                        'Shift':'Shift', \n",
    "                        'ArrowRight':'ArrowRight', \n",
    "                        'Leftclick':'Leftclick', \n",
    "                        'ArrowLeft':'ArrowLeft', \n",
    "                        '.':'fullstop', \n",
    "                        ',':'comma', \n",
    "                        'ArrowDown':'ArrowDown', \n",
    "                        'ArrowUp':'ArrowUp', \n",
    "                        'Enter':'Enter', \n",
    "                        'CapsLock':'CapsLock', \n",
    "                        \"'\":'single_quote', \n",
    "                        'Delete':'Delete', \n",
    "                        'Unidentified':'Unidentified',\n",
    "                      }\n",
    "        for i in tqdm(activities):\n",
    "            for j in events_dict:\n",
    "                a[f'{i}_{events_dict[j]}_count'] = df.groupby('id').apply(lambda x:len(x[(x['activity']==i)&(x['down_event']==j)]))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df['down_event_shift'] = df.groupby('id')['down_event'].shift(-1)\n",
    "        letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')|((x['down_event']=='Shift')&(x['down_event_shift']=='q'))]))\n",
    "        a['letter_big_count'] = letter_upper.values\n",
    "        a['id'] = df['id'].unique()\n",
    "\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df['sent_len']!=0]\n",
    "        errors_num = (essay_df.groupby('id').apply(len)-letter_upper).values\n",
    "        a['error_num'] = errors_num                          #如果句子个数大于大写字母按键次数，那么文章会有语法错误\n",
    "\n",
    "        return a \n",
    "\n",
    "    def sentence_error(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        essay_df = essay_df[essay_df['paragraph_len']!=0]\n",
    "        essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        essay_df = essay_df[essay_df['only_space']==False]\n",
    "        a = pd.DataFrame()\n",
    "        a['para_error'] = essay_df.groupby('id').apply(lambda x:len(x[x['paragraph_len']<25]))   #一个段落字符过少可能不是完整的一句话，可能存在语法错误\n",
    "\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error_letter(self):\n",
    "        essay_df = copy.deepcopy(self.essays_with_upper)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: str(x).replace('\\n','').strip()) \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        essay_df['language_error_letter'] = essay_df['sent'].apply(lambda x:x[0])\n",
    "        essay_df['if_q'] = essay_df['language_error_letter'].apply(lambda x:x.lower()=='q')\n",
    "        essay_df = essay_df[essay_df['if_q']==True]\n",
    "        a = pd.DataFrame()\n",
    "        a['language_error_letter'] = essay_df.groupby('id').apply(lambda x:len(x[x['language_error_letter']=='q']))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def R_burst(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df = df[(df['activity']=='Input')|(df['activity']=='Remove/Cut')].reset_index(drop=True)\n",
    "        df['activity_shift'] = df.groupby('id')['activity'].shift().fillna(method='bfill')\n",
    "        df['is_R_burst'] = df['activity'] != df['activity_shift']\n",
    "        a['revision_count'] = df.groupby('id').apply(lambda x:x['is_R_burst'].sum())\n",
    "        df['keystroke_duration'] = df.groupby('id')['down_time'].diff()\n",
    "        df = df[df['keystroke_duration'].notnull()]\n",
    "\n",
    "        a['revision_count_above2s'] = df.groupby('id').apply(lambda x:x[(x['is_R_burst']==True)&(x['keystroke_duration']>2)]['is_R_burst'].sum()).values\n",
    "        Rburst =  df[(df['is_R_burst']==True)&(df['keystroke_duration']>2)]   #&(df['keystroke_duration']>2)\n",
    "        Rburst_statistic = Rburst.groupby('id').agg({'keystroke_duration':['mean','max','sum','median']})\n",
    "        Rburst_statistic.columns = ['_'.join(x) for x in Rburst_statistic.columns]\n",
    "        \n",
    "        return a.merge(Rburst_statistic.reset_index(),on='id',how='left')\n",
    "\n",
    "\n",
    "    def split_essays_into_sentences(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "\n",
    "        # Number of words in sentences\n",
    "        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['sent_word_count_diff'] = essay_df.groupby(['id'])['sent_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "        essay_df['words_len_above10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_above10'] = essay_df['words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['words_len_5-10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_5-10'] = essay_df['words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['words_len_first'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_first'] = essay_df['words_len_first'].apply(lambda x:len(x[0]))\n",
    "\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "    \n",
    "    def compute_sentence_aggregations(self,df):\n",
    "        sent_agg_df = pd.concat(\n",
    "            [df[['id','sent_len']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ],\n",
    "             axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index    \n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "\n",
    "    def split_essays_into_paragraphs(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        \n",
    "        # Number of words in paragraphs\n",
    "        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['paragraph_word_count_diff'] = essay_df.groupby(['id'])['paragraph_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "\n",
    "        essay_df['para_words_len_above10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_above10'] = essay_df['para_words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_5-10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_5-10'] = essay_df['para_words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_first'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_first'] = essay_df['para_words_len_first'].apply(lambda x:len(x[0]))\n",
    "        \n",
    "        essay_df['num_question'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\?', x)))\n",
    "        essay_df['num_yinyong'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\\"', x)))\n",
    "\n",
    "        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "        #有些段落可能全部是空格，类似于：'    '\n",
    "        #essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        #essay_df = essay_df[essay_df['only_space']==False]\n",
    "        return essay_df\n",
    "\n",
    "    def compute_paragraph_aggregations(self,df):\n",
    "        paragraph_agg_df = pd.concat(\n",
    "            [df[['id','paragraph_len']].groupby(['id']).agg(self.AGGREGATIONS),\\\n",
    "             df[['id','paragraph_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','paragraph_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','para_words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','para_words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','para_words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_question']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_yinyong']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ], axis=1) \n",
    "        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "        paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "        paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "        paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "        return paragraph_agg_df\n",
    "    \n",
    "    def difficulty(self):\n",
    "        df = copy.deepcopy(self.essays)\n",
    "        df['token'] = [word_tokenize(p) for p in df[\"essay\"]]\n",
    "        df['token_len'] = df['token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df['verylong']  = df['token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df['long']      = df['token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df['mid']       = df['token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df['difficulty'] = df['verylong']*5 + df['long']*3 + df['mid']*1\n",
    "        df['long_words'] = df['verylong']+df['long']\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index':'id'},inplace=True)\n",
    "\n",
    "        #sentence\n",
    "        df_sentence = copy.deepcopy(self.essays)\n",
    "        df_sentence['id'] = df_sentence.index\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        df_sentence['essay'] = df_sentence['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        df_sentence['sent'] = df_sentence['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        df_sentence = df_sentence.explode('sent')   #explode将列表里的元素展开\n",
    "        df_sentence['sent'] = df_sentence['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        df_sentence['sent_len'] = df_sentence['sent'].apply(lambda x: len(x))\n",
    "        df_sentence['sent_word_count'] = df_sentence['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df_sentence = df_sentence[df_sentence['sent_len']!=0]\n",
    "\n",
    "        df_sentence['sentence_token'] = [word_tokenize(p) for p in df_sentence[\"sent\"]]\n",
    "        df_sentence['sentence_token_len'] = df_sentence['sentence_token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df_sentence['sentence_verylong']  = df_sentence['sentence_token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df_sentence['sentence_long']      = df_sentence['sentence_token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df_sentence['sentence_mid']       = df_sentence['sentence_token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df_sentence['sentence_difficulty'] = df_sentence['sentence_verylong']*5 + df_sentence['sentence_long']*3 + df_sentence['sentence_mid']*1\n",
    "        df_sentence['sentence_long_words'] = df_sentence['sentence_verylong']+df_sentence['sentence_long']\n",
    "        a = df_sentence.groupby('id')[['sentence_verylong','sentence_long','sentence_mid','sentence_difficulty','sentence_long_words']].agg(['max','mean','sum'])\n",
    "        a.columns = ['_'.join(x) for x in a.columns]\n",
    "\n",
    "        return (df[['id','verylong','long','mid','difficulty','long_words']]).merge(a,on='id',how='left')\n",
    "    \n",
    "    def entropy(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df['complexity'] = essay_df['sent'].apply(lambda x:calculate_entropy(x))\n",
    "        a = essay_df.groupby('id').agg({'complexity':['max','mean','std','sum','median']})\n",
    "        a.columns = ['_'.join(i) for i in a.columns]\n",
    "        return a.reset_index()\n",
    "    \n",
    "    def make_feats(self, df):\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "            \n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)        \n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Calculating pause features\")\n",
    "        tmp_df = self.calculate_pauses(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge brute force agg.>')\n",
    "        tmp_df = self.brute_force_agg(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
    "        feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n",
    "\n",
    "        print('<merge duration_features.>')\n",
    "        tmp_df = self.duration_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            feats = feats.merge(self.train_scores, on='id', how='left')\n",
    "        \n",
    "        print('<merge countvectorizer_and_tfidf_features.>')\n",
    "        if self.method == 'train':\n",
    "            tmp_df,tokenizer,save_cols = self.essay_CountVectorizer_and_tfidf()\n",
    "        else:\n",
    "            tmp_df = self.essay_CountVectorizer_and_tfidf()\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge other features.>')\n",
    "        if self.method == 'train':\n",
    "            if os.path.exists('/kaggle/input/lgbm-and-nn-on-sentences'):  \n",
    "                tmp_df = pd.read_csv('/kaggle/input/lgbm-and-nn-on-sentences/train_agg_ratio.csv')\n",
    "            else:\n",
    "                tmp_df = self.other_features(df)\n",
    "        else:\n",
    "            tmp_df = self.other_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge errors features.>')\n",
    "        tmp_df = self.language_error(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        tmp_df = self.sentence_error()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        else:\n",
    "            essays_upper = getEssays_with_upper(df)\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('merge sentence and paragraph agg features')\n",
    "        sent_df = self.split_essays_into_sentences()\n",
    "        tmp_df = self.compute_sentence_aggregations(sent_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        paragraph_df = self.split_essays_into_paragraphs()\n",
    "        tmp_df = self.compute_paragraph_aggregations(paragraph_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge R burst features')\n",
    "        tmp_df = self.R_burst(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge difficulty agg features')\n",
    "        tmp_df = self.difficulty()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        #print('merge sentence entropy features')\n",
    "        #tmp_df = self.entropy()\n",
    "        #feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        if self.method == 'train':\n",
    "            return feats,tokenizer,save_cols\n",
    "        else:\n",
    "            return feats\n",
    "        \n",
    "        \n",
    "\n",
    "class Preprocessor_v2:\n",
    "    \n",
    "    def __init__(self,seed,essays,essays_with_upper,train_scores=None,tokenizer=None,method='train',save_cols=None):\n",
    "        self.seed = seed\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_scores = train_scores\n",
    "        self.save_cols = save_cols\n",
    "        self.essays = essays\n",
    "        self.essays_with_upper = essays_with_upper\n",
    "        self.method =method \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        self.idf = defaultdict(float)\n",
    "        self.text_changes_dict = {\n",
    "            'q': 'q', \n",
    "            ' ': 'space', \n",
    "            'NoChange': 'NoChange', \n",
    "            '.': 'full_stop', \n",
    "            ',': 'comma', \n",
    "            '\\n': 'newline', \n",
    "            \"'\": 'single_quote', \n",
    "            '\"': 'double_quote', \n",
    "            '-': 'dash', \n",
    "            '?': 'question_mark', \n",
    "            ';': 'semicolon', \n",
    "            '=': 'equals', \n",
    "            '/': 'slash', \n",
    "            '\\\\': 'double_backslash', \n",
    "            ':': 'colon'\n",
    "        }\n",
    "        self.AGGREGATIONS =  ['nunique','count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "        self.AGGREGATIONS2 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS3 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS4 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS5 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "                \n",
    "            di[\"move_to\"] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "                else:\n",
    "                    di[\"move_to\"] += v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "    \n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def calculate_pauses(self, df, pause_threshold=2000):\n",
    "        # Compute IKI within each 'id' group\n",
    "        df['IKI'] = df.groupby('id')['down_time'].diff()\n",
    "\n",
    "        # Define pauses\n",
    "        df['is_pause'] = (df['IKI'] > pause_threshold)\n",
    "\n",
    "        # Compute statistics for IKI\n",
    "        iki_stats = df.groupby('id')['IKI'].agg(['mean', 'median', 'std', 'max']).reset_index().rename(columns={\n",
    "            'mean': 'iki_mean',\n",
    "            'median': 'iki_median',\n",
    "            'std': 'iki_std',\n",
    "            'max': 'iki_max'\n",
    "        })\n",
    "\n",
    "        # Compute pause counts\n",
    "        pause_counts = df.groupby('id')['is_pause'].sum().reset_index(name='pause_count')\n",
    "\n",
    "        # Compute average pause time excluding NaNs\n",
    "        pause_times = df[df['is_pause']].groupby('id')['IKI'].mean().reset_index(name='average_pause_time')\n",
    "\n",
    "        # Compute total pause time for paragraph\n",
    "        para_pause_duration = df.groupby('id').apply(lambda group: group['IKI'].where(group['text_change'] == '\\n').sum()).reset_index(name='para_pause_duration')\n",
    "\n",
    "        # Merge pause features\n",
    "        pause_features = pause_counts.merge(pause_times, on='id', how='left')\n",
    "        pause_features = pause_features.merge(para_pause_duration, on='id', how='left')\n",
    "        pause_features = pause_features.merge(iki_stats, on='id', how='left')\n",
    "\n",
    "        # Compute total IKI time and exclude NaNs\n",
    "        total_time = df.groupby('id')['IKI'].sum().reset_index(name='total_time')\n",
    "        \n",
    "        # Merge the total time into pause_features\n",
    "        pause_features = pause_features.merge(total_time, on='id', how='left')\n",
    "\n",
    "        # Calculate pause time ratio\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_count'] * pause_features['average_pause_time']\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_time_ratio'] / pause_features['total_time'].replace(0, np.nan)\n",
    "\n",
    "        # Calculate times between sentences within each 'id' group\n",
    "        df['sentence_end_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].isin(['.', '?', '!']))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for times between sentences\n",
    "        between_sentences_stats = df.groupby('id')['sentence_end_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_sentences_IKI', 'std': 'std_between_sentences_IKI'})\n",
    "\n",
    "        # Calculate within-word IKI for 'q' characters within each 'id'\n",
    "        df['within_word_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'] == 'q')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for within-word IKI\n",
    "        within_word_stats = df.groupby('id')['within_word_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_within_word_IKI', 'std': 'std_within_word_IKI'})\n",
    "\n",
    "        # Calculate between-words IKI for spaces or punctuation followed by 'q'\n",
    "        df['between_words_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].shift().isin([' '] + self.punctuations) & (group['text_change'] == 'q'))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for between-words IKI\n",
    "        between_words_stats = df.groupby('id')['between_words_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_words_IKI', 'std': 'std_between_words_IKI'})\n",
    "\n",
    "        # Combine all the IKI related features into one DataFrame\n",
    "        pause_features = pause_features.merge(between_sentences_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(within_word_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(between_words_stats, on='id', how='left')\n",
    "\n",
    "        return pause_features\n",
    "\n",
    "    def brute_force_agg(self,df):\n",
    "        #bruteforce agg\n",
    "        agg_fe_df = df.groupby(\"id\")[['down_time', 'cursor_position', 'word_count']].agg(\n",
    "            ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "        agg_fe_df.columns = ['_'.join(x) for x in agg_fe_df.columns]\n",
    "        agg_fe_df = agg_fe_df.add_prefix(\"tmp_\")\n",
    "        agg_fe_df.reset_index(inplace=True)\n",
    "        return agg_fe_df\n",
    "    \n",
    "    def duration_features(self,df):\n",
    "        logs = copy.deepcopy(df)\n",
    "        logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "        logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "        group = logs.groupby('id')['time_diff']\n",
    "        initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        data = pd.DataFrame({\n",
    "            'id': logs['id'].unique(),\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True)\n",
    "        return data\n",
    "    \n",
    "    def essay_CountVectorizer_and_tfidf(self):\n",
    "        if self.method=='train':\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            merged_data = essaysdf.merge(self.train_scores, on='id')\n",
    "            count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "            tokenizer = count_vectorizer.fit_transform(merged_data['essay'])\n",
    "            y = merged_data['score']\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]) : \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            \n",
    "            save_cols = []\n",
    "            for i in count_vector.columns:\n",
    "                if sum(count_vector[i]==0)/len(count_vector)<0.1:\n",
    "                    save_cols.append(i)\n",
    "            return count_vector[save_cols],count_vectorizer,save_cols\n",
    "\n",
    "        else:\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            tokenizer = self.tokenizer.transform(essaysdf['essay'])\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]): \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            return count_vector[self.save_cols]       \n",
    "        \n",
    "    def other_features(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        a['Input_all_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['activity']!='Input'))/df.groupby(['id']).apply(lambda x:sum(x['activity']=='Input'))\n",
    "        a['all_q_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['down_event']!='q'))/df.groupby(['id']).apply(lambda x:sum(x['down_event']=='q'))\n",
    "        activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        events_dict = {\n",
    "                        'q':'q', \n",
    "                        'Space':'Space', \n",
    "                        'Backspace':'Backspace', \n",
    "                        'Shift':'Shift', \n",
    "                        'ArrowRight':'ArrowRight', \n",
    "                        'Leftclick':'Leftclick', \n",
    "                        'ArrowLeft':'ArrowLeft', \n",
    "                        '.':'fullstop', \n",
    "                        ',':'comma', \n",
    "                        'ArrowDown':'ArrowDown', \n",
    "                        'ArrowUp':'ArrowUp', \n",
    "                        'Enter':'Enter', \n",
    "                        'CapsLock':'CapsLock', \n",
    "                        \"'\":'single_quote', \n",
    "                        'Delete':'Delete', \n",
    "                        'Unidentified':'Unidentified',\n",
    "                      }\n",
    "        for i in tqdm(activities):\n",
    "            for j in events_dict:\n",
    "                a[f'{i}_{events_dict[j]}_count'] = df.groupby('id').apply(lambda x:len(x[(x['activity']==i)&(x['down_event']==j)]))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df['down_event_shift'] = df.groupby('id')['down_event'].shift(-1)\n",
    "        letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')|((x['down_event']=='Shift')&(x['down_event_shift']=='q'))]))\n",
    "        a['letter_big_count'] = letter_upper.values\n",
    "        a['id'] = df['id'].unique()\n",
    "\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df['sent_len']!=0]\n",
    "        errors_num = (essay_df.groupby('id').apply(len)-letter_upper).values\n",
    "        a['error_num'] = errors_num                          #如果句子个数大于大写字母按键次数，那么文章会有语法错误\n",
    "\n",
    "        return a \n",
    "\n",
    "    def sentence_error(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        essay_df = essay_df[essay_df['paragraph_len']!=0]\n",
    "        essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        essay_df = essay_df[essay_df['only_space']==False]\n",
    "        a = pd.DataFrame()\n",
    "        a['para_error'] = essay_df.groupby('id').apply(lambda x:len(x[x['paragraph_len']<25]))   #一个段落字符过少可能不是完整的一句话，可能存在语法错误\n",
    "\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error_letter(self):\n",
    "        essay_df = copy.deepcopy(self.essays_with_upper)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: str(x).replace('\\n','').strip()) \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        essay_df['language_error_letter'] = essay_df['sent'].apply(lambda x:x[0])\n",
    "        essay_df['if_q'] = essay_df['language_error_letter'].apply(lambda x:x.lower()=='q')\n",
    "        essay_df = essay_df[essay_df['if_q']==True]\n",
    "        a = pd.DataFrame()\n",
    "        a['language_error_letter'] = essay_df.groupby('id').apply(lambda x:len(x[x['language_error_letter']=='q']))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def R_burst(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df = df[(df['activity']=='Input')|(df['activity']=='Remove/Cut')].reset_index(drop=True)\n",
    "        df['activity_shift'] = df.groupby('id')['activity'].shift().fillna(method='bfill')\n",
    "        df['is_R_burst'] = df['activity'] != df['activity_shift']\n",
    "        a['revision_count'] = df.groupby('id').apply(lambda x:x['is_R_burst'].sum())\n",
    "        df['keystroke_duration'] = df.groupby('id')['down_time'].diff()\n",
    "        df = df[df['keystroke_duration'].notnull()]\n",
    "\n",
    "        a['revision_count_above2s'] = df.groupby('id').apply(lambda x:x[(x['is_R_burst']==True)&(x['keystroke_duration']>2)]['is_R_burst'].sum()).values\n",
    "        Rburst =  df[(df['is_R_burst']==True)&(df['keystroke_duration']>2)]   #&(df['keystroke_duration']>2)\n",
    "        Rburst_statistic = Rburst.groupby('id').agg({'keystroke_duration':['mean','max','sum','median']})\n",
    "        Rburst_statistic.columns = ['_'.join(x) for x in Rburst_statistic.columns]\n",
    "        \n",
    "        return a.merge(Rburst_statistic.reset_index(),on='id',how='left')\n",
    "\n",
    "\n",
    "    def split_essays_into_sentences(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "\n",
    "        # Number of words in sentences\n",
    "        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['sent_word_count_diff'] = essay_df.groupby(['id'])['sent_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "        essay_df['words_len_above10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_above10'] = essay_df['words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['words_len_5-10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_5-10'] = essay_df['words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['words_len_first'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_first'] = essay_df['words_len_first'].apply(lambda x:len(x[0]))\n",
    "\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "    \n",
    "    def compute_sentence_aggregations(self,df):\n",
    "        sent_agg_df = pd.concat(\n",
    "            [df[['id','sent_len']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ],\n",
    "             axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index    \n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "\n",
    "    def split_essays_into_paragraphs(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        \n",
    "        # Number of words in paragraphs\n",
    "        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['paragraph_word_count_diff'] = essay_df.groupby(['id'])['paragraph_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "\n",
    "        essay_df['para_words_len_above10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_above10'] = essay_df['para_words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_5-10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_5-10'] = essay_df['para_words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_first'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_first'] = essay_df['para_words_len_first'].apply(lambda x:len(x[0]))\n",
    "        \n",
    "        essay_df['num_question'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\?', x)))\n",
    "        essay_df['num_yinyong'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\\"', x)))\n",
    "\n",
    "        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "        #有些段落可能全部是空格，类似于：'    '\n",
    "        #essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        #essay_df = essay_df[essay_df['only_space']==False]\n",
    "        return essay_df\n",
    "\n",
    "    def compute_paragraph_aggregations(self,df):\n",
    "        paragraph_agg_df = pd.concat(\n",
    "            [df[['id','paragraph_len']].groupby(['id']).agg(self.AGGREGATIONS),\\\n",
    "             df[['id','paragraph_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','paragraph_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','para_words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','para_words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','para_words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_question']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_yinyong']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ], axis=1) \n",
    "        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "        paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "        paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "        paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "        return paragraph_agg_df\n",
    "    \n",
    "    def difficulty(self):\n",
    "        df = copy.deepcopy(self.essays)\n",
    "        df['token'] = [word_tokenize(p) for p in df[\"essay\"]]\n",
    "        df['token_len'] = df['token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df['verylong']  = df['token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df['long']      = df['token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df['mid']       = df['token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df['difficulty'] = df['verylong']*5 + df['long']*3 + df['mid']*1\n",
    "        df['long_words'] = df['verylong']+df['long']\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index':'id'},inplace=True)\n",
    "\n",
    "        #sentence\n",
    "        df_sentence = copy.deepcopy(self.essays)\n",
    "        df_sentence['id'] = df_sentence.index\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        df_sentence['essay'] = df_sentence['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        df_sentence['sent'] = df_sentence['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        df_sentence = df_sentence.explode('sent')   #explode将列表里的元素展开\n",
    "        df_sentence['sent'] = df_sentence['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        df_sentence['sent_len'] = df_sentence['sent'].apply(lambda x: len(x))\n",
    "        df_sentence['sent_word_count'] = df_sentence['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df_sentence = df_sentence[df_sentence['sent_len']!=0]\n",
    "\n",
    "        df_sentence['sentence_token'] = [word_tokenize(p) for p in df_sentence[\"sent\"]]\n",
    "        df_sentence['sentence_token_len'] = df_sentence['sentence_token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df_sentence['sentence_verylong']  = df_sentence['sentence_token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df_sentence['sentence_long']      = df_sentence['sentence_token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df_sentence['sentence_mid']       = df_sentence['sentence_token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df_sentence['sentence_difficulty'] = df_sentence['sentence_verylong']*5 + df_sentence['sentence_long']*3 + df_sentence['sentence_mid']*1\n",
    "        df_sentence['sentence_long_words'] = df_sentence['sentence_verylong']+df_sentence['sentence_long']\n",
    "        a = df_sentence.groupby('id')[['sentence_verylong','sentence_long','sentence_mid','sentence_difficulty','sentence_long_words']].agg(['max','mean','sum'])\n",
    "        a.columns = ['_'.join(x) for x in a.columns]\n",
    "\n",
    "        return (df[['id','verylong','long','mid','difficulty','long_words']]).merge(a,on='id',how='left')\n",
    "    \n",
    "    def make_feats(self, df):\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)        \n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Calculating pause features\")\n",
    "        tmp_df = self.calculate_pauses(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge brute force agg.>')\n",
    "        tmp_df = self.brute_force_agg(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
    "        feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n",
    "\n",
    "        print('<merge duration_features.>')\n",
    "        tmp_df = self.duration_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            feats = feats.merge(self.train_scores, on='id', how='left')\n",
    "        \n",
    "        print('<merge countvectorizer_and_tfidf_features.>')\n",
    "        if self.method == 'train':\n",
    "            tmp_df,tokenizer,save_cols = self.essay_CountVectorizer_and_tfidf()\n",
    "        else:\n",
    "            tmp_df = self.essay_CountVectorizer_and_tfidf()\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge other features.>')\n",
    "        if self.method == 'train':\n",
    "            if os.path.exists('/kaggle/input/lgbm-and-nn-on-sentences'):  \n",
    "                tmp_df = pd.read_csv('/kaggle/input/lgbm-and-nn-on-sentences/train_agg_ratio.csv')\n",
    "            else:\n",
    "                tmp_df = self.other_features(df)\n",
    "        else:\n",
    "            tmp_df = self.other_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge errors features.>')\n",
    "        tmp_df = self.language_error(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        tmp_df = self.sentence_error()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        else:\n",
    "            essays_upper = getEssays_with_upper(df)\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('merge sentence and paragraph agg features')\n",
    "        sent_df = self.split_essays_into_sentences()\n",
    "        tmp_df = self.compute_sentence_aggregations(sent_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        paragraph_df = self.split_essays_into_paragraphs()\n",
    "        tmp_df = self.compute_paragraph_aggregations(paragraph_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge R burst features')\n",
    "        tmp_df = self.R_burst(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge difficulty agg features')\n",
    "        tmp_df = self.difficulty()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        #print('merge sentence entropy features')\n",
    "        #tmp_df = self.entropy()\n",
    "        #feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        if self.method == 'train':\n",
    "            return feats,tokenizer,save_cols\n",
    "        else:\n",
    "            return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7677c2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:09:51.655084Z",
     "iopub.status.busy": "2024-01-09T08:09:51.654735Z",
     "iopub.status.idle": "2024-01-09T08:17:02.033101Z",
     "shell.execute_reply": "2024-01-09T08:17:02.032133Z"
    },
    "papermill": {
     "duration": 430.403657,
     "end_time": "2024-01-09T08:17:02.035318",
     "exception": false,
     "start_time": "2024-01-09T08:09:51.631661",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [03:14<00:00,  6.72s/it, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5822.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5648.13it/s]\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 5696.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5462.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5678.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n",
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:635: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:636: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_25/506724820.py:637: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:638: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:276: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:278: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n",
      "<merge errors features.>\n",
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1966.69it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 709.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 20.19it/s, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 18669.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22509.68it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 23876.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22231.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 26434.69it/s]\n",
      "/tmp/ipykernel_25/506724820.py:635: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:636: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_25/506724820.py:637: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:638: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n",
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n",
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:298: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge errors features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 589.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    }
   ],
   "source": [
    "#LGBM V1 train_feats and test_feats\n",
    "preprocessor = Preprocessor_v1(42,train_essays,train_essays_with_upper,train_scores = train_scores)\n",
    "train_feats,tokenizer,save_cols = preprocessor.make_feats(train_logs)\n",
    "\n",
    "test_essays = EssayConstructor().getEssays(test_logs)\n",
    "test_essays.set_index('id',inplace=True)\n",
    "test_essays.index.name = None\n",
    "test_essays_with_upper = getEssays_with_upper(test_logs)\n",
    "preprocessor = Preprocessor_v1(42,test_essays,test_essays_with_upper,tokenizer=tokenizer,method='test',save_cols=save_cols)\n",
    "test_feats = preprocessor.make_feats(test_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87efff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:17:02.252315Z",
     "iopub.status.busy": "2024-01-09T08:17:02.251399Z",
     "iopub.status.idle": "2024-01-09T08:17:02.272470Z",
     "shell.execute_reply": "2024-01-09T08:17:02.271661Z"
    },
    "papermill": {
     "duration": 0.137115,
     "end_time": "2024-01-09T08:17:02.274664",
     "exception": false,
     "start_time": "2024-01-09T08:17:02.137549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "f_read = open('/kaggle/input/select-features/feats_dict.pkl', 'rb')\n",
    "lgb_cols_v1 = pickle.load(f_read)\n",
    "f_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0410913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:17:02.497677Z",
     "iopub.status.busy": "2024-01-09T08:17:02.497280Z",
     "iopub.status.idle": "2024-01-09T08:21:46.724091Z",
     "shell.execute_reply": "2024-01-09T08:21:46.723226Z"
    },
    "papermill": {
     "duration": 284.346567,
     "end_time": "2024-01-09T08:21:46.726432",
     "exception": false,
     "start_time": "2024-01-09T08:17:02.379865",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.570975\n",
      "[200]\tvalid_0's rmse: 0.552935\n",
      "[300]\tvalid_0's rmse: 0.552601\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.572846\n",
      "[200]\tvalid_0's rmse: 0.557329\n",
      "[300]\tvalid_0's rmse: 0.559589\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.67257\n",
      "[200]\tvalid_0's rmse: 0.627245\n",
      "[300]\tvalid_0's rmse: 0.621372\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.668965\n",
      "[200]\tvalid_0's rmse: 0.626455\n",
      "[300]\tvalid_0's rmse: 0.618569\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.623739\n",
      "[200]\tvalid_0's rmse: 0.587749\n",
      "[300]\tvalid_0's rmse: 0.584579\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.621602\n",
      "[200]\tvalid_0's rmse: 0.587819\n",
      "[300]\tvalid_0's rmse: 0.585662\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.60401\n",
      "[200]\tvalid_0's rmse: 0.590277\n",
      "[300]\tvalid_0's rmse: 0.589767\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.605328\n",
      "[200]\tvalid_0's rmse: 0.587523\n",
      "[300]\tvalid_0's rmse: 0.587333\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.646657\n",
      "[200]\tvalid_0's rmse: 0.608414\n",
      "[300]\tvalid_0's rmse: 0.606198\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.64646\n",
      "[200]\tvalid_0's rmse: 0.606672\n",
      "[300]\tvalid_0's rmse: 0.604759\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.648742\n",
      "[200]\tvalid_0's rmse: 0.621252\n",
      "[300]\tvalid_0's rmse: 0.62079\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.646954\n",
      "[200]\tvalid_0's rmse: 0.619585\n",
      "[300]\tvalid_0's rmse: 0.620479\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.615423\n",
      "[200]\tvalid_0's rmse: 0.579141\n",
      "[300]\tvalid_0's rmse: 0.575838\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.616544\n",
      "[200]\tvalid_0's rmse: 0.580476\n",
      "[300]\tvalid_0's rmse: 0.57597\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.646838\n",
      "[200]\tvalid_0's rmse: 0.606019\n",
      "[300]\tvalid_0's rmse: 0.604807\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.649041\n",
      "[200]\tvalid_0's rmse: 0.608434\n",
      "[300]\tvalid_0's rmse: 0.606624\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.651467\n",
      "[200]\tvalid_0's rmse: 0.613145\n",
      "[300]\tvalid_0's rmse: 0.608113\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.653474\n",
      "[200]\tvalid_0's rmse: 0.617215\n",
      "[300]\tvalid_0's rmse: 0.611796\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.707349\n",
      "[200]\tvalid_0's rmse: 0.678773\n",
      "[300]\tvalid_0's rmse: 0.679418\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.706558\n",
      "[200]\tvalid_0's rmse: 0.680743\n",
      "[300]\tvalid_0's rmse: 0.681564\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.692338\n",
      "[200]\tvalid_0's rmse: 0.673204\n",
      "[300]\tvalid_0's rmse: 0.673405\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.690108\n",
      "[200]\tvalid_0's rmse: 0.671406\n",
      "[300]\tvalid_0's rmse: 0.669635\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.651901\n",
      "[200]\tvalid_0's rmse: 0.616659\n",
      "[300]\tvalid_0's rmse: 0.616848\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.652432\n",
      "[200]\tvalid_0's rmse: 0.61552\n",
      "[300]\tvalid_0's rmse: 0.610947\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.621573\n",
      "[200]\tvalid_0's rmse: 0.594071\n",
      "[300]\tvalid_0's rmse: 0.592593\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.626593\n",
      "[200]\tvalid_0's rmse: 0.597523\n",
      "[300]\tvalid_0's rmse: 0.594228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.621565\n",
      "[200]\tvalid_0's rmse: 0.588881\n",
      "[300]\tvalid_0's rmse: 0.587644\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.621555\n",
      "[200]\tvalid_0's rmse: 0.586958\n",
      "[300]\tvalid_0's rmse: 0.586769\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.68989\n",
      "[200]\tvalid_0's rmse: 0.645504\n",
      "[300]\tvalid_0's rmse: 0.641093\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.691634\n",
      "[200]\tvalid_0's rmse: 0.646885\n",
      "[300]\tvalid_0's rmse: 0.644543\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.601666\n",
      "[200]\tvalid_0's rmse: 0.590915\n",
      "[300]\tvalid_0's rmse: 0.592673\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.601176\n",
      "[200]\tvalid_0's rmse: 0.590934\n",
      "[300]\tvalid_0's rmse: 0.594383\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.660201\n",
      "[200]\tvalid_0's rmse: 0.626077\n",
      "[300]\tvalid_0's rmse: 0.624401\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.655502\n",
      "[200]\tvalid_0's rmse: 0.621225\n",
      "[300]\tvalid_0's rmse: 0.615696\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.578436\n",
      "[200]\tvalid_0's rmse: 0.548475\n",
      "[300]\tvalid_0's rmse: 0.544936\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.584181\n",
      "[200]\tvalid_0's rmse: 0.55042\n",
      "[300]\tvalid_0's rmse: 0.547841\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.648903\n",
      "[200]\tvalid_0's rmse: 0.601994\n",
      "[300]\tvalid_0's rmse: 0.5931\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.644351\n",
      "[200]\tvalid_0's rmse: 0.596937\n",
      "[300]\tvalid_0's rmse: 0.586514\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.614456\n",
      "[200]\tvalid_0's rmse: 0.58838\n",
      "[300]\tvalid_0's rmse: 0.586212\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.617281\n",
      "[200]\tvalid_0's rmse: 0.591426\n",
      "[300]\tvalid_0's rmse: 0.589398\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.638897\n",
      "[200]\tvalid_0's rmse: 0.616491\n",
      "[300]\tvalid_0's rmse: 0.621217\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.636255\n",
      "[200]\tvalid_0's rmse: 0.613797\n",
      "[300]\tvalid_0's rmse: 0.61778\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.615709\n",
      "[200]\tvalid_0's rmse: 0.587312\n",
      "[300]\tvalid_0's rmse: 0.584704\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.610292\n",
      "[200]\tvalid_0's rmse: 0.580068\n",
      "[300]\tvalid_0's rmse: 0.577615\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.63775\n",
      "[200]\tvalid_0's rmse: 0.613004\n",
      "[300]\tvalid_0's rmse: 0.609501\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.638305\n",
      "[200]\tvalid_0's rmse: 0.613474\n",
      "[300]\tvalid_0's rmse: 0.611791\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.645401\n",
      "[200]\tvalid_0's rmse: 0.604972\n",
      "[300]\tvalid_0's rmse: 0.59681\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.648411\n",
      "[200]\tvalid_0's rmse: 0.608496\n",
      "[300]\tvalid_0's rmse: 0.602485\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.652818\n",
      "[200]\tvalid_0's rmse: 0.607562\n",
      "[300]\tvalid_0's rmse: 0.603717\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.651277\n",
      "[200]\tvalid_0's rmse: 0.606274\n",
      "[300]\tvalid_0's rmse: 0.60307\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.63527\n",
      "[200]\tvalid_0's rmse: 0.597267\n",
      "[300]\tvalid_0's rmse: 0.590698\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.637916\n",
      "[200]\tvalid_0's rmse: 0.59852\n",
      "[300]\tvalid_0's rmse: 0.594692\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.638796\n",
      "[200]\tvalid_0's rmse: 0.629942\n",
      "[300]\tvalid_0's rmse: 0.631938\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.642567\n",
      "[200]\tvalid_0's rmse: 0.634868\n",
      "[300]\tvalid_0's rmse: 0.635073\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.640283\n",
      "[200]\tvalid_0's rmse: 0.607272\n",
      "[300]\tvalid_0's rmse: 0.604938\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.638602\n",
      "[200]\tvalid_0's rmse: 0.604946\n",
      "[300]\tvalid_0's rmse: 0.603545\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.693116\n",
      "[200]\tvalid_0's rmse: 0.647873\n",
      "[300]\tvalid_0's rmse: 0.640297\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.691224\n",
      "[200]\tvalid_0's rmse: 0.643311\n",
      "[300]\tvalid_0's rmse: 0.634015\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.585019\n",
      "[200]\tvalid_0's rmse: 0.553472\n",
      "[300]\tvalid_0's rmse: 0.554376\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.585319\n",
      "[200]\tvalid_0's rmse: 0.551545\n",
      "[300]\tvalid_0's rmse: 0.553267\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.642023\n",
      "[200]\tvalid_0's rmse: 0.615407\n",
      "[300]\tvalid_0's rmse: 0.614803\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.640851\n",
      "[200]\tvalid_0's rmse: 0.617218\n",
      "[300]\tvalid_0's rmse: 0.616433\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.532345\n",
      "[200]\tvalid_0's rmse: 0.499288\n",
      "[300]\tvalid_0's rmse: 0.496392\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.530784\n",
      "[200]\tvalid_0's rmse: 0.502253\n",
      "[300]\tvalid_0's rmse: 0.501991\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.584611\n",
      "[200]\tvalid_0's rmse: 0.544172\n",
      "[300]\tvalid_0's rmse: 0.543051\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.582573\n",
      "[200]\tvalid_0's rmse: 0.542138\n",
      "[300]\tvalid_0's rmse: 0.541958\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.647145\n",
      "[200]\tvalid_0's rmse: 0.613719\n",
      "[300]\tvalid_0's rmse: 0.614316\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.649035\n",
      "[200]\tvalid_0's rmse: 0.615534\n",
      "[300]\tvalid_0's rmse: 0.615083\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.662403\n",
      "[200]\tvalid_0's rmse: 0.619338\n",
      "[300]\tvalid_0's rmse: 0.614843\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.657022\n",
      "[200]\tvalid_0's rmse: 0.61782\n",
      "[300]\tvalid_0's rmse: 0.611921\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.661886\n",
      "[200]\tvalid_0's rmse: 0.635673\n",
      "[300]\tvalid_0's rmse: 0.634011\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.660943\n",
      "[200]\tvalid_0's rmse: 0.633058\n",
      "[300]\tvalid_0's rmse: 0.630984\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.673654\n",
      "[200]\tvalid_0's rmse: 0.652108\n",
      "[300]\tvalid_0's rmse: 0.646876\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.6722\n",
      "[200]\tvalid_0's rmse: 0.65047\n",
      "[300]\tvalid_0's rmse: 0.648032\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.667444\n",
      "[200]\tvalid_0's rmse: 0.651703\n",
      "[300]\tvalid_0's rmse: 0.652762\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.666714\n",
      "[200]\tvalid_0's rmse: 0.653417\n",
      "[300]\tvalid_0's rmse: 0.654451\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.634187\n",
      "[200]\tvalid_0's rmse: 0.597435\n",
      "[300]\tvalid_0's rmse: 0.595484\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.634334\n",
      "[200]\tvalid_0's rmse: 0.594064\n",
      "[300]\tvalid_0's rmse: 0.589577\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.671342\n",
      "[200]\tvalid_0's rmse: 0.624428\n",
      "[300]\tvalid_0's rmse: 0.615461\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.673299\n",
      "[200]\tvalid_0's rmse: 0.629298\n",
      "[300]\tvalid_0's rmse: 0.622107\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.628367\n",
      "[200]\tvalid_0's rmse: 0.593274\n",
      "[300]\tvalid_0's rmse: 0.59215\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.625647\n",
      "[200]\tvalid_0's rmse: 0.591746\n",
      "[300]\tvalid_0's rmse: 0.591895\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.614602\n",
      "[200]\tvalid_0's rmse: 0.589222\n",
      "[300]\tvalid_0's rmse: 0.586196\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.613957\n",
      "[200]\tvalid_0's rmse: 0.591483\n",
      "[300]\tvalid_0's rmse: 0.588908\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.612487\n",
      "[200]\tvalid_0's rmse: 0.568176\n",
      "[300]\tvalid_0's rmse: 0.573001\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.611781\n",
      "[200]\tvalid_0's rmse: 0.567682\n",
      "[300]\tvalid_0's rmse: 0.569085\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.613941\n",
      "[200]\tvalid_0's rmse: 0.586371\n",
      "[300]\tvalid_0's rmse: 0.584701\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.615277\n",
      "[200]\tvalid_0's rmse: 0.587487\n",
      "[300]\tvalid_0's rmse: 0.584876\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.623881\n",
      "[200]\tvalid_0's rmse: 0.600669\n",
      "[300]\tvalid_0's rmse: 0.603871\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.622412\n",
      "[200]\tvalid_0's rmse: 0.599708\n",
      "[300]\tvalid_0's rmse: 0.603002\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.654384\n",
      "[200]\tvalid_0's rmse: 0.637431\n",
      "[300]\tvalid_0's rmse: 0.639197\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.655264\n",
      "[200]\tvalid_0's rmse: 0.637181\n",
      "[300]\tvalid_0's rmse: 0.638647\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.669798\n",
      "[200]\tvalid_0's rmse: 0.639559\n",
      "[300]\tvalid_0's rmse: 0.632591\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.669999\n",
      "[200]\tvalid_0's rmse: 0.638775\n",
      "[300]\tvalid_0's rmse: 0.633749\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.619552\n",
      "[200]\tvalid_0's rmse: 0.58211\n",
      "[300]\tvalid_0's rmse: 0.578165\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.620288\n",
      "[200]\tvalid_0's rmse: 0.582844\n",
      "[300]\tvalid_0's rmse: 0.577446\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.685739\n",
      "[200]\tvalid_0's rmse: 0.655188\n",
      "[300]\tvalid_0's rmse: 0.650843\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.684103\n",
      "[200]\tvalid_0's rmse: 0.654167\n",
      "[300]\tvalid_0's rmse: 0.650489\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5420247656839267 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.9778252382803456 will be ignored. Current value: bagging_fraction=0.75\n",
      "[100]\tvalid_0's rmse: 0.673338\n",
      "[200]\tvalid_0's rmse: 0.628938\n",
      "[300]\tvalid_0's rmse: 0.621537\n",
      "--------------------------------------------------\n",
      "[100]\tvalid_0's rmse: 0.672753\n",
      "[200]\tvalid_0's rmse: 0.62904\n",
      "[300]\tvalid_0's rmse: 0.623087\n"
     ]
    }
   ],
   "source": [
    "def LGBM_train_and_test_v1(features,params):\n",
    "    OOF_PREDS = np.zeros(len(train_feats))\n",
    "    TEST_PREDS = np.zeros(len(test_feats))\n",
    "    best_iters_dict = defaultdict(list)\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = params\n",
    "    best_iterations = [340, 318, 325, 301, 361]\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                'random_state': 42,\n",
    "                \"n_estimators\" : best_iterations[i],\n",
    "                \"verbosity\": -1,\n",
    "                **best_params\n",
    "            }\n",
    "            \n",
    "            X_train_pre, y_train_pre = train_feats.iloc[train_idx][features], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid_pre, y_valid_pre = train_feats.iloc[valid_idx][features], train_feats.iloc[valid_idx][target_col]\n",
    "            pre_model = lgb.LGBMRegressor(**params)\n",
    "            pre_model.fit(X_train_pre, y_train_pre, eval_set=[(X_valid_pre, y_valid_pre)],verbose=100)\n",
    "            imp_df = pd.DataFrame()\n",
    "            imp_df[\"feature\"] = features       \n",
    "            imp_df[\"importance\"] = pre_model.feature_importances_\n",
    "            imp_df = imp_df.sort_values(by='importance',ascending=False)\n",
    "            features_select = list(imp_df[imp_df['importance']!=0]['feature'].values)\n",
    "            print('-'*50)\n",
    "            \n",
    "            X_train, y_train = train_feats.iloc[train_idx][features_select], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][features_select], train_feats.iloc[valid_idx][target_col]\n",
    "            model = lgb.LGBMRegressor(**params)    \n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=100)\n",
    "            best_iters_dict[str(seed)].append(model.best_iteration_)\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[features_select])\n",
    "            TEST_PREDS += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "        scores.append(oof_score)\n",
    "    return OOF_PREDS,TEST_PREDS\n",
    "\n",
    "params1 =  { 'boosting_type': 'gbdt', \n",
    "            'metric': 'rmse',\n",
    "            'reg_alpha': 0.003188447814669599, \n",
    "            'reg_lambda': 0.0010228604507564066, \n",
    "            'colsample_bytree': 0.5420247656839267, \n",
    "            'subsample': 0.9778252382803456, \n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.75,\n",
    "            'num_leaves': 19, \n",
    "            'learning_rate': 0.01716485155812008,\n",
    "            'min_child_samples': 46}\n",
    "OOF_PREDS_v1,TEST_PREDS_v1 = LGBM_train_and_test_v1(lgb_cols_v1,params1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b36a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:21:46.990694Z",
     "iopub.status.busy": "2024-01-09T08:21:46.989940Z",
     "iopub.status.idle": "2024-01-09T08:21:46.998856Z",
     "shell.execute_reply": "2024-01-09T08:21:46.997856Z"
    },
    "papermill": {
     "duration": 0.143474,
     "end_time": "2024-01-09T08:21:47.001554",
     "exception": false,
     "start_time": "2024-01-09T08:21:46.858080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric LGBM v1 = 0.60246\n"
     ]
    }
   ],
   "source": [
    "print('OOF metric LGBM v1 = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS_v1, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5df14cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:21:47.290323Z",
     "iopub.status.busy": "2024-01-09T08:21:47.289952Z",
     "iopub.status.idle": "2024-01-09T08:29:07.685057Z",
     "shell.execute_reply": "2024-01-09T08:29:07.684114Z"
    },
    "papermill": {
     "duration": 440.542575,
     "end_time": "2024-01-09T08:29:07.687257",
     "exception": false,
     "start_time": "2024-01-09T08:21:47.144682",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [03:15<00:00,  6.75s/it, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5380.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5675.22it/s]\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 5686.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5865.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5685.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n",
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:1322: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:1323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_25/506724820.py:1324: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:1325: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:979: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:981: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n",
      "<merge errors features.>\n",
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1866.07it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 737.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 20.13it/s, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 19328.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 27413.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 30030.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 26829.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 23475.58it/s]\n",
      "/tmp/ipykernel_25/506724820.py:1322: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:1323: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_25/506724820.py:1324: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_25/506724820.py:1325: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n",
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n",
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:998: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/506724820.py:1000: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge errors features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 570.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor_v2(42,train_essays,train_essays_with_upper,train_scores = train_scores)\n",
    "train_feats,tokenizer,save_cols = preprocessor.make_feats(train_logs)\n",
    "\n",
    "test_essays = EssayConstructor().getEssays(test_logs)\n",
    "test_essays.set_index('id',inplace=True)\n",
    "test_essays.index.name = None\n",
    "test_essays_with_upper = getEssays_with_upper(test_logs)\n",
    "preprocessor = Preprocessor_v2(42,test_essays,test_essays_with_upper,tokenizer=tokenizer,method='test',save_cols=save_cols)\n",
    "test_feats = preprocessor.make_feats(test_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3187454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:29:08.111351Z",
     "iopub.status.busy": "2024-01-09T08:29:08.110975Z",
     "iopub.status.idle": "2024-01-09T08:29:12.789815Z",
     "shell.execute_reply": "2024-01-09T08:29:12.789024Z"
    },
    "papermill": {
     "duration": 4.890382,
     "end_time": "2024-01-09T08:29:12.792176",
     "exception": false,
     "start_time": "2024-01-09T08:29:07.901794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "null1 = train_feats.isnull().sum().sort_values(ascending=False) / len(train_feats)\n",
    "drop1 = list(null1[null1>0.9].index)\n",
    "for col in train_feats.columns:\n",
    "    if train_feats[col].nunique()==1:\n",
    "        drop1.append(col)\n",
    "cols = [col for col in train_feats.columns if col!='id']\n",
    "corr_under_001 = (abs(train_feats[cols].corr()['score']).sort_values()<0.01)\n",
    "corr_sort = abs(train_feats[cols].corr()['score']).sort_values()\n",
    "drop2 = list(corr_sort[corr_under_001].index)\n",
    "\n",
    "drop_cols = ['id']\n",
    "train_cols = [col for col in train_feats.columns if col not in target_col + drop_cols]    #for NN\n",
    "\n",
    "drop_cols_lgb = []\n",
    "for i in train_feats.columns:\n",
    "    if 'action_time' in i:\n",
    "        drop_cols_lgb.append(i)\n",
    "    if 'up_time' in i:\n",
    "        drop_cols_lgb.append(i)\n",
    "#lgbm v2 features\n",
    "lgb_cols_v2 = [col for col in train_feats.columns if col not in target_col + drop_cols+drop_cols_lgb+drop1+drop2] #for LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "849d277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:29:13.206016Z",
     "iopub.status.busy": "2024-01-09T08:29:13.205650Z",
     "iopub.status.idle": "2024-01-09T08:51:37.095391Z",
     "shell.execute_reply": "2024-01-09T08:51:37.094421Z"
    },
    "papermill": {
     "duration": 1344.100329,
     "end_time": "2024-01-09T08:51:37.098116",
     "exception": false,
     "start_time": "2024-01-09T08:29:12.997787",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.715597\n",
      "[200]\tvalid_0's rmse: 0.622316\n",
      "[300]\tvalid_0's rmse: 0.585275\n",
      "[400]\tvalid_0's rmse: 0.575264\n",
      "[500]\tvalid_0's rmse: 0.573256\n",
      "[600]\tvalid_0's rmse: 0.573894\n",
      "[700]\tvalid_0's rmse: 0.575023\n",
      "[800]\tvalid_0's rmse: 0.577544\n",
      "[900]\tvalid_0's rmse: 0.579043\n",
      "[1000]\tvalid_0's rmse: 0.579474\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.827335\n",
      "[200]\tvalid_0's rmse: 0.725054\n",
      "[300]\tvalid_0's rmse: 0.67702\n",
      "[400]\tvalid_0's rmse: 0.651847\n",
      "[500]\tvalid_0's rmse: 0.63833\n",
      "[600]\tvalid_0's rmse: 0.630155\n",
      "[700]\tvalid_0's rmse: 0.624507\n",
      "[800]\tvalid_0's rmse: 0.621096\n",
      "[900]\tvalid_0's rmse: 0.619846\n",
      "[1000]\tvalid_0's rmse: 0.618866\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.801297\n",
      "[200]\tvalid_0's rmse: 0.68598\n",
      "[300]\tvalid_0's rmse: 0.631692\n",
      "[400]\tvalid_0's rmse: 0.60768\n",
      "[500]\tvalid_0's rmse: 0.597588\n",
      "[600]\tvalid_0's rmse: 0.594379\n",
      "[700]\tvalid_0's rmse: 0.593159\n",
      "[800]\tvalid_0's rmse: 0.593741\n",
      "[900]\tvalid_0's rmse: 0.593637\n",
      "[1000]\tvalid_0's rmse: 0.593368\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.728201\n",
      "[200]\tvalid_0's rmse: 0.634649\n",
      "[300]\tvalid_0's rmse: 0.602366\n",
      "[400]\tvalid_0's rmse: 0.588915\n",
      "[500]\tvalid_0's rmse: 0.584988\n",
      "[600]\tvalid_0's rmse: 0.582004\n",
      "[700]\tvalid_0's rmse: 0.583176\n",
      "[800]\tvalid_0's rmse: 0.582555\n",
      "[900]\tvalid_0's rmse: 0.582486\n",
      "[1000]\tvalid_0's rmse: 0.580518\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.813367\n",
      "[200]\tvalid_0's rmse: 0.709722\n",
      "[300]\tvalid_0's rmse: 0.659453\n",
      "[400]\tvalid_0's rmse: 0.63703\n",
      "[500]\tvalid_0's rmse: 0.624901\n",
      "[600]\tvalid_0's rmse: 0.617915\n",
      "[700]\tvalid_0's rmse: 0.614244\n",
      "[800]\tvalid_0's rmse: 0.612686\n",
      "[900]\tvalid_0's rmse: 0.610908\n",
      "[1000]\tvalid_0's rmse: 0.609429\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.808185\n",
      "[200]\tvalid_0's rmse: 0.703703\n",
      "[300]\tvalid_0's rmse: 0.657477\n",
      "[400]\tvalid_0's rmse: 0.63899\n",
      "[500]\tvalid_0's rmse: 0.632247\n",
      "[600]\tvalid_0's rmse: 0.628042\n",
      "[700]\tvalid_0's rmse: 0.626305\n",
      "[800]\tvalid_0's rmse: 0.626203\n",
      "[900]\tvalid_0's rmse: 0.625251\n",
      "[1000]\tvalid_0's rmse: 0.624807\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.787497\n",
      "[200]\tvalid_0's rmse: 0.678502\n",
      "[300]\tvalid_0's rmse: 0.629521\n",
      "[400]\tvalid_0's rmse: 0.606692\n",
      "[500]\tvalid_0's rmse: 0.59751\n",
      "[600]\tvalid_0's rmse: 0.59497\n",
      "[700]\tvalid_0's rmse: 0.592505\n",
      "[800]\tvalid_0's rmse: 0.590304\n",
      "[900]\tvalid_0's rmse: 0.589396\n",
      "[1000]\tvalid_0's rmse: 0.589027\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.825424\n",
      "[200]\tvalid_0's rmse: 0.698689\n",
      "[300]\tvalid_0's rmse: 0.645911\n",
      "[400]\tvalid_0's rmse: 0.622073\n",
      "[500]\tvalid_0's rmse: 0.609417\n",
      "[600]\tvalid_0's rmse: 0.605511\n",
      "[700]\tvalid_0's rmse: 0.60426\n",
      "[800]\tvalid_0's rmse: 0.603949\n",
      "[900]\tvalid_0's rmse: 0.604333\n",
      "[1000]\tvalid_0's rmse: 0.605042\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.808864\n",
      "[200]\tvalid_0's rmse: 0.708997\n",
      "[300]\tvalid_0's rmse: 0.660301\n",
      "[400]\tvalid_0's rmse: 0.637618\n",
      "[500]\tvalid_0's rmse: 0.62394\n",
      "[600]\tvalid_0's rmse: 0.61886\n",
      "[700]\tvalid_0's rmse: 0.614352\n",
      "[800]\tvalid_0's rmse: 0.612026\n",
      "[900]\tvalid_0's rmse: 0.610958\n",
      "[1000]\tvalid_0's rmse: 0.610861\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.859072\n",
      "[200]\tvalid_0's rmse: 0.757395\n",
      "[300]\tvalid_0's rmse: 0.714143\n",
      "[400]\tvalid_0's rmse: 0.699676\n",
      "[500]\tvalid_0's rmse: 0.690097\n",
      "[600]\tvalid_0's rmse: 0.687779\n",
      "[700]\tvalid_0's rmse: 0.687009\n",
      "[800]\tvalid_0's rmse: 0.687084\n",
      "[900]\tvalid_0's rmse: 0.686287\n",
      "[1000]\tvalid_0's rmse: 0.687064\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.831909\n",
      "[200]\tvalid_0's rmse: 0.731327\n",
      "[300]\tvalid_0's rmse: 0.692269\n",
      "[400]\tvalid_0's rmse: 0.675777\n",
      "[500]\tvalid_0's rmse: 0.6718\n",
      "[600]\tvalid_0's rmse: 0.672509\n",
      "[700]\tvalid_0's rmse: 0.672948\n",
      "[800]\tvalid_0's rmse: 0.674723\n",
      "[900]\tvalid_0's rmse: 0.675142\n",
      "[1000]\tvalid_0's rmse: 0.677487\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.819632\n",
      "[200]\tvalid_0's rmse: 0.709769\n",
      "[300]\tvalid_0's rmse: 0.65921\n",
      "[400]\tvalid_0's rmse: 0.636151\n",
      "[500]\tvalid_0's rmse: 0.625858\n",
      "[600]\tvalid_0's rmse: 0.620159\n",
      "[700]\tvalid_0's rmse: 0.618193\n",
      "[800]\tvalid_0's rmse: 0.617617\n",
      "[900]\tvalid_0's rmse: 0.616873\n",
      "[1000]\tvalid_0's rmse: 0.615852\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.782496\n",
      "[200]\tvalid_0's rmse: 0.683683\n",
      "[300]\tvalid_0's rmse: 0.644154\n",
      "[400]\tvalid_0's rmse: 0.62181\n",
      "[500]\tvalid_0's rmse: 0.611737\n",
      "[600]\tvalid_0's rmse: 0.605897\n",
      "[700]\tvalid_0's rmse: 0.603506\n",
      "[800]\tvalid_0's rmse: 0.602346\n",
      "[900]\tvalid_0's rmse: 0.601857\n",
      "[1000]\tvalid_0's rmse: 0.602042\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.773102\n",
      "[200]\tvalid_0's rmse: 0.666617\n",
      "[300]\tvalid_0's rmse: 0.616691\n",
      "[400]\tvalid_0's rmse: 0.593396\n",
      "[500]\tvalid_0's rmse: 0.583413\n",
      "[600]\tvalid_0's rmse: 0.579087\n",
      "[700]\tvalid_0's rmse: 0.577382\n",
      "[800]\tvalid_0's rmse: 0.577742\n",
      "[900]\tvalid_0's rmse: 0.577419\n",
      "[1000]\tvalid_0's rmse: 0.577062\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.87992\n",
      "[200]\tvalid_0's rmse: 0.754474\n",
      "[300]\tvalid_0's rmse: 0.691477\n",
      "[400]\tvalid_0's rmse: 0.66135\n",
      "[500]\tvalid_0's rmse: 0.647017\n",
      "[600]\tvalid_0's rmse: 0.640862\n",
      "[700]\tvalid_0's rmse: 0.639152\n",
      "[800]\tvalid_0's rmse: 0.637888\n",
      "[900]\tvalid_0's rmse: 0.637714\n",
      "[1000]\tvalid_0's rmse: 0.639202\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.734766\n",
      "[200]\tvalid_0's rmse: 0.644751\n",
      "[300]\tvalid_0's rmse: 0.613858\n",
      "[400]\tvalid_0's rmse: 0.602382\n",
      "[500]\tvalid_0's rmse: 0.599953\n",
      "[600]\tvalid_0's rmse: 0.597985\n",
      "[700]\tvalid_0's rmse: 0.597272\n",
      "[800]\tvalid_0's rmse: 0.600761\n",
      "[900]\tvalid_0's rmse: 0.603301\n",
      "[1000]\tvalid_0's rmse: 0.604817\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.819265\n",
      "[200]\tvalid_0's rmse: 0.719839\n",
      "[300]\tvalid_0's rmse: 0.678758\n",
      "[400]\tvalid_0's rmse: 0.659855\n",
      "[500]\tvalid_0's rmse: 0.650565\n",
      "[600]\tvalid_0's rmse: 0.646875\n",
      "[700]\tvalid_0's rmse: 0.643667\n",
      "[800]\tvalid_0's rmse: 0.641799\n",
      "[900]\tvalid_0's rmse: 0.641544\n",
      "[1000]\tvalid_0's rmse: 0.641157\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.760275\n",
      "[200]\tvalid_0's rmse: 0.646904\n",
      "[300]\tvalid_0's rmse: 0.601007\n",
      "[400]\tvalid_0's rmse: 0.582563\n",
      "[500]\tvalid_0's rmse: 0.575596\n",
      "[600]\tvalid_0's rmse: 0.572036\n",
      "[700]\tvalid_0's rmse: 0.566534\n",
      "[800]\tvalid_0's rmse: 0.564551\n",
      "[900]\tvalid_0's rmse: 0.566166\n",
      "[1000]\tvalid_0's rmse: 0.567043\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.813879\n",
      "[200]\tvalid_0's rmse: 0.69719\n",
      "[300]\tvalid_0's rmse: 0.640605\n",
      "[400]\tvalid_0's rmse: 0.612828\n",
      "[500]\tvalid_0's rmse: 0.600464\n",
      "[600]\tvalid_0's rmse: 0.592622\n",
      "[700]\tvalid_0's rmse: 0.589803\n",
      "[800]\tvalid_0's rmse: 0.587251\n",
      "[900]\tvalid_0's rmse: 0.584451\n",
      "[1000]\tvalid_0's rmse: 0.582885\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.7684\n",
      "[200]\tvalid_0's rmse: 0.666926\n",
      "[300]\tvalid_0's rmse: 0.622997\n",
      "[400]\tvalid_0's rmse: 0.603221\n",
      "[500]\tvalid_0's rmse: 0.599159\n",
      "[600]\tvalid_0's rmse: 0.595662\n",
      "[700]\tvalid_0's rmse: 0.595625\n",
      "[800]\tvalid_0's rmse: 0.594972\n",
      "[900]\tvalid_0's rmse: 0.594466\n",
      "[1000]\tvalid_0's rmse: 0.593962\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.815013\n",
      "[200]\tvalid_0's rmse: 0.703111\n",
      "[300]\tvalid_0's rmse: 0.656433\n",
      "[400]\tvalid_0's rmse: 0.637006\n",
      "[500]\tvalid_0's rmse: 0.630994\n",
      "[600]\tvalid_0's rmse: 0.628078\n",
      "[700]\tvalid_0's rmse: 0.627319\n",
      "[800]\tvalid_0's rmse: 0.628109\n",
      "[900]\tvalid_0's rmse: 0.629156\n",
      "[1000]\tvalid_0's rmse: 0.630885\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.775384\n",
      "[200]\tvalid_0's rmse: 0.67135\n",
      "[300]\tvalid_0's rmse: 0.625323\n",
      "[400]\tvalid_0's rmse: 0.600886\n",
      "[500]\tvalid_0's rmse: 0.589731\n",
      "[600]\tvalid_0's rmse: 0.583382\n",
      "[700]\tvalid_0's rmse: 0.583879\n",
      "[800]\tvalid_0's rmse: 0.584952\n",
      "[900]\tvalid_0's rmse: 0.583997\n",
      "[1000]\tvalid_0's rmse: 0.582803\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.760097\n",
      "[200]\tvalid_0's rmse: 0.674876\n",
      "[300]\tvalid_0's rmse: 0.64377\n",
      "[400]\tvalid_0's rmse: 0.630732\n",
      "[500]\tvalid_0's rmse: 0.619945\n",
      "[600]\tvalid_0's rmse: 0.614245\n",
      "[700]\tvalid_0's rmse: 0.612753\n",
      "[800]\tvalid_0's rmse: 0.612905\n",
      "[900]\tvalid_0's rmse: 0.612348\n",
      "[1000]\tvalid_0's rmse: 0.611371\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.809207\n",
      "[200]\tvalid_0's rmse: 0.703444\n",
      "[300]\tvalid_0's rmse: 0.652872\n",
      "[400]\tvalid_0's rmse: 0.62878\n",
      "[500]\tvalid_0's rmse: 0.615303\n",
      "[600]\tvalid_0's rmse: 0.608519\n",
      "[700]\tvalid_0's rmse: 0.606532\n",
      "[800]\tvalid_0's rmse: 0.603629\n",
      "[900]\tvalid_0's rmse: 0.603278\n",
      "[1000]\tvalid_0's rmse: 0.602825\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.84327\n",
      "[200]\tvalid_0's rmse: 0.716731\n",
      "[300]\tvalid_0's rmse: 0.658244\n",
      "[400]\tvalid_0's rmse: 0.634105\n",
      "[500]\tvalid_0's rmse: 0.625114\n",
      "[600]\tvalid_0's rmse: 0.618738\n",
      "[700]\tvalid_0's rmse: 0.615913\n",
      "[800]\tvalid_0's rmse: 0.612513\n",
      "[900]\tvalid_0's rmse: 0.611711\n",
      "[1000]\tvalid_0's rmse: 0.611562\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.804818\n",
      "[200]\tvalid_0's rmse: 0.692146\n",
      "[300]\tvalid_0's rmse: 0.639373\n",
      "[400]\tvalid_0's rmse: 0.619224\n",
      "[500]\tvalid_0's rmse: 0.606774\n",
      "[600]\tvalid_0's rmse: 0.60085\n",
      "[700]\tvalid_0's rmse: 0.597805\n",
      "[800]\tvalid_0's rmse: 0.59739\n",
      "[900]\tvalid_0's rmse: 0.599209\n",
      "[1000]\tvalid_0's rmse: 0.600296\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.756045\n",
      "[200]\tvalid_0's rmse: 0.671737\n",
      "[300]\tvalid_0's rmse: 0.640173\n",
      "[400]\tvalid_0's rmse: 0.632945\n",
      "[500]\tvalid_0's rmse: 0.629656\n",
      "[600]\tvalid_0's rmse: 0.630076\n",
      "[700]\tvalid_0's rmse: 0.629301\n",
      "[800]\tvalid_0's rmse: 0.629628\n",
      "[900]\tvalid_0's rmse: 0.630773\n",
      "[1000]\tvalid_0's rmse: 0.631838\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.798275\n",
      "[200]\tvalid_0's rmse: 0.698175\n",
      "[300]\tvalid_0's rmse: 0.653442\n",
      "[400]\tvalid_0's rmse: 0.632653\n",
      "[500]\tvalid_0's rmse: 0.622748\n",
      "[600]\tvalid_0's rmse: 0.617779\n",
      "[700]\tvalid_0's rmse: 0.615259\n",
      "[800]\tvalid_0's rmse: 0.612833\n",
      "[900]\tvalid_0's rmse: 0.611015\n",
      "[1000]\tvalid_0's rmse: 0.609953\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.856575\n",
      "[200]\tvalid_0's rmse: 0.7485\n",
      "[300]\tvalid_0's rmse: 0.69305\n",
      "[400]\tvalid_0's rmse: 0.665892\n",
      "[500]\tvalid_0's rmse: 0.655033\n",
      "[600]\tvalid_0's rmse: 0.647603\n",
      "[700]\tvalid_0's rmse: 0.646358\n",
      "[800]\tvalid_0's rmse: 0.645166\n",
      "[900]\tvalid_0's rmse: 0.644153\n",
      "[1000]\tvalid_0's rmse: 0.642743\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.761944\n",
      "[200]\tvalid_0's rmse: 0.654611\n",
      "[300]\tvalid_0's rmse: 0.607473\n",
      "[400]\tvalid_0's rmse: 0.59023\n",
      "[500]\tvalid_0's rmse: 0.580818\n",
      "[600]\tvalid_0's rmse: 0.577065\n",
      "[700]\tvalid_0's rmse: 0.574874\n",
      "[800]\tvalid_0's rmse: 0.576006\n",
      "[900]\tvalid_0's rmse: 0.575202\n",
      "[1000]\tvalid_0's rmse: 0.575562\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.790823\n",
      "[200]\tvalid_0's rmse: 0.692853\n",
      "[300]\tvalid_0's rmse: 0.64945\n",
      "[400]\tvalid_0's rmse: 0.629406\n",
      "[500]\tvalid_0's rmse: 0.623075\n",
      "[600]\tvalid_0's rmse: 0.617585\n",
      "[700]\tvalid_0's rmse: 0.615837\n",
      "[800]\tvalid_0's rmse: 0.615223\n",
      "[900]\tvalid_0's rmse: 0.614852\n",
      "[1000]\tvalid_0's rmse: 0.615091\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.71643\n",
      "[200]\tvalid_0's rmse: 0.592674\n",
      "[300]\tvalid_0's rmse: 0.539408\n",
      "[400]\tvalid_0's rmse: 0.516022\n",
      "[500]\tvalid_0's rmse: 0.507363\n",
      "[600]\tvalid_0's rmse: 0.503491\n",
      "[700]\tvalid_0's rmse: 0.502718\n",
      "[800]\tvalid_0's rmse: 0.503074\n",
      "[900]\tvalid_0's rmse: 0.502313\n",
      "[1000]\tvalid_0's rmse: 0.501946\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.780688\n",
      "[200]\tvalid_0's rmse: 0.661388\n",
      "[300]\tvalid_0's rmse: 0.604246\n",
      "[400]\tvalid_0's rmse: 0.579054\n",
      "[500]\tvalid_0's rmse: 0.566284\n",
      "[600]\tvalid_0's rmse: 0.560619\n",
      "[700]\tvalid_0's rmse: 0.558123\n",
      "[800]\tvalid_0's rmse: 0.558308\n",
      "[900]\tvalid_0's rmse: 0.556528\n",
      "[1000]\tvalid_0's rmse: 0.556177\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.791703\n",
      "[200]\tvalid_0's rmse: 0.675109\n",
      "[300]\tvalid_0's rmse: 0.627191\n",
      "[400]\tvalid_0's rmse: 0.606888\n",
      "[500]\tvalid_0's rmse: 0.594885\n",
      "[600]\tvalid_0's rmse: 0.592641\n",
      "[700]\tvalid_0's rmse: 0.591266\n",
      "[800]\tvalid_0's rmse: 0.592615\n",
      "[900]\tvalid_0's rmse: 0.593684\n",
      "[1000]\tvalid_0's rmse: 0.594273\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.839259\n",
      "[200]\tvalid_0's rmse: 0.731576\n",
      "[300]\tvalid_0's rmse: 0.684842\n",
      "[400]\tvalid_0's rmse: 0.660444\n",
      "[500]\tvalid_0's rmse: 0.650049\n",
      "[600]\tvalid_0's rmse: 0.644623\n",
      "[700]\tvalid_0's rmse: 0.641758\n",
      "[800]\tvalid_0's rmse: 0.639368\n",
      "[900]\tvalid_0's rmse: 0.637991\n",
      "[1000]\tvalid_0's rmse: 0.638573\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.814172\n",
      "[200]\tvalid_0's rmse: 0.703404\n",
      "[300]\tvalid_0's rmse: 0.655512\n",
      "[400]\tvalid_0's rmse: 0.638156\n",
      "[500]\tvalid_0's rmse: 0.630235\n",
      "[600]\tvalid_0's rmse: 0.625377\n",
      "[700]\tvalid_0's rmse: 0.624857\n",
      "[800]\tvalid_0's rmse: 0.625009\n",
      "[900]\tvalid_0's rmse: 0.625964\n",
      "[1000]\tvalid_0's rmse: 0.627164\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.800133\n",
      "[200]\tvalid_0's rmse: 0.711629\n",
      "[300]\tvalid_0's rmse: 0.671678\n",
      "[400]\tvalid_0's rmse: 0.659142\n",
      "[500]\tvalid_0's rmse: 0.654143\n",
      "[600]\tvalid_0's rmse: 0.651785\n",
      "[700]\tvalid_0's rmse: 0.650976\n",
      "[800]\tvalid_0's rmse: 0.650678\n",
      "[900]\tvalid_0's rmse: 0.649449\n",
      "[1000]\tvalid_0's rmse: 0.647905\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.789676\n",
      "[200]\tvalid_0's rmse: 0.710708\n",
      "[300]\tvalid_0's rmse: 0.678402\n",
      "[400]\tvalid_0's rmse: 0.667308\n",
      "[500]\tvalid_0's rmse: 0.662535\n",
      "[600]\tvalid_0's rmse: 0.660777\n",
      "[700]\tvalid_0's rmse: 0.661314\n",
      "[800]\tvalid_0's rmse: 0.660627\n",
      "[900]\tvalid_0's rmse: 0.658548\n",
      "[1000]\tvalid_0's rmse: 0.658321\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.793496\n",
      "[200]\tvalid_0's rmse: 0.68369\n",
      "[300]\tvalid_0's rmse: 0.632829\n",
      "[400]\tvalid_0's rmse: 0.608387\n",
      "[500]\tvalid_0's rmse: 0.596308\n",
      "[600]\tvalid_0's rmse: 0.590868\n",
      "[700]\tvalid_0's rmse: 0.587451\n",
      "[800]\tvalid_0's rmse: 0.586134\n",
      "[900]\tvalid_0's rmse: 0.586511\n",
      "[1000]\tvalid_0's rmse: 0.58775\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.8572\n",
      "[200]\tvalid_0's rmse: 0.743259\n",
      "[300]\tvalid_0's rmse: 0.691764\n",
      "[400]\tvalid_0's rmse: 0.665285\n",
      "[500]\tvalid_0's rmse: 0.651819\n",
      "[600]\tvalid_0's rmse: 0.646205\n",
      "[700]\tvalid_0's rmse: 0.642295\n",
      "[800]\tvalid_0's rmse: 0.63976\n",
      "[900]\tvalid_0's rmse: 0.637671\n",
      "[1000]\tvalid_0's rmse: 0.637411\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.793511\n",
      "[200]\tvalid_0's rmse: 0.676386\n",
      "[300]\tvalid_0's rmse: 0.620338\n",
      "[400]\tvalid_0's rmse: 0.596546\n",
      "[500]\tvalid_0's rmse: 0.585262\n",
      "[600]\tvalid_0's rmse: 0.579196\n",
      "[700]\tvalid_0's rmse: 0.578981\n",
      "[800]\tvalid_0's rmse: 0.579111\n",
      "[900]\tvalid_0's rmse: 0.579703\n",
      "[1000]\tvalid_0's rmse: 0.58068\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.781529\n",
      "[200]\tvalid_0's rmse: 0.666734\n",
      "[300]\tvalid_0's rmse: 0.614935\n",
      "[400]\tvalid_0's rmse: 0.594213\n",
      "[500]\tvalid_0's rmse: 0.589398\n",
      "[600]\tvalid_0's rmse: 0.587479\n",
      "[700]\tvalid_0's rmse: 0.58576\n",
      "[800]\tvalid_0's rmse: 0.585511\n",
      "[900]\tvalid_0's rmse: 0.5856\n",
      "[1000]\tvalid_0's rmse: 0.585379\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.796249\n",
      "[200]\tvalid_0's rmse: 0.667434\n",
      "[300]\tvalid_0's rmse: 0.613201\n",
      "[400]\tvalid_0's rmse: 0.584768\n",
      "[500]\tvalid_0's rmse: 0.56888\n",
      "[600]\tvalid_0's rmse: 0.562806\n",
      "[700]\tvalid_0's rmse: 0.560338\n",
      "[800]\tvalid_0's rmse: 0.559152\n",
      "[900]\tvalid_0's rmse: 0.559042\n",
      "[1000]\tvalid_0's rmse: 0.560928\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.771251\n",
      "[200]\tvalid_0's rmse: 0.666172\n",
      "[300]\tvalid_0's rmse: 0.62329\n",
      "[400]\tvalid_0's rmse: 0.604284\n",
      "[500]\tvalid_0's rmse: 0.596252\n",
      "[600]\tvalid_0's rmse: 0.592191\n",
      "[700]\tvalid_0's rmse: 0.591079\n",
      "[800]\tvalid_0's rmse: 0.591228\n",
      "[900]\tvalid_0's rmse: 0.592208\n",
      "[1000]\tvalid_0's rmse: 0.591187\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.750758\n",
      "[200]\tvalid_0's rmse: 0.652998\n",
      "[300]\tvalid_0's rmse: 0.606521\n",
      "[400]\tvalid_0's rmse: 0.588017\n",
      "[500]\tvalid_0's rmse: 0.579524\n",
      "[600]\tvalid_0's rmse: 0.579314\n",
      "[700]\tvalid_0's rmse: 0.578443\n",
      "[800]\tvalid_0's rmse: 0.577835\n",
      "[900]\tvalid_0's rmse: 0.577878\n",
      "[1000]\tvalid_0's rmse: 0.578476\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.783127\n",
      "[200]\tvalid_0's rmse: 0.688339\n",
      "[300]\tvalid_0's rmse: 0.654539\n",
      "[400]\tvalid_0's rmse: 0.644812\n",
      "[500]\tvalid_0's rmse: 0.641088\n",
      "[600]\tvalid_0's rmse: 0.642118\n",
      "[700]\tvalid_0's rmse: 0.643565\n",
      "[800]\tvalid_0's rmse: 0.646104\n",
      "[900]\tvalid_0's rmse: 0.646897\n",
      "[1000]\tvalid_0's rmse: 0.647979\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.818198\n",
      "[200]\tvalid_0's rmse: 0.719289\n",
      "[300]\tvalid_0's rmse: 0.678663\n",
      "[400]\tvalid_0's rmse: 0.662326\n",
      "[500]\tvalid_0's rmse: 0.653735\n",
      "[600]\tvalid_0's rmse: 0.648126\n",
      "[700]\tvalid_0's rmse: 0.646266\n",
      "[800]\tvalid_0's rmse: 0.643476\n",
      "[900]\tvalid_0's rmse: 0.64203\n",
      "[1000]\tvalid_0's rmse: 0.64082\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.782372\n",
      "[200]\tvalid_0's rmse: 0.670446\n",
      "[300]\tvalid_0's rmse: 0.621083\n",
      "[400]\tvalid_0's rmse: 0.600209\n",
      "[500]\tvalid_0's rmse: 0.592241\n",
      "[600]\tvalid_0's rmse: 0.587442\n",
      "[700]\tvalid_0's rmse: 0.583795\n",
      "[800]\tvalid_0's rmse: 0.581496\n",
      "[900]\tvalid_0's rmse: 0.580691\n",
      "[1000]\tvalid_0's rmse: 0.579061\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.851231\n",
      "[200]\tvalid_0's rmse: 0.754428\n",
      "[300]\tvalid_0's rmse: 0.709112\n",
      "[400]\tvalid_0's rmse: 0.689951\n",
      "[500]\tvalid_0's rmse: 0.678543\n",
      "[600]\tvalid_0's rmse: 0.676866\n",
      "[700]\tvalid_0's rmse: 0.674766\n",
      "[800]\tvalid_0's rmse: 0.673662\n",
      "[900]\tvalid_0's rmse: 0.673903\n",
      "[1000]\tvalid_0's rmse: 0.67366\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[100]\tvalid_0's rmse: 0.856688\n",
      "[200]\tvalid_0's rmse: 0.742371\n",
      "[300]\tvalid_0's rmse: 0.68905\n",
      "[400]\tvalid_0's rmse: 0.665856\n",
      "[500]\tvalid_0's rmse: 0.652034\n",
      "[600]\tvalid_0's rmse: 0.642961\n",
      "[700]\tvalid_0's rmse: 0.639066\n",
      "[800]\tvalid_0's rmse: 0.636985\n",
      "[900]\tvalid_0's rmse: 0.636649\n",
      "[1000]\tvalid_0's rmse: 0.635529\n"
     ]
    }
   ],
   "source": [
    "def LGBM_train_and_test_v2(features,params):\n",
    "    OOF_PREDS = np.zeros(len(train_feats))\n",
    "    TEST_PREDS = np.zeros(len(test_feats))\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = params\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats[train_cols]\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                **best_params\n",
    "            }\n",
    "            \n",
    "            X_train, y_train = train_feats.iloc[train_idx][features], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][features], train_feats.iloc[valid_idx][target_col]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params)    \n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=100)\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[features])\n",
    "            TEST_PREDS += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "        scores.append(oof_score)\n",
    "    return OOF_PREDS,TEST_PREDS\n",
    "params2 = {'n_estimators': 1024,\n",
    "         'learning_rate': 0.005,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': 42,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 0,}\n",
    "\n",
    "OOF_PREDS_v2,TEST_PREDS_v2 = LGBM_train_and_test_v2(lgb_cols_v2,params2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cc1a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:37.646242Z",
     "iopub.status.busy": "2024-01-09T08:51:37.645271Z",
     "iopub.status.idle": "2024-01-09T08:51:37.655352Z",
     "shell.execute_reply": "2024-01-09T08:51:37.654336Z"
    },
    "papermill": {
     "duration": 0.286641,
     "end_time": "2024-01-09T08:51:37.657535",
     "exception": false,
     "start_time": "2024-01-09T08:51:37.370894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric LGBM v2 = 0.60429\n"
     ]
    }
   ],
   "source": [
    "print('OOF metric LGBM v2 = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS_v2, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca02797d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:38.202021Z",
     "iopub.status.busy": "2024-01-09T08:51:38.201063Z",
     "iopub.status.idle": "2024-01-09T08:51:40.294309Z",
     "shell.execute_reply": "2024-01-09T08:51:40.293163Z"
    },
    "papermill": {
     "duration": 2.364423,
     "end_time": "2024-01-09T08:51:40.296710",
     "exception": false,
     "start_time": "2024-01-09T08:51:37.932287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition best W = 0.583\n"
     ]
    }
   ],
   "source": [
    "best_sc = 1\n",
    "for w in np.arange(0, 1.01, 0.001):\n",
    "    sc = metrics.mean_squared_error(train_feats[target_col], \n",
    "                                    w * OOF_PREDS_v1 + (1-w) * OOF_PREDS_v2, \n",
    "                                    squared=False)\n",
    "    if sc < best_sc:\n",
    "        best_sc = sc\n",
    "        best_w = w\n",
    "print('Composition best W = {:.3f}'.format(best_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f3948a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:40.846249Z",
     "iopub.status.busy": "2024-01-09T08:51:40.845846Z",
     "iopub.status.idle": "2024-01-09T08:51:40.854721Z",
     "shell.execute_reply": "2024-01-09T08:51:40.853798Z"
    },
    "papermill": {
     "duration": 0.272472,
     "end_time": "2024-01-09T08:51:40.856807",
     "exception": false,
     "start_time": "2024-01-09T08:51:40.584335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.27922582, 1.25867622, 1.28349684])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [best_w,1 - best_w]\n",
    "OOF_PREDS_lgbm_pandas = OOF_PREDS_v1 * W[0] + OOF_PREDS_v2 * W[1]\n",
    "TEST_PREDS_lgbm_pandas = TEST_PREDS_v1 * W[0] + TEST_PREDS_v2 * W[1]\n",
    "TEST_PREDS_lgbm_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73d5056f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:41.390243Z",
     "iopub.status.busy": "2024-01-09T08:51:41.389338Z",
     "iopub.status.idle": "2024-01-09T08:51:41.398170Z",
     "shell.execute_reply": "2024-01-09T08:51:41.397232Z"
    },
    "papermill": {
     "duration": 0.277972,
     "end_time": "2024-01-09T08:51:41.400557",
     "exception": false,
     "start_time": "2024-01-09T08:51:41.122585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric LGBM×2 = 0.60053\n"
     ]
    }
   ],
   "source": [
    "print('OOF metric LGBM×2 = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS_lgbm_pandas, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315d5ca",
   "metadata": {
    "papermill": {
     "duration": 0.260558,
     "end_time": "2024-01-09T08:51:41.922562",
     "exception": false,
     "start_time": "2024-01-09T08:51:41.662004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "327c0de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:42.518944Z",
     "iopub.status.busy": "2024-01-09T08:51:42.518558Z",
     "iopub.status.idle": "2024-01-09T08:51:53.896862Z",
     "shell.execute_reply": "2024-01-09T08:51:53.895811Z"
    },
    "papermill": {
     "duration": 11.710107,
     "end_time": "2024-01-09T08:51:53.899445",
     "exception": false,
     "start_time": "2024-01-09T08:51:42.189338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "668b364c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:54.402956Z",
     "iopub.status.busy": "2024-01-09T08:51:54.402586Z",
     "iopub.status.idle": "2024-01-09T08:51:54.408604Z",
     "shell.execute_reply": "2024-01-09T08:51:54.407676Z"
    },
    "papermill": {
     "duration": 0.26043,
     "end_time": "2024-01-09T08:51:54.411268",
     "exception": false,
     "start_time": "2024-01-09T08:51:54.150838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n/100)\n",
    "    percentile_.__name__ = 'pct_{:02.0f}'.format(n)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b9223a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:54.954028Z",
     "iopub.status.busy": "2024-01-09T08:51:54.953153Z",
     "iopub.status.idle": "2024-01-09T08:51:54.973412Z",
     "shell.execute_reply": "2024-01-09T08:51:54.972429Z"
    },
    "papermill": {
     "duration": 0.279484,
     "end_time": "2024-01-09T08:51:54.975496",
     "exception": false,
     "start_time": "2024-01-09T08:51:54.696012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_essay_aggregations(essay_df):\n",
    "    cols_to_drop = ['essay']\n",
    "    # Total essay length\n",
    "    essay_df['essay_len'] = essay_df['essay'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df.drop(columns=cols_to_drop)\n",
    "    return essay_df\n",
    "\n",
    "def split_essays_into_words(essay_df):\n",
    "    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('word')\n",
    "    # Word length (number of characters in word)\n",
    "    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "    return essay_df\n",
    "\n",
    "def compute_word_aggregations(word_df):\n",
    "    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(self.aggregations)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    # New features: computing the # of words whose length exceed word_l\n",
    "    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "def split_essays_into_sentences(essay_df):\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(sent_df):\n",
    "    sent_agg_df = sent_df[['id','sent_len','sent_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    # New features: computing the # of sentences whose (character) length exceed sent_l\n",
    "    for sent_l in [50, 60, 75, 100]:\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_df[sent_df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(essay_df):\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of sentences in paragraphs\n",
    "    essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(x.split('\\\\.|\\\\?|\\\\!')))\n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(paragraph_df):\n",
    "    paragraph_agg_df = paragraph_df[['id','paragraph_len', 'paragraph_sent_count', 'paragraph_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e89d57ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:55.496825Z",
     "iopub.status.busy": "2024-01-09T08:51:55.495962Z",
     "iopub.status.idle": "2024-01-09T08:51:55.583067Z",
     "shell.execute_reply": "2024-01-09T08:51:55.582061Z"
    },
    "papermill": {
     "duration": 0.351629,
     "end_time": "2024-01-09T08:51:55.585667",
     "exception": false,
     "start_time": "2024-01-09T08:51:55.234038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nth percentile function for agg\n",
    "class Preprocessor:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes_dict = {\n",
    "            'q': 'q', \n",
    "            ' ': 'space', \n",
    "            'NoChange': 'NoChange', \n",
    "            '.': 'full_stop', \n",
    "            ',': 'comma', \n",
    "            '\\n': 'newline', \n",
    "            \"'\": 'single_quote', \n",
    "            '\"': 'double_quote', \n",
    "            '-': 'dash', \n",
    "            '?': 'question_mark', \n",
    "            ';': 'semicolon', \n",
    "            '=': 'equals', \n",
    "            '/': 'slash', \n",
    "            '\\\\': 'double_backslash', \n",
    "            ':': 'colon'\n",
    "        }\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 70, 100]\n",
    "        self.percentiles = [5, 10, 25, 50, 75, 90, 95]\n",
    "        self.percentiles_cols = [percentile(n) for n in self.percentiles]\n",
    "        self.aggregations = ['mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "        self.idf = defaultdict(float)\n",
    "        \n",
    "        self.essay_constructor = EssayConstructor()\n",
    "    \n",
    "    def get_essay_aggregations(self, essay_df):\n",
    "        cols_to_drop = ['essay']\n",
    "        # Total essay length\n",
    "        essay_df['essay_len'] = essay_df['essay'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df.drop(columns=cols_to_drop)\n",
    "        return essay_df\n",
    "    \n",
    "    def split_essays_into_words(self, essay_df):\n",
    "        essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('word')\n",
    "        # Word length (number of characters in word)\n",
    "        essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "        return essay_df\n",
    "    \n",
    "    def compute_word_aggregations(self, word_df):\n",
    "        word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(self.aggregations)\n",
    "        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "        word_agg_df['id'] = word_agg_df.index\n",
    "        # New features: computing the # of words whose length exceed word_l\n",
    "        for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "            word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "            word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "        word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "        return word_agg_df\n",
    "    \n",
    "    def split_essays_into_sentences(self, essay_df):\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        # Number of words in sentences\n",
    "        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "\n",
    "    def compute_sentence_aggregations(self, sent_df):\n",
    "        sent_agg_df = sent_df[['id','sent_len','sent_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index\n",
    "        # New features: computing the # of sentences whose (character) length exceed sent_l\n",
    "        for sent_l in [50, 60, 75, 100]:\n",
    "            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_df[sent_df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        return sent_agg_df\n",
    "\n",
    "    def split_essays_into_paragraphs(self, essay_df):\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        # Number of sentences in paragraphs\n",
    "        essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(x.split('\\\\.|\\\\?|\\\\!')))\n",
    "        # Number of words in paragraphs\n",
    "        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "\n",
    "    def compute_paragraph_aggregations(self, paragraph_df):\n",
    "        paragraph_agg_df = paragraph_df[['id','paragraph_len', 'paragraph_sent_count', 'paragraph_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "        paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "        return paragraph_agg_df\n",
    "        \n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            # make dictionary entry for \"move from X to Y\"\n",
    "            di[\"move_to\"] = 0\n",
    "            \n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "                else:\n",
    "                    # we can do this because there are no missing values\n",
    "                    di[\"move_to\"] += v\n",
    "            ret.append(di)\n",
    "        \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'activity_{act}_tfidf_count' for act in ret.columns]\n",
    "        normal_cols = [f'activity_{act}_normal_count' for act in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "            \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'{colname}_{event}_tfidf_count' for event in ret.columns]\n",
    "        normal_cols = [f'{colname}_{event}_normal_count' for event in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes_dict.keys():\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "            \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'text_change_{self.text_changes_dict[txt_change]}_tfidf_count' for txt_change in ret.columns]\n",
    "        normal_cols = [f'text_change_{self.text_changes_dict[txt_change]}_normal_count' for txt_change in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "    \n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "    # Credit: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs/notebook\n",
    "    def make_space_features(self, df):\n",
    "        df['up_time_lagged'] = df.groupby('id')['up_time'].shift(1).fillna(df['down_time'])\n",
    "        df['time_diff'] = abs(df['down_time'] - df['up_time_lagged']) / 1000\n",
    "\n",
    "        group = df.groupby('id')['time_diff']\n",
    "        largest_lantency = group.max()\n",
    "        smallest_lantency = group.min()\n",
    "        median_lantency = group.median()\n",
    "        initial_pause = df.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            'id': df['id'].unique(),\n",
    "            'largest_lantency': largest_lantency,\n",
    "            'smallest_lantency': smallest_lantency,\n",
    "            'median_lantency': median_lantency,\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True)\n",
    "        return result\n",
    "    \n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        for percentile in self.percentiles:\n",
    "            tmp_df[f'input_word_length_pct_{percentile}'] = tmp_df['text_change'].apply(lambda x: np.percentile([len(i) for i in x] if len(x) > 0 else 0, \n",
    "                                                                                                               percentile))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def make_feats(self, df: pd.DataFrame, save_essays_path: str):\n",
    "        \n",
    "        print(\"Starting to engineer features\")\n",
    "        \n",
    "        # initialize features dataframe\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        # get essay feats\n",
    "        print(\"Getting essays\")\n",
    "        essay_df = self.essay_constructor.getEssays(df)\n",
    "        essay_df.to_csv(save_essays_path, index=False)\n",
    "\n",
    "        print(\"Getting essay aggregations data\")\n",
    "        essay_agg_df = self.get_essay_aggregations(essay_df)\n",
    "        feats = feats.merge(essay_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay word aggregations data\")\n",
    "        word_df = self.split_essays_into_words(essay_df)\n",
    "        word_agg_df = self.compute_word_aggregations(word_df)\n",
    "        feats = feats.merge(word_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay sentence aggregations data\")\n",
    "        sent_df = self.split_essays_into_sentences(essay_df)\n",
    "        sent_agg_df = self.compute_sentence_aggregations(sent_df)\n",
    "        feats = feats.merge(sent_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay paragraph aggregations data\")\n",
    "        paragraph_df = self.split_essays_into_paragraphs(essay_df)\n",
    "        paragraph_agg_df = self.compute_paragraph_aggregations(paragraph_df)\n",
    "        feats = feats.merge(paragraph_agg_df, on='id', how='left')\n",
    "        \n",
    "        # engineer counts data\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        # space features\n",
    "        print(\"Engineering space-related data\")\n",
    "        tmp_df = self.make_space_features(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        # get shifted features\n",
    "        # time shift\n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # cursor position shift\n",
    "        print(\"Engineering cursor position data - gaps\")\n",
    "        for gap in self.gaps: \n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # word count shift\n",
    "        print(\"Engineering word count data - gaps\")\n",
    "        for gap in self.gaps: \n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # get aggregate statistical features\n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        # [(feature name, [ stat summaries to add ])]\n",
    "        percentiles_cols = [percentile(n) for n in self.percentiles]\n",
    "        feats_stat = [\n",
    "            ('action_time',self.percentiles_cols),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', [ 'nunique']),\n",
    "            ('up_event', [ 'nunique']),\n",
    "            ('text_change', [ 'nunique']),\n",
    "            ] \n",
    "\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['first','last', 'max', 'min', 'mean', 'std', 'sem', 'skew', pd.DataFrame.kurt]+ percentiles_cols),\n",
    "                (f'cursor_position_change{gap}', ['first','last','max', 'mean', 'std','sem', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'sum', 'sem', 'skew', pd.DataFrame.kurt] + percentiles_cols),\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                    \n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left') \n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        # compare feats\n",
    "#         print(\"Engineering ratios data\")\n",
    "#         feats['word_time_ratio'] = feats['word_count_max'] / feats['up_time_max']\n",
    "#         feats['word_event_ratio'] = feats['word_count_max'] / feats['event_id_max']\n",
    "#         feats['event_time_ratio'] = feats['event_id_max']  / feats['up_time_max']\n",
    "#         feats['idle_time_ratio'] = feats['action_time_gap1_mean'] / feats['up_time_max']\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15eb8c18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:51:56.098326Z",
     "iopub.status.busy": "2024-01-09T08:51:56.097930Z",
     "iopub.status.idle": "2024-01-09T08:52:00.444206Z",
     "shell.execute_reply": "2024-01-09T08:52:00.442946Z"
    },
    "papermill": {
     "duration": 4.604538,
     "end_time": "2024-01-09T08:52:00.446718",
     "exception": false,
     "start_time": "2024-01-09T08:51:55.842180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#bruteforce agg\n",
    "train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n",
    "train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n",
    "train_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n",
    "test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n",
    "test_agg_fe_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32f64c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:52:00.958511Z",
     "iopub.status.busy": "2024-01-09T08:52:00.958100Z",
     "iopub.status.idle": "2024-01-09T08:52:00.969477Z",
     "shell.execute_reply": "2024-01-09T08:52:00.968445Z"
    },
    "papermill": {
     "duration": 0.270417,
     "end_time": "2024-01-09T08:52:00.971613",
     "exception": false,
     "start_time": "2024-01-09T08:52:00.701196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duration_features(train_logs,test_logs):\n",
    "    tr_logs,te_logs = copy.deepcopy(train_logs),copy.deepcopy(test_logs)\n",
    "    data = []\n",
    "    for logs in [tr_logs, te_logs]:\n",
    "        #logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "        #logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "        logs['time_diff'] = logs.groupby('id')['down_time'].diff()\n",
    "        group = logs.groupby('id')['time_diff']\n",
    "        largest_lantency = group.max()\n",
    "        smallest_lantency = group.min()\n",
    "        median_lantency = group.median()\n",
    "        initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        data.append(pd.DataFrame({\n",
    "            'id': logs['id'].unique(),\n",
    "            'largest_lantency': largest_lantency,\n",
    "            'smallest_lantency': smallest_lantency,\n",
    "            'median_lantency': median_lantency,\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bed4fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:52:01.489640Z",
     "iopub.status.busy": "2024-01-09T08:52:01.489240Z",
     "iopub.status.idle": "2024-01-09T08:52:01.501831Z",
     "shell.execute_reply": "2024-01-09T08:52:01.500883Z"
    },
    "papermill": {
     "duration": 0.273719,
     "end_time": "2024-01-09T08:52:01.504135",
     "exception": false,
     "start_time": "2024-01-09T08:52:01.230416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def essay_CountVectorizer(train_essays,test_essays):\n",
    "    train_essaysdf = copy.deepcopy(train_essays['essay'])\n",
    "    test_essaysdf = copy.deepcopy(test_essays['essay'])\n",
    "    train_essaysdf = pd.DataFrame({'id': train_essaysdf.index, 'essay': train_essaysdf.values})\n",
    "    test_essaysdf = pd.DataFrame({'id': test_essaysdf.index, 'essay': test_essaysdf.values})\n",
    "    merged_data = train_essaysdf.merge(train_scores, on='id')\n",
    "\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\n",
    "    X_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\n",
    "    y = merged_data['score']\n",
    "\n",
    "    X_tokenizer_train = X_tokenizer_train.todense()\n",
    "    X_tokenizer_test = X_tokenizer_test.todense()\n",
    "\n",
    "    train_count_vector = pd.DataFrame()\n",
    "    test_count_vector = pd.DataFrame()\n",
    "    for i in range(X_tokenizer_train.shape[1]) : \n",
    "        L = list(X_tokenizer_train[:,i])\n",
    "        li = [int(x) for x in L ]\n",
    "        train_count_vector[f'feature {i}'] = li\n",
    "    for i in range(X_tokenizer_test.shape[1]): \n",
    "        L = list(X_tokenizer_test[:,i])\n",
    "        li = [int(x) for x in L ]\n",
    "        test_count_vector[f'feature {i}'] = li\n",
    "\n",
    "    df_train_index = train_essaysdf['id']\n",
    "    df_test_index = test_essaysdf['id']\n",
    "    train_count_vector.loc[:, 'id'] = df_train_index\n",
    "    test_count_vector.loc[:, 'id'] = df_test_index\n",
    "\n",
    "    #保留部分特征，保留90%以上的人出现过的特征\n",
    "    save_cols = []\n",
    "    for i in train_count_vector.columns:\n",
    "        if sum(train_count_vector[i]==0)/len(train_count_vector)<0.1:\n",
    "            save_cols.append(i)\n",
    "    return train_count_vector,test_count_vector,save_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b1e8c0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:52:02.084222Z",
     "iopub.status.busy": "2024-01-09T08:52:02.083856Z",
     "iopub.status.idle": "2024-01-09T08:52:02.094122Z",
     "shell.execute_reply": "2024-01-09T08:52:02.093272Z"
    },
    "papermill": {
     "duration": 0.267618,
     "end_time": "2024-01-09T08:52:02.096142",
     "exception": false,
     "start_time": "2024-01-09T08:52:01.828524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def other_features(df,method='train'):\n",
    "    a = pd.DataFrame()\n",
    "    a['Input_all_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['activity']!='Input'))/df.groupby(['id']).apply(lambda x:sum(x['activity']=='Input'))\n",
    "    a['all_q_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['down_event']!='q'))/df.groupby(['id']).apply(lambda x:sum(x['down_event']=='q'))\n",
    "    activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "    events_dict = {\n",
    "                    'q':'q', \n",
    "                    'Space':'Space', \n",
    "                    'Backspace':'Backspace', \n",
    "                    'Shift':'Shift', \n",
    "                    'ArrowRight':'ArrowRight', \n",
    "                    'Leftclick':'Leftclick', \n",
    "                    'ArrowLeft':'ArrowLeft', \n",
    "                    '.':'fullstop', \n",
    "                    ',':'comma', \n",
    "                    'ArrowDown':'ArrowDown', \n",
    "                    'ArrowUp':'ArrowUp', \n",
    "                    'Enter':'Enter', \n",
    "                    'CapsLock':'CapsLock', \n",
    "                    \"'\":'single_quote', \n",
    "                    'Delete':'Delete', \n",
    "                    'Unidentified':'Unidentified',\n",
    "                  }\n",
    "    for i in tqdm(activities):\n",
    "        for j in events_dict:\n",
    "            a[f'{i}_{events_dict[j]}_count'] = df.groupby('id').apply(lambda x:len(x[(x['activity']==i)&(x['down_event']==j)]))\n",
    "    return a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c898a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:52:02.621575Z",
     "iopub.status.busy": "2024-01-09T08:52:02.621159Z",
     "iopub.status.idle": "2024-01-09T08:52:02.641723Z",
     "shell.execute_reply": "2024-01-09T08:52:02.640715Z"
    },
    "papermill": {
     "duration": 0.288365,
     "end_time": "2024-01-09T08:52:02.644146",
     "exception": false,
     "start_time": "2024-01-09T08:52:02.355781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def language_error(df,df_essays):\n",
    "    a = pd.DataFrame()\n",
    "    df['down_event_shift'] = df.groupby('id')['down_event'].shift(-1)\n",
    "    letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')|((x['down_event']=='Shift')&(x['down_event_shift']=='q'))]))\n",
    "    a['letter_big_count'] = letter_upper.values\n",
    "    a['id'] = df['id'].unique()\n",
    "    \n",
    "    essay_df = copy.deepcopy(df_essays)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    \n",
    "    #避免将qqq.).切分成多个句子\n",
    "    #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "    \n",
    "    essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df['sent_len']!=0]\n",
    "    errors_num = (essay_df.groupby('id').apply(len)-letter_upper).values\n",
    "    a['error_num'] = errors_num                          #如果句子个数大于大写字母按键次数，那么文章会有语法错误\n",
    "    return a\n",
    "def sentence_error(df):\n",
    "    essay_df = copy.deepcopy(df)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    essay_df = essay_df[essay_df['paragraph_len']!=0]\n",
    "    essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "    essay_df = essay_df[essay_df['only_space']==False]\n",
    "    a = pd.DataFrame()\n",
    "    a['para_error'] = essay_df.groupby('id').apply(lambda x:len(x[x['paragraph_len']<25]))   #一个段落字符过少可能不是完整的一句话，可能存在语法错误\n",
    "\n",
    "    return a.reset_index()\n",
    "\n",
    "def language_error_letter(df):\n",
    "    essay_df = copy.deepcopy(df)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    \n",
    "    #避免将qqq.).切分成多个句子\n",
    "    #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "    \n",
    "    essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip()) \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    essay_df['language_error_letter'] = essay_df['sent'].apply(lambda x:x[0])\n",
    "    essay_df['if_q'] = essay_df['language_error_letter'].apply(lambda x:x.lower()=='q')\n",
    "    essay_df = essay_df[essay_df['if_q']==True]\n",
    "    a = pd.DataFrame()\n",
    "    a['language_error_letter'] = essay_df.groupby('id').apply(lambda x:len(x[x['language_error_letter']=='q']))\n",
    "    return a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640d6d18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T08:52:03.163650Z",
     "iopub.status.busy": "2024-01-09T08:52:03.163253Z",
     "iopub.status.idle": "2024-01-09T09:03:33.150635Z",
     "shell.execute_reply": "2024-01-09T09:03:33.149575Z"
    },
    "papermill": {
     "duration": 690.248611,
     "end_time": "2024-01-09T09:03:33.153845",
     "exception": false,
     "start_time": "2024-01-09T08:52:02.905234",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to engineer features\n",
      "Getting essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:16<00:00, 149.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting essay aggregations data\n",
      "Getting essay word aggregations data\n",
      "Getting essay sentence aggregations data\n",
      "Getting essay paragraph aggregations data\n",
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5534.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5489.43it/s]\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 5456.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5781.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5411.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering space-related data\n",
      "Engineering time data\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering cursor position data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering word count data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [08:46<00:00, 16.44s/it, column=word_count_change100, method=pct_95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Done!\n",
      "Starting to engineer features\n",
      "Getting essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1279.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting essay aggregations data\n",
      "Getting essay word aggregations data\n",
      "Getting essay sentence aggregations data\n",
      "Getting essay paragraph aggregations data\n",
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 21620.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14429.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 19538.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17379.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 16799.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering space-related data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering cursor position data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering word count data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  8.91it/s, column=word_count_change100, method=pct_95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Done!\n",
      "<train_feats and test_feats done.>\n",
      "<merge train and test agg done.>\n",
      "<merge duration features|train label done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1938.52it/s]\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_25/857373842.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector.loc[:, 'id'] = df_train_index\n",
      "/tmp/ipykernel_25/857373842.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector.loc[:, 'id'] = df_test_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge train and test essay CountVectorizer done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other featuers done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 558.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge language errors done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "use_pre_fe_data = False\n",
    "if use_pre_fe_data:\n",
    "    pass\n",
    "else:\n",
    "    preprocessor_train = Preprocessor(seed = 42)\n",
    "    train_feats = preprocessor_train.make_feats(train_logs,save_essays_path = 'train_essays.csv')\n",
    "    del preprocessor_train\n",
    "    gc.collect()\n",
    "\n",
    "preprocessor_test = Preprocessor(seed = 42)\n",
    "test_feats = preprocessor_test.make_feats(test_logs,save_essays_path = 'test_essays.csv')\n",
    "print('<train_feats and test_feats done.>')\n",
    "\n",
    "train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')\n",
    "print('<merge train and test agg done.>')\n",
    "\n",
    "train_eD592674, test_eD592674 = duration_features(train_logs,test_logs)\n",
    "train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "print('<merge duration features|train label done.>')\n",
    "\n",
    "test_essays = (EssayConstructor().getEssays(test_logs))\n",
    "test_essays.index = test_essays['id'].values\n",
    "test_essays.drop(columns='id',inplace=True)\n",
    "train_count_vector,test_count_vector,save_cols = essay_CountVectorizer(train_essays,test_essays)\n",
    "train_feats = train_feats.merge(train_count_vector[save_cols], on='id', how='left')\n",
    "test_feats = test_feats.merge(test_count_vector[save_cols], on='id', how='left')\n",
    "print('<merge train and test essay CountVectorizer done.>')\n",
    "\n",
    "if os.path.exists('/kaggle/input/lgbm-and-nn-on-sentences'):  \n",
    "    train_agg_ratio = pd.read_csv('/kaggle/input/lgbm-and-nn-on-sentences/train_agg_ratio.csv')\n",
    "else:\n",
    "    train_agg_ratio = other_features(train_logs)\n",
    "#删除掉全部为0的列\n",
    "save_cols = []\n",
    "for i in train_agg_ratio.columns:\n",
    "    if sum(train_agg_ratio[i]==0)<len(train_agg_ratio):\n",
    "        save_cols.append(i)\n",
    "test_agg_ratio = other_features(test_logs)                          \n",
    "train_feats = train_feats.merge(train_agg_ratio[save_cols], on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_ratio[save_cols], on='id', how='left')\n",
    "train_agg_ratio.to_csv('/kaggle/working/train_agg_ratio.csv',index=0)\n",
    "print('<merge other featuers done.>')\n",
    "\n",
    "tr_language_error_agg = language_error(train_logs,train_essays)\n",
    "te_language_error_agg = language_error(test_logs,test_essays)\n",
    "train_feats = train_feats.merge(tr_language_error_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_language_error_agg, on='id', how='left')\n",
    "\n",
    "tr_sentence_error_agg = sentence_error(train_essays)\n",
    "te_sentence_error_agg = sentence_error(test_essays)\n",
    "train_feats = train_feats.merge(tr_sentence_error_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_sentence_error_agg, on='id', how='left')\n",
    "\n",
    "tr_language_error_letter_agg =  language_error_letter(train_essays_with_upper)\n",
    "test_essays_with_upper = getEssays_with_upper(test_logs)\n",
    "te_language_error_letter_agg =  language_error_letter(test_essays_with_upper)\n",
    "train_feats = train_feats.merge(tr_language_error_letter_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_language_error_letter_agg, on='id', how='left')\n",
    "print('<merge language errors done.>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a52bb1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:03:34.049285Z",
     "iopub.status.busy": "2024-01-09T09:03:34.048937Z",
     "iopub.status.idle": "2024-01-09T09:03:34.061622Z",
     "shell.execute_reply": "2024-01-09T09:03:34.060620Z"
    },
    "papermill": {
     "duration": 0.413476,
     "end_time": "2024-01-09T09:03:34.063691",
     "exception": false,
     "start_time": "2024-01-09T09:03:33.650215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "has_inf_cols = ['activity_Remove/Cut_tfidf_count',\n",
    " 'activity_Replace_tfidf_count',\n",
    " 'activity_Paste_tfidf_count',\n",
    " 'activity_move_to_tfidf_count',\n",
    " 'down_event_Backspace_tfidf_count',\n",
    " 'down_event_Shift_tfidf_count',\n",
    " 'down_event_ArrowRight_tfidf_count',\n",
    " 'down_event_Leftclick_tfidf_count',\n",
    " 'down_event_ArrowLeft_tfidf_count',\n",
    " 'down_event_._tfidf_count',\n",
    " 'down_event_,_tfidf_count',\n",
    " 'down_event_ArrowDown_tfidf_count',\n",
    " 'down_event_ArrowUp_tfidf_count',\n",
    " 'down_event_Enter_tfidf_count',\n",
    " 'down_event_CapsLock_tfidf_count',\n",
    " \"down_event_'_tfidf_count\",\n",
    " 'down_event_Delete_tfidf_count',\n",
    " 'down_event_Unidentified_tfidf_count',\n",
    " 'up_event_Backspace_tfidf_count',\n",
    " 'up_event_Shift_tfidf_count',\n",
    " 'up_event_ArrowRight_tfidf_count',\n",
    " 'up_event_Leftclick_tfidf_count',\n",
    " 'up_event_ArrowLeft_tfidf_count',\n",
    " 'up_event_._tfidf_count',\n",
    " 'up_event_,_tfidf_count',\n",
    " 'up_event_ArrowDown_tfidf_count',\n",
    " 'up_event_ArrowUp_tfidf_count',\n",
    " 'up_event_Enter_tfidf_count',\n",
    " 'up_event_CapsLock_tfidf_count',\n",
    " \"up_event_'_tfidf_count\",\n",
    " 'up_event_Delete_tfidf_count',\n",
    " 'up_event_Unidentified_tfidf_count',\n",
    " 'text_change_comma_tfidf_count',\n",
    " 'text_change_newline_tfidf_count',\n",
    " 'text_change_single_quote_tfidf_count',\n",
    " 'text_change_double_quote_tfidf_count',\n",
    " 'text_change_dash_tfidf_count',\n",
    " 'text_change_question_mark_tfidf_count',\n",
    " 'text_change_semicolon_tfidf_count',\n",
    " 'text_change_equals_tfidf_count',\n",
    " 'text_change_slash_tfidf_count',\n",
    " 'text_change_double_backslash_tfidf_count',\n",
    " 'text_change_colon_tfidf_count']\n",
    "drop_cols_NN = []\n",
    "for i in train_feats.columns:\n",
    "    if 'action_time' in i:\n",
    "        drop_cols_NN.append(i)\n",
    "    if 'up_time' in i:\n",
    "        drop_cols_NN.append(i)\n",
    "cols = [col for col in train_feats.columns if col not in drop_cols+drop_cols_NN+has_inf_cols]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fea8792a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:03:34.874632Z",
     "iopub.status.busy": "2024-01-09T09:03:34.874238Z",
     "iopub.status.idle": "2024-01-09T09:03:34.904297Z",
     "shell.execute_reply": "2024-01-09T09:03:34.903241Z"
    },
    "papermill": {
     "duration": 0.444217,
     "end_time": "2024-01-09T09:03:34.906527",
     "exception": false,
     "start_time": "2024-01-09T09:03:34.462310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK = 'reg'\n",
    "task = Task(TASK)\n",
    "TIMEOUT = 10000\n",
    "N_THREADS = 2\n",
    "TRAIN_BS = 128\n",
    "USE_PLR = True\n",
    "USE_QNT = True\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "ADVANCED_ROLES = False\n",
    "TARGET_NAME = 'score'\n",
    "roles = {'target':TARGET_NAME}\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27ada595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:03:35.725112Z",
     "iopub.status.busy": "2024-01-09T09:03:35.724745Z",
     "iopub.status.idle": "2024-01-09T09:03:35.732460Z",
     "shell.execute_reply": "2024-01-09T09:03:35.731428Z"
    },
    "papermill": {
     "duration": 0.413757,
     "end_time": "2024-01-09T09:03:35.734475",
     "exception": false,
     "start_time": "2024-01-09T09:03:35.320718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(task, y_true, y_pred):\n",
    "    if task.name == 'binary':\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    elif task.name == 'multiclass':\n",
    "        return log_loss(y_true, y_pred)\n",
    "    elif task.name == 'reg' or task.name == 'multi:reg':\n",
    "        return mean_squared_error(y_true,y_pred,squared=False)\n",
    "    else:\n",
    "        raise 'Task is not correct.'\n",
    "def use_plr(USE_PLR):\n",
    "    if USE_PLR:\n",
    "        return \"plr\"\n",
    "    else:\n",
    "        return \"cont\"\n",
    "def take_pred_from_task(pred, task):\n",
    "    if task.name == 'binary' or task.name == 'reg':\n",
    "        return pred[:, 0]\n",
    "    elif task.name == 'multiclass' or task.name == 'multi:reg':\n",
    "        return pred\n",
    "    else:\n",
    "        raise 'Task is not correct.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7777ea17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:03:36.573770Z",
     "iopub.status.busy": "2024-01-09T09:03:36.573419Z",
     "iopub.status.idle": "2024-01-09T09:07:35.178645Z",
     "shell.execute_reply": "2024-01-09T09:07:35.177787Z"
    },
    "papermill": {
     "duration": 239.037082,
     "end_time": "2024-01-09T09:07:35.180992",
     "exception": false,
     "start_time": "2024-01-09T09:03:36.143910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:03:36] Stdout logging level is INFO3.\n",
      "[09:03:36] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[09:03:36] Task: reg\n",
      "\n",
      "[09:03:36] Start automl preset with listed constraints:\n",
      "[09:03:36] - time: 10000.00 seconds\n",
      "[09:03:36] - CPU: 2 cores\n",
      "[09:03:36] - memory: 16 GB\n",
      "\n",
      "[09:03:36] \u001b[1mTrain data shape: (2471, 498)\u001b[0m\n",
      "\n",
      "[09:03:37] Layer \u001b[1m1\u001b[0m train process start. Time left 9999.55 secs\n",
      "[09:03:38] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[09:03:38] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[09:03:43] Epoch: 0, train loss: 0.9810305833816528, val loss: 0.7626069784164429, val metric: -0.7649670243263245\n",
      "[09:03:48] Epoch: 1, train loss: 0.6814952492713928, val loss: 0.4305188059806824, val metric: -0.4298667907714844\n",
      "[09:03:53] Epoch: 2, train loss: 0.45020735263824463, val loss: 0.32762372493743896, val metric: -0.3262951672077179\n",
      "[09:03:57] Epoch: 3, train loss: 0.397036612033844, val loss: 0.3089464008808136, val metric: -0.3081304430961609\n",
      "[09:04:02] Epoch: 4, train loss: 0.3741583824157715, val loss: 0.30493903160095215, val metric: -0.3041883707046509\n",
      "[09:04:06] Epoch: 5, train loss: 0.3573160767555237, val loss: 0.29568272829055786, val metric: -0.29507625102996826\n",
      "[09:04:11] Epoch: 6, train loss: 0.34100568294525146, val loss: 0.289006769657135, val metric: -0.28836268186569214\n",
      "[09:04:15] Epoch: 7, train loss: 0.3362475037574768, val loss: 0.2914918065071106, val metric: -0.2912205159664154\n",
      "[09:04:20] Epoch: 8, train loss: 0.3212756812572479, val loss: 0.29029789566993713, val metric: -0.29007551074028015\n",
      "[09:04:24] Epoch: 9, train loss: 0.32385340332984924, val loss: 0.29664725065231323, val metric: -0.29669156670570374\n",
      "[09:04:25] Early stopping: val loss: 0.28681299090385437, val metric: -0.28643137216567993\n",
      "[09:04:25] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[09:04:30] Epoch: 0, train loss: 0.9515281915664673, val loss: 0.9345845580101013, val metric: -0.9263876080513\n",
      "[09:04:34] Epoch: 1, train loss: 0.6699231863021851, val loss: 0.5695532560348511, val metric: -0.563831627368927\n",
      "[09:04:39] Epoch: 2, train loss: 0.4343009889125824, val loss: 0.46843773126602173, val metric: -0.46478673815727234\n",
      "[09:04:43] Epoch: 3, train loss: 0.3771454691886902, val loss: 0.4293701648712158, val metric: -0.42663538455963135\n",
      "[09:04:48] Epoch: 4, train loss: 0.34527117013931274, val loss: 0.43311381340026855, val metric: -0.4306129217147827\n",
      "[09:04:53] Epoch: 5, train loss: 0.32767754793167114, val loss: 0.41210830211639404, val metric: -0.40993639826774597\n",
      "[09:04:57] Epoch: 6, train loss: 0.3122810423374176, val loss: 0.40176546573638916, val metric: -0.40023279190063477\n",
      "[09:05:02] Epoch: 7, train loss: 0.30903294682502747, val loss: 0.40651935338974, val metric: -0.4045776426792145\n",
      "[09:05:06] Epoch: 8, train loss: 0.2976270914077759, val loss: 0.4007945954799652, val metric: -0.3994971513748169\n",
      "[09:05:11] Epoch: 9, train loss: 0.29398804903030396, val loss: 0.4041910469532013, val metric: -0.4024786651134491\n",
      "[09:05:11] Early stopping: val loss: 0.39940720796585083, val metric: -0.39789366722106934\n",
      "[09:05:11] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[09:05:16] Epoch: 0, train loss: 0.9793614149093628, val loss: 0.8497956991195679, val metric: -0.8466967940330505\n",
      "[09:05:21] Epoch: 1, train loss: 0.6865735054016113, val loss: 0.5296184420585632, val metric: -0.5305876731872559\n",
      "[09:05:25] Epoch: 2, train loss: 0.4372751712799072, val loss: 0.4136893153190613, val metric: -0.4169125556945801\n",
      "[09:05:30] Epoch: 3, train loss: 0.37560081481933594, val loss: 0.374543160200119, val metric: -0.3773437440395355\n",
      "[09:05:35] Epoch: 4, train loss: 0.35382968187332153, val loss: 0.36544764041900635, val metric: -0.36853355169296265\n",
      "[09:05:39] Epoch: 5, train loss: 0.3336426615715027, val loss: 0.3562626838684082, val metric: -0.3596106767654419\n",
      "[09:05:44] Epoch: 6, train loss: 0.3231065273284912, val loss: 0.3595580458641052, val metric: -0.3634960353374481\n",
      "[09:05:48] Epoch: 7, train loss: 0.31735366582870483, val loss: 0.36360347270965576, val metric: -0.36753493547439575\n",
      "[09:05:53] Epoch: 8, train loss: 0.3003491461277008, val loss: 0.3576275706291199, val metric: -0.3614480197429657\n",
      "[09:05:57] Epoch: 9, train loss: 0.3016086220741272, val loss: 0.35686635971069336, val metric: -0.360495001077652\n",
      "[09:05:58] Early stopping: val loss: 0.35263997316360474, val metric: -0.3562726378440857\n",
      "[09:05:58] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[09:06:03] Epoch: 0, train loss: 0.9679414629936218, val loss: 0.8937817215919495, val metric: -0.8971239328384399\n",
      "[09:06:07] Epoch: 1, train loss: 0.674898624420166, val loss: 0.5519437789916992, val metric: -0.552452564239502\n",
      "[09:06:12] Epoch: 2, train loss: 0.4346697926521301, val loss: 0.4402918815612793, val metric: -0.4393906593322754\n",
      "[09:06:16] Epoch: 3, train loss: 0.3828659653663635, val loss: 0.4082317352294922, val metric: -0.4075514078140259\n",
      "[09:06:21] Epoch: 4, train loss: 0.34731537103652954, val loss: 0.40099549293518066, val metric: -0.4003864824771881\n",
      "[09:06:26] Epoch: 5, train loss: 0.333342969417572, val loss: 0.3942727744579315, val metric: -0.39369216561317444\n",
      "[09:06:30] Epoch: 6, train loss: 0.31861770153045654, val loss: 0.39975473284721375, val metric: -0.3991137146949768\n",
      "[09:06:35] Epoch: 7, train loss: 0.3049255609512329, val loss: 0.3938775658607483, val metric: -0.3929077684879303\n",
      "[09:06:39] Epoch: 8, train loss: 0.29867345094680786, val loss: 0.406494140625, val metric: -0.4054427444934845\n",
      "[09:06:44] Epoch: 9, train loss: 0.29036223888397217, val loss: 0.41358301043510437, val metric: -0.4122769236564636\n",
      "[09:06:44] Early stopping: val loss: 0.3940872550010681, val metric: -0.39337706565856934\n",
      "[09:06:44] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[09:06:49] Epoch: 0, train loss: 0.9717608690261841, val loss: 0.8193182945251465, val metric: -0.8235642313957214\n",
      "[09:06:54] Epoch: 1, train loss: 0.6774671077728271, val loss: 0.48692867159843445, val metric: -0.48798868060112\n",
      "[09:06:58] Epoch: 2, train loss: 0.4539906978607178, val loss: 0.4060993790626526, val metric: -0.40496769547462463\n",
      "[09:07:03] Epoch: 3, train loss: 0.3912798762321472, val loss: 0.374796062707901, val metric: -0.37403160333633423\n",
      "[09:07:08] Epoch: 4, train loss: 0.36228710412979126, val loss: 0.3749352991580963, val metric: -0.37456417083740234\n",
      "[09:07:12] Epoch: 5, train loss: 0.3464202880859375, val loss: 0.3555053174495697, val metric: -0.3544129431247711\n",
      "[09:07:17] Epoch: 6, train loss: 0.3326992690563202, val loss: 0.3500192165374756, val metric: -0.3488612473011017\n",
      "[09:07:21] Epoch: 7, train loss: 0.3267839550971985, val loss: 0.360755980014801, val metric: -0.36028483510017395\n",
      "[09:07:26] Epoch: 8, train loss: 0.3121652603149414, val loss: 0.35208725929260254, val metric: -0.3509708046913147\n",
      "[09:07:30] Epoch: 9, train loss: 0.30575141310691833, val loss: 0.3528357148170471, val metric: -0.3522709608078003\n",
      "[09:07:31] Early stopping: val loss: 0.3473047614097595, val metric: -0.34630605578422546\n",
      "[09:07:31] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m-0.3560279861874488\u001b[0m\n",
      "[09:07:31] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[09:07:31] Time left 9765.06 secs\n",
      "\n",
      "[09:07:31] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[09:07:31] \u001b[1mAutoml preset training completed in 234.94 seconds\u001b[0m\n",
      "\n",
      "[09:07:31] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "OOF_PREDS_mlp = np.zeros(len(train_feats))\n",
    "algo = \"mlp\"\n",
    "for RANDOM_STATE in range(42,43):\n",
    "    automl = TabularAutoML(\n",
    "        task = task, \n",
    "        timeout = TIMEOUT,\n",
    "        cpu_limit = N_THREADS,\n",
    "        general_params = {\"use_algos\": [[algo]]}, # ['nn', 'mlp', 'dense', 'denselight', 'resnet', 'snn', 'node', 'autoint', 'fttransformer'] or custom torch model\n",
    "        nn_params = {\n",
    "            \"n_epochs\": 10, \n",
    "            \"bs\": TRAIN_BS, \n",
    "            \"num_workers\": 0, \n",
    "            \"path_to_save\": None, \n",
    "            \"freeze_defaults\": True, \n",
    "            \"cont_embedder\": use_plr(USE_PLR), \n",
    "            \"hidden_size\": 32,\n",
    "            'clip_grad': True, \n",
    "            'clip_grad_params': {'max_norm': 1},\n",
    "        },\n",
    "        nn_pipeline_params = {\"use_qnt\": USE_QNT, \"use_te\": False},\n",
    "        reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE, 'advanced_roles': ADVANCED_ROLES},\n",
    "    )\n",
    "    OOF_PREDS_mlp += take_pred_from_task((automl.fit_predict(train_data=train_feats[cols],roles = {'target':'score'}, verbose = 3)).data, task)  /len(range(42,43))\n",
    "    joblib.dump(automl,f'MLP_{RANDOM_STATE}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d91d67ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:35.995088Z",
     "iopub.status.busy": "2024-01-09T09:07:35.994705Z",
     "iopub.status.idle": "2024-01-09T09:07:36.002471Z",
     "shell.execute_reply": "2024-01-09T09:07:36.001555Z"
    },
    "papermill": {
     "duration": 0.412833,
     "end_time": "2024-01-09T09:07:36.004662",
     "exception": false,
     "start_time": "2024-01-09T09:07:35.591829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5966808076245195"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(train_feats['score'],OOF_PREDS_mlp,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceb32934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:36.811390Z",
     "iopub.status.busy": "2024-01-09T09:07:36.810582Z",
     "iopub.status.idle": "2024-01-09T09:07:36.819485Z",
     "shell.execute_reply": "2024-01-09T09:07:36.818341Z"
    },
    "papermill": {
     "duration": 0.415262,
     "end_time": "2024-01-09T09:07:36.821702",
     "exception": false,
     "start_time": "2024-01-09T09:07:36.406440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cols = [col for col in train_feats.columns if col not in drop_cols+drop_cols_NN+target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9221f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:37.726394Z",
     "iopub.status.busy": "2024-01-09T09:07:37.725564Z",
     "iopub.status.idle": "2024-01-09T09:07:45.212195Z",
     "shell.execute_reply": "2024-01-09T09:07:45.211226Z"
    },
    "papermill": {
     "duration": 7.976386,
     "end_time": "2024-01-09T09:07:45.214663",
     "exception": false,
     "start_time": "2024-01-09T09:07:37.238277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PREDS_mlp = np.zeros(len(test_feats))\n",
    "for i in range(42,43):\n",
    "    automl = joblib.load('/kaggle/working/MLP_{}.pkl'.format(i))\n",
    "    TEST_PREDS_mlp += automl.predict(test_feats[test_cols]).data[:, 0] / len(range(42,43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65bf32",
   "metadata": {
    "papermill": {
     "duration": 0.404484,
     "end_time": "2024-01-09T09:07:46.027072",
     "exception": false,
     "start_time": "2024-01-09T09:07:45.622588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89fd6de",
   "metadata": {
    "papermill": {
     "duration": 0.403193,
     "end_time": "2024-01-09T09:07:46.833279",
     "exception": false,
     "start_time": "2024-01-09T09:07:46.430086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 0.582 public notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a383252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:47.641561Z",
     "iopub.status.busy": "2024-01-09T09:07:47.640475Z",
     "iopub.status.idle": "2024-01-09T09:07:47.843131Z",
     "shell.execute_reply": "2024-01-09T09:07:47.841944Z"
    },
    "papermill": {
     "duration": 0.605445,
     "end_time": "2024-01-09T09:07:47.845820",
     "exception": false,
     "start_time": "2024-01-09T09:07:47.240375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ab103b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:48.705279Z",
     "iopub.status.busy": "2024-01-09T09:07:48.704947Z",
     "iopub.status.idle": "2024-01-09T09:07:48.743210Z",
     "shell.execute_reply": "2024-01-09T09:07:48.742438Z"
    },
    "papermill": {
     "duration": 0.432962,
     "end_time": "2024-01-09T09:07:48.745344",
     "exception": false,
     "start_time": "2024-01-09T09:07:48.312382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "# def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "#     skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "#     test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n",
    "#     for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "#         train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "#         model.fit(train_x, train_y)\n",
    "#         if test_x is None:\n",
    "#             test_y[valid_index] = model.predict(valid_x)\n",
    "#         else:\n",
    "#             test_y[:, i] = model.predict(test_x)\n",
    "#     return test_y if (test_x is None) else np.mean(test_y, axis=1)\n",
    "\n",
    "\n",
    "def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    test_y = np.zeros(len(data_x)) \n",
    "    p_y   = np.zeros((len(test_x), n_splits))\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "        model.fit(train_x, train_y)\n",
    "        test_y[valid_index] = model.predict(valid_x)\n",
    "        p_y[:, i] = model.predict(test_x)\n",
    "\n",
    "    return test_y, np.mean(p_y, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1badf73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:49.571873Z",
     "iopub.status.busy": "2024-01-09T09:07:49.571497Z",
     "iopub.status.idle": "2024-01-09T09:07:49.611419Z",
     "shell.execute_reply": "2024-01-09T09:07:49.610358Z"
    },
    "papermill": {
     "duration": 0.460518,
     "end_time": "2024-01-09T09:07:49.613561",
     "exception": false,
     "start_time": "2024-01-09T09:07:49.153043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "    sp_df = df[df.sent_len==0].reset_index(drop=True)\n",
    "    sp_df = sp_df.groupby(['id'])['essay'].count().reset_index()\n",
    "    sp_df.columns = ['id', 'sp_cn']\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    def sent_mic_feat(x):\n",
    "        res = []\n",
    "        numx = []\n",
    "        for h in x.split(','):\n",
    "            numx.append(len(h.split( )))\n",
    "        res = [np.mean(numx), np.std(numx), np.max(numx), np.min(numx), len(numx)]\n",
    "        return res\n",
    "    df['mic_feat'] = df['sent'].apply(sent_mic_feat)\n",
    "    df = pd.concat([df,df['mic_feat'].apply(pd.Series)], axis=1).drop(['mic_feat'], axis=1)\n",
    "    df.columns = ['id','essay','word','sent','sent_len','sent_word_count','mic_f_mean','mic_f_std','mic_f_max','mic_f_min','mic_f_cn']\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','mic_f_mean']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','mic_f_std']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','mic_f_max']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','mic_f_min']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                df[['id','mic_f_cn']].groupby(['id']).agg(AGGREGATIONS)], \n",
    "                                axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    sent_agg_df = sent_agg_df.merge(sp_df, on=['id'], how='left')\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dbd7481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:07:50.415236Z",
     "iopub.status.busy": "2024-01-09T09:07:50.414504Z",
     "iopub.status.idle": "2024-01-09T09:09:41.179035Z",
     "shell.execute_reply": "2024-01-09T09:09:41.177957Z"
    },
    "papermill": {
     "duration": 111.578571,
     "end_time": "2024-01-09T09:09:41.590922",
     "exception": false,
     "start_time": "2024-01-09T09:07:50.012351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 216\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    }
   ],
   "source": [
    "data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "test_ids = test_feats['id'].values\n",
    "testin_x = test_feats.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1c8a69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:09:42.442489Z",
     "iopub.status.busy": "2024-01-09T09:09:42.442081Z",
     "iopub.status.idle": "2024-01-09T09:09:58.713761Z",
     "shell.execute_reply": "2024-01-09T09:09:58.712750Z"
    },
    "papermill": {
     "duration": 16.669363,
     "end_time": "2024-01-09T09:09:58.716349",
     "exception": false,
     "start_time": "2024-01-09T09:09:42.046986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Learning and Evaluation >\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Info] Total Bins 31864\n",
      "[LightGBM] [Info] Number of data points in the train set: 1976, number of used features: 213\n",
      "[LightGBM] [Info] Start training from score 3.711032\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Info] Total Bins 31911\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 214\n",
      "[LightGBM] [Info] Start training from score 3.710420\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Info] Total Bins 31898\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 214\n",
      "[LightGBM] [Info] Start training from score 3.710420\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Info] Total Bins 31928\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 214\n",
      "[LightGBM] [Info] Start training from score 3.712443\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.030531675309677896, reg_alpha=0.5220090593805945 will be ignored. Current value: lambda_l1=0.030531675309677896\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8719524472246258, subsample=0.8100111797463894 will be ignored. Current value: bagging_fraction=0.8719524472246258\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.0004004028086907504, reg_lambda=8.852759022621703 will be ignored. Current value: lambda_l2=0.0004004028086907504\n",
      "[LightGBM] [Info] Total Bins 31873\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 214\n",
      "[LightGBM] [Info] Start training from score 3.711937\n"
     ]
    }
   ],
   "source": [
    "print('< Learning and Evaluation >')\n",
    "param_best = {'reg_alpha': 0.5220090593805945, 'reg_lambda': 8.852759022621703, 'colsample_bytree': 0.5401337103950415, 'subsample': 0.8100111797463894, 'learning_rate': 0.012550862839485352, 'max_depth': 20, 'num_leaves': 11, 'min_child_samples': 10, 'bagging_fraction': 0.8719524472246258, 'lambda_l1': 0.030531675309677896, 'lambda_l2': 0.0004004028086907504, 'bagging_freq': 1}\n",
    "param = {'n_estimators': 1024,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': 42,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 1,\n",
    "         **param_best\n",
    "}\n",
    "\n",
    "solution = LGBMRegressor(**param)\n",
    "lx_off_pred1, lx_y_pred1   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67d9a06a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:09:59.561882Z",
     "iopub.status.busy": "2024-01-09T09:09:59.561513Z",
     "iopub.status.idle": "2024-01-09T09:10:02.298257Z",
     "shell.execute_reply": "2024-01-09T09:10:02.297254Z"
    },
    "papermill": {
     "duration": 3.147204,
     "end_time": "2024-01-09T09:10:02.300665",
     "exception": false,
     "start_time": "2024-01-09T09:09:59.153461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332d22208d354b98a81a2a1c505d491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed497b1c96e48628e3ba5630c903312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "tokenized_texts_train = []\n",
    "for text in tqdm(train_essays['word'].tolist()):\n",
    "    # tokenized_texts_train.append(tokenizer.tokenize(text))\n",
    "    tokenized_texts_train.append(text)\n",
    "\n",
    "tokenized_texts_test = []\n",
    "for text in tqdm(test_essays['word'].tolist()):\n",
    "    # tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "    tokenized_texts_test.append(text)\n",
    "\n",
    "text_tfidf = tokenized_texts_train+tokenized_texts_test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def dummy(text):\n",
    "    return text\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                            max_features=150,\n",
    "                            lowercase=False, \n",
    "                            sublinear_tf=True, \n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = dummy,\n",
    "                            preprocessor = dummy,\n",
    "                            token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "vectorizer.fit(text_tfidf)\n",
    "tffeature_train = vectorizer.transform(tokenized_texts_train).toarray()\n",
    "tffeature_test = vectorizer.transform(tokenized_texts_test).toarray()\n",
    "\n",
    "\n",
    "collst = []\n",
    "for i in range(150):\n",
    "    tpcol = f'tffeat_{i}'\n",
    "    collst.append(tpcol)\n",
    "    x[tpcol] = tffeature_train[:,i]\n",
    "    testin_x[tpcol] = tffeature_test[:,i]\n",
    "    data[tpcol] = tffeature_train[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f77797e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:10:03.117320Z",
     "iopub.status.busy": "2024-01-09T09:10:03.116521Z",
     "iopub.status.idle": "2024-01-09T09:10:42.859615Z",
     "shell.execute_reply": "2024-01-09T09:10:42.858623Z"
    },
    "papermill": {
     "duration": 40.15286,
     "end_time": "2024-01-09T09:10:42.862646",
     "exception": false,
     "start_time": "2024-01-09T09:10:02.709786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Learning and Evaluation >\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Info] Total Bins 69200\n",
      "[LightGBM] [Info] Number of data points in the train set: 1976, number of used features: 359\n",
      "[LightGBM] [Info] Start training from score 3.711032\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Info] Total Bins 69292\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 359\n",
      "[LightGBM] [Info] Start training from score 3.710420\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Info] Total Bins 69274\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 359\n",
      "[LightGBM] [Info] Start training from score 3.710420\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Info] Total Bins 69289\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 359\n",
      "[LightGBM] [Info] Start training from score 3.712443\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.8206447468653433e-05, reg_alpha=0.31392305787884783 will be ignored. Current value: lambda_l1=2.8206447468653433e-05\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7037344869543838, subsample=0.7167488247593671 will be ignored. Current value: bagging_fraction=0.7037344869543838\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.25550846605472e-08, reg_lambda=0.048717707444993374 will be ignored. Current value: lambda_l2=2.25550846605472e-08\n",
      "[LightGBM] [Info] Total Bins 69226\n",
      "[LightGBM] [Info] Number of data points in the train set: 1977, number of used features: 359\n",
      "[LightGBM] [Info] Start training from score 3.711937\n"
     ]
    }
   ],
   "source": [
    "print('< Learning and Evaluation >')\n",
    "# param_best = {'reg_alpha': 0.5220090593805945, 'reg_lambda': 8.852759022621703, 'colsample_bytree': 0.5401337103950415, 'subsample': 0.8100111797463894, 'learning_rate': 0.012550862839485352, 'max_depth': 20, 'num_leaves': 11, 'min_child_samples': 10, 'bagging_fraction': 0.8719524472246258, 'lambda_l1': 0.030531675309677896, 'lambda_l2': 0.0004004028086907504, 'bagging_freq': 1}\n",
    "param_best = {'reg_alpha': 0.31392305787884783, 'reg_lambda': 0.048717707444993374, 'colsample_bytree': 0.5487892487545942, 'subsample': 0.7167488247593671, 'learning_rate': 0.0074954259728873115, 'max_depth': 20, 'num_leaves': 17, 'min_child_samples': 32, 'bagging_fraction': 0.7037344869543838, 'lambda_l1': 2.8206447468653433e-05, 'lambda_l2': 2.25550846605472e-08, 'bagging_freq': 2}\n",
    "param = {'n_estimators': 1024,\n",
    "         'metric': 'rmse',\n",
    "         'random_state': 42,\n",
    "         'force_col_wise': True,\n",
    "         'verbosity': 1,\n",
    "         **param_best\n",
    "}\n",
    "\n",
    "solution = LGBMRegressor(**param)\n",
    "lx_off_pred2, lx_y_pred2   = evaluate(x.copy(), y.copy(), solution, test_x=testin_x.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b827218f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:10:43.760928Z",
     "iopub.status.busy": "2024-01-09T09:10:43.760044Z",
     "iopub.status.idle": "2024-01-09T09:10:44.019969Z",
     "shell.execute_reply": "2024-01-09T09:10:44.019022Z"
    },
    "papermill": {
     "duration": 0.678544,
     "end_time": "2024-01-09T09:10:44.022148",
     "exception": false,
     "start_time": "2024-01-09T09:10:43.343604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition OOF score = 0.59565\n",
      "Composition best W = 0.236\n"
     ]
    }
   ],
   "source": [
    "best_sc = 1\n",
    "for w in np.arange(0, 1.001, 0.001):\n",
    "    sc = metrics.mean_squared_error(y, \n",
    "                                    w * lx_off_pred1 + (1-w) * lx_off_pred2, \n",
    "                                    squared=False)\n",
    "    if sc < best_sc:\n",
    "        best_sc = sc\n",
    "        best_w = w\n",
    "        \n",
    "print('Composition OOF score = {:.5f}'.format(best_sc))\n",
    "print('Composition best W = {:.3f}'.format(best_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f275736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:10:44.834419Z",
     "iopub.status.busy": "2024-01-09T09:10:44.833644Z",
     "iopub.status.idle": "2024-01-09T09:10:44.838730Z",
     "shell.execute_reply": "2024-01-09T09:10:44.837739Z"
    },
    "papermill": {
     "duration": 0.420578,
     "end_time": "2024-01-09T09:10:44.840798",
     "exception": false,
     "start_time": "2024-01-09T09:10:44.420220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# W = [best_w, 1 - best_w]\n",
    "#lx_off_pred = lx_off_pred1 * W[0] + lx_off_pred2 * W[1]\n",
    "# lx_y_pred = lx_y_pred1 * W[0] + lx_y_pred2 * W[1]\n",
    "lx_off_pred = lx_off_pred1 * 0.5 + lx_off_pred2 * 0.5\n",
    "lx_y_pred = lx_y_pred1*0.5+lx_y_pred2* 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af14b32",
   "metadata": {
    "papermill": {
     "duration": 0.404663,
     "end_time": "2024-01-09T09:10:45.649121",
     "exception": false,
     "start_time": "2024-01-09T09:10:45.244458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.82 public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b095b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:10:46.519562Z",
     "iopub.status.busy": "2024-01-09T09:10:46.519054Z",
     "iopub.status.idle": "2024-01-09T09:46:11.416426Z",
     "shell.execute_reply": "2024-01-09T09:46:11.415427Z"
    },
    "papermill": {
     "duration": 2125.368012,
     "end_time": "2024-01-09T09:46:11.419101",
     "exception": false,
     "start_time": "2024-01-09T09:10:46.051089",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 165\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "Started the catboost model...\n",
      "Fold: 0 - Score: 0.61398\n",
      "Fold: 1 - Score: 0.59990\n",
      "Fold: 2 - Score: 0.60852\n",
      "Fold: 3 - Score: 0.62396\n",
      "Fold: 4 - Score: 0.65914\n",
      "catboost_seed0 cv_score:  0.62144\n",
      "Started the xgboost model...\n",
      "Fold: 0 - Score: 0.60045\n",
      "Fold: 1 - Score: 0.60438\n",
      "Fold: 2 - Score: 0.60173\n",
      "Fold: 3 - Score: 0.61313\n",
      "Fold: 4 - Score: 0.66174\n",
      "xgboost_seed0 cv_score:  0.61671\n",
      "Started the lgbm model...\n",
      "Fold: 0 - Score: 0.60417\n",
      "Fold: 1 - Score: 0.59835\n",
      "Fold: 2 - Score: 0.59072\n",
      "Fold: 3 - Score: 0.61666\n",
      "Fold: 4 - Score: 0.67026\n",
      "lgbm_seed0 cv_score:  0.61668\n",
      "Started the catboost model...\n",
      "Fold: 0 - Score: 0.61954\n",
      "Fold: 1 - Score: 0.62991\n",
      "Fold: 2 - Score: 0.60162\n",
      "Fold: 3 - Score: 0.65541\n",
      "Fold: 4 - Score: 0.60547\n",
      "catboost_seed1 cv_score:  0.62269\n",
      "Started the xgboost model...\n",
      "Fold: 0 - Score: 0.62675\n",
      "Fold: 1 - Score: 0.63047\n",
      "Fold: 2 - Score: 0.60634\n",
      "Fold: 3 - Score: 0.64561\n",
      "Fold: 4 - Score: 0.61001\n",
      "xgboost_seed1 cv_score:  0.624\n",
      "Started the lgbm model...\n",
      "Fold: 0 - Score: 0.61715\n",
      "Fold: 1 - Score: 0.63666\n",
      "Fold: 2 - Score: 0.60217\n",
      "Fold: 3 - Score: 0.63739\n",
      "Fold: 4 - Score: 0.60677\n",
      "lgbm_seed1 cv_score:  0.6202\n",
      "Started the catboost model...\n",
      "Fold: 0 - Score: 0.61817\n",
      "Fold: 1 - Score: 0.62091\n",
      "Fold: 2 - Score: 0.63380\n",
      "Fold: 3 - Score: 0.59850\n",
      "Fold: 4 - Score: 0.60800\n",
      "catboost_seed2 cv_score:  0.616\n",
      "Started the xgboost model...\n",
      "Fold: 0 - Score: 0.62361\n",
      "Fold: 1 - Score: 0.63200\n",
      "Fold: 2 - Score: 0.63867\n",
      "Fold: 3 - Score: 0.60336\n",
      "Fold: 4 - Score: 0.60506\n",
      "xgboost_seed2 cv_score:  0.6207\n",
      "Started the lgbm model...\n",
      "Fold: 0 - Score: 0.62625\n",
      "Fold: 1 - Score: 0.62937\n",
      "Fold: 2 - Score: 0.64566\n",
      "Fold: 3 - Score: 0.60504\n",
      "Fold: 4 - Score: 0.59831\n",
      "lgbm_seed2 cv_score:  0.62117\n",
      "Started the catboost model...\n",
      "Fold: 0 - Score: 0.58912\n",
      "Fold: 1 - Score: 0.61632\n",
      "Fold: 2 - Score: 0.64276\n",
      "Fold: 3 - Score: 0.63255\n",
      "Fold: 4 - Score: 0.61829\n",
      "catboost_seed3 cv_score:  0.62006\n",
      "Started the xgboost model...\n",
      "Fold: 0 - Score: 0.60139\n",
      "Fold: 1 - Score: 0.62286\n",
      "Fold: 2 - Score: 0.64780\n",
      "Fold: 3 - Score: 0.63635\n",
      "Fold: 4 - Score: 0.62726\n",
      "xgboost_seed3 cv_score:  0.62731\n",
      "Started the lgbm model...\n",
      "Fold: 0 - Score: 0.60052\n",
      "Fold: 1 - Score: 0.61816\n",
      "Fold: 2 - Score: 0.63448\n",
      "Fold: 3 - Score: 0.62880\n",
      "Fold: 4 - Score: 0.62022\n",
      "lgbm_seed3 cv_score:  0.62054\n",
      "Started the catboost model...\n",
      "Fold: 0 - Score: 0.58697\n",
      "Fold: 1 - Score: 0.63713\n",
      "Fold: 2 - Score: 0.63657\n",
      "Fold: 3 - Score: 0.61380\n",
      "Fold: 4 - Score: 0.62411\n",
      "catboost_seed4 cv_score:  0.61998\n",
      "Started the xgboost model...\n",
      "Fold: 0 - Score: 0.58598\n",
      "Fold: 1 - Score: 0.64144\n",
      "Fold: 2 - Score: 0.64912\n",
      "Fold: 3 - Score: 0.62378\n",
      "Fold: 4 - Score: 0.61350\n",
      "xgboost_seed4 cv_score:  0.62315\n",
      "Started the lgbm model...\n",
      "Fold: 0 - Score: 0.58795\n",
      "Fold: 1 - Score: 0.62913\n",
      "Fold: 2 - Score: 0.65607\n",
      "Fold: 3 - Score: 0.61775\n",
      "Fold: 4 - Score: 0.60718\n",
      "lgbm_seed4 cv_score:  0.62002\n",
      "[0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\n",
      " 0.06666667 0.06666667 0.06666667]\n",
      "Optimized Weights: [0.00000000e+00 2.09479166e-01 1.58648965e-01 0.00000000e+00\n",
      " 9.04626523e-03 9.59147410e-02 3.20769315e-01 2.76020656e-02\n",
      " 1.69609069e-18 3.14254984e-02 6.64325190e-18 5.22135863e-02\n",
      " 9.67266068e-03 2.32059267e-02 6.20218099e-02]\n",
      "cv_score with optimized weights:  0.60734\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "    \n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    \n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< P-bursts features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'), \n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    \n",
    "    return feats\n",
    "\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]\n",
    "\n",
    "\n",
    "data_path     = '/kaggle/input/linking-writing-processes-to-writing-quality/'\n",
    "train_logs    = pl.scan_csv(data_path + 'train_logs.csv')\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pd.read_csv(data_path + 'train_scores.csv')\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(data_path + 'test_logs.csv')\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')\n",
    "\n",
    "test_ids = test_feats['id'].values\n",
    "testin_x = test_feats.drop(['id'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "data[\"fold\"] = -1\n",
    "for idx, (train_idx, val_idx) in enumerate(skf.split(data, data[\"score\"].astype(str))):\n",
    "    data.loc[val_idx, \"fold\"] = idx\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "drop_cols = [\"id\", \"score\", \"fold\"]\n",
    "oof_df = pd.DataFrame()\n",
    "models = defaultdict(list)\n",
    "models_to_ensemble = [\"catboost\", \"xgboost\", \"lgbm\"]\n",
    "model_params = {\n",
    "    \"catboost\": {\n",
    "        \"iterations\": 5000,\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"depth\": 6,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"random_seed\": 42,\n",
    "        \"silent\": True\n",
    "    },\n",
    "    \"lgbm\": {\n",
    "        'n_estimators': 1024,\n",
    "        'learning_rate': 0.005,\n",
    "        'metric': 'rmse',\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True,\n",
    "        'verbosity': 0\n",
    "    },\n",
    "    \"xgboost\": {\n",
    "        \"max_depth\": 4,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"num_estimators\": 1000,\n",
    "        \"num_boost_round\": 1000,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"seed\": 42\n",
    "    },\n",
    "}\n",
    "OOF_DF = data[['id','score']]\n",
    "n_splits = 5\n",
    "for i in list(range(5)):\n",
    "    for idx, model_name in enumerate(models_to_ensemble):\n",
    "        params = model_params[model_name]\n",
    "        oof_folds = np.zeros(len(data))\n",
    "        print(f\"Started the {model_name} model...\")\n",
    "        for j,fold in enumerate(range(n_splits)):\n",
    "            if model_name == \"lgbm\":\n",
    "                model = LGBMRegressor(**params)\n",
    "            elif model_name == \"xgboost\":\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "            elif model_name == \"catboost\":\n",
    "                model = CatBoostRegressor(**params)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown base model name.\")\n",
    "            save_model_name = model_name+f\"_seed{i}\"\n",
    "            oof_cols_name_ = model_name+f\"_seed{i}\"\n",
    "            if j==0:\n",
    "                OOF_DF[f'{oof_cols_name_}_preds'] = np.zeros(len(data))\n",
    "            skf = StratifiedKFold(n_splits=n_splits, random_state=42+i, shuffle=True)\n",
    "            data[\"fold\"] = -1\n",
    "            for idx, (train_idx, val_idx) in enumerate(skf.split(data, data[\"score\"].astype(str))):\n",
    "                data.loc[val_idx, \"fold\"] = idx\n",
    "\n",
    "            x_train = data[data[\"fold\"] != fold].reset_index(drop=True)\n",
    "            x_valid = data[data[\"fold\"] == fold].reset_index(drop=True)\n",
    "            valid_index = data[data[\"fold\"] == fold].index\n",
    "            y_train = x_train[\"score\"]\n",
    "            y_valid = x_valid[\"score\"]\n",
    "            ids = x_valid[\"id\"]\n",
    "\n",
    "            x_train = x_train.drop(drop_cols, axis=\"columns\")\n",
    "            x_valid = x_valid.drop(drop_cols, axis=\"columns\")\n",
    "            model.fit(x_train, y_train)\n",
    "            val_preds = model.predict(x_valid)\n",
    "            oof_fold = pd.concat([ids, y_valid, pd.Series(val_preds)],axis=1).rename({0: f\"{oof_cols_name_}_preds\"}, axis=\"columns\")\n",
    "            oof_folds[valid_index] = oof_fold[f\"{oof_cols_name_}_preds\"]\n",
    "            models[save_model_name].append(model)\n",
    "            print(f\"Fold: {fold} - Score: {rmse(oof_fold['score'], oof_fold[f'{oof_cols_name_}_preds']):.5f}\")\n",
    "        OOF_DF[f\"{oof_cols_name_}_preds\"] = oof_folds\n",
    "        cv_score = rmse(OOF_DF[\"score\"], OOF_DF[f\"{oof_cols_name_}_preds\"])\n",
    "        print(f\"{oof_cols_name_} cv_score: \", round(cv_score, 5))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "pred_cols = [i for i in OOF_DF.columns if i not in ['id','score']]\n",
    "true_targets = OOF_DF[\"score\"]\n",
    "\n",
    "def objective_function(weights):\n",
    "    ensemble_preds = (OOF_DF[pred_cols] * weights).sum(axis=1)\n",
    "    score = rmse(OOF_DF[\"score\"], ensemble_preds)\n",
    "    return score\n",
    "\n",
    "def find_weights(oof_df):\n",
    "    len_models = len(pred_cols)\n",
    "    initial_weights = np.ones(len_models) / len_models\n",
    "    print(initial_weights)\n",
    "    bounds = [(0, 1)] * len_models\n",
    "    result = minimize(objective_function, initial_weights, bounds=bounds, method='SLSQP') # L-BFGS-B\n",
    "    optimized_weights = result.x\n",
    "    optimized_weights /= np.sum(optimized_weights)\n",
    "    return optimized_weights\n",
    "\n",
    "optimized_weights = find_weights(OOF_DF)\n",
    "print(\"Optimized Weights:\", optimized_weights)\n",
    "\n",
    "OOF_DF[\"ensemble_optimized_preds\"] = (OOF_DF[pred_cols] * optimized_weights).sum(axis=1)\n",
    "cv_optimized = rmse(OOF_DF[\"score\"], OOF_DF[\"ensemble_optimized_preds\"])\n",
    "print(\"cv_score with optimized weights: \", round(cv_optimized, 5))\n",
    "\n",
    "preds = None\n",
    "MODELS = [f\"{i}_seed{j}\" for j in range(5) for i in models_to_ensemble]\n",
    "for weights, model_name in zip(optimized_weights,MODELS):\n",
    "    models_list = models[model_name]\n",
    "    current_preds = weights * np.mean([model.predict(testin_x) for model in models_list], axis=0)\n",
    "    if preds is None:\n",
    "        preds = current_preds\n",
    "    else:\n",
    "        preds += current_preds\n",
    "\n",
    "y_pred581 = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0563de",
   "metadata": {
    "papermill": {
     "duration": 0.412168,
     "end_time": "2024-01-09T09:46:12.319556",
     "exception": false,
     "start_time": "2024-01-09T09:46:11.907388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ad549d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:46:13.149856Z",
     "iopub.status.busy": "2024-01-09T09:46:13.149128Z",
     "iopub.status.idle": "2024-01-09T09:46:13.168006Z",
     "shell.execute_reply": "2024-01-09T09:46:13.166886Z"
    },
    "papermill": {
     "duration": 0.437957,
     "end_time": "2024-01-09T09:46:13.170146",
     "exception": false,
     "start_time": "2024-01-09T09:46:12.732189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric blend = 0.58983\n"
     ]
    }
   ],
   "source": [
    "optimize_cv = False\n",
    "if optimize_cv:\n",
    "    def objective_function(weights):\n",
    "        ensemble_preds = (OOF_PREDS_ALL[['LGBM_pandas','MLP','LGMB_polars1','LGBM_polars2','LGBM_polars_public']] * weights).sum(axis=1)\n",
    "        score = rmse(OOF_PREDS_ALL[\"score\"], ensemble_preds)\n",
    "        return score\n",
    "\n",
    "    def find_weights(oof_df):\n",
    "        len_models = len(['LGBM_pandas','MLP','LGMB_polars1','LGBM_polars2','LGBM_polars_public'])\n",
    "        initial_weights = np.ones(len_models) / len_models\n",
    "        print(initial_weights)\n",
    "        bounds = [(0, 1)] * len_models\n",
    "        result = minimize(objective_function, initial_weights, bounds=bounds, method='SLSQP') # L-BFGS-B\n",
    "        optimized_weights = result.x\n",
    "        optimized_weights /= np.sum(optimized_weights)\n",
    "        return optimized_weights\n",
    "\n",
    "    OOF_PREDS_ALL = pd.DataFrame()\n",
    "    OOF_PREDS_ALL['score'] = train_scores['score']\n",
    "    OOF_PREDS_ALL['LGBM_pandas'] = OOF_PREDS_lgbm_pandas\n",
    "    OOF_PREDS_ALL['MLP'] = OOF_PREDS_mlp\n",
    "    OOF_PREDS_ALL['LGMB_polars1'] = lx_off_pred1\n",
    "    OOF_PREDS_ALL['LGBM_polars2'] = lx_off_pred2\n",
    "    OOF_PREDS_ALL['LGBM_polars_public'] = OOF_DF[\"ensemble_optimized_preds\"]\n",
    "\n",
    "    optimized_weights = find_weights(OOF_PREDS_ALL)\n",
    "    print(\"Optimized Weights:\", optimized_weights)\n",
    "\n",
    "    OOF_PREDS_ALL[\"ensemble_all\"] = (OOF_PREDS_ALL[['LGBM_pandas','MLP','LGMB_polars1','LGBM_polars2','LGBM_polars_public']] * optimized_weights).sum(axis=1)\n",
    "    cv_optimized = rmse(OOF_PREDS_ALL[\"score\"], OOF_PREDS_ALL[\"ensemble_all\"])\n",
    "    print('cv_optimized',cv_optimized)\n",
    "    test_preds_blend = TEST_PREDS_lgbm_pandas * optimized_weights[0] + TEST_PREDS_mlp * optimized_weights[1] +lx_y_pred1 * optimized_weights[2] + lx_y_pred2 * optimized_weights[3] + y_pred581*optimized_weights[4]\n",
    "    test_preds_blend\n",
    "    \n",
    "else:\n",
    "    \n",
    "    weights = [0.35,0.35,0.03,0.27]  \n",
    "    OOF_PREDS_blend = lx_off_pred*weights[0] + OOF_PREDS_lgbm_pandas*weights[1] + OOF_DF[\"ensemble_optimized_preds\"]*weights[2] + OOF_PREDS_mlp*weights[3]\n",
    "    print('OOF metric blend = {:.5f}'.format(metrics.mean_squared_error(train_scores[target_col], OOF_PREDS_blend, squared=False)))\n",
    "    test_preds_blend = lx_y_pred * weights[0] + TEST_PREDS_lgbm_pandas * weights[1] + y_pred581*weights[2] + TEST_PREDS_mlp*weights[3]\n",
    "    test_preds_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e14aba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:46:14.019122Z",
     "iopub.status.busy": "2024-01-09T09:46:14.018221Z",
     "iopub.status.idle": "2024-01-09T09:46:14.026395Z",
     "shell.execute_reply": "2024-01-09T09:46:14.025345Z"
    },
    "papermill": {
     "duration": 0.425036,
     "end_time": "2024-01-09T09:46:14.028802",
     "exception": false,
     "start_time": "2024-01-09T09:46:13.603766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_feats['score'] = test_preds_blend\n",
    "test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a538185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T09:46:14.971179Z",
     "iopub.status.busy": "2024-01-09T09:46:14.970475Z",
     "iopub.status.idle": "2024-01-09T09:46:14.984473Z",
     "shell.execute_reply": "2024-01-09T09:46:14.983428Z"
    },
    "papermill": {
     "duration": 0.524491,
     "end_time": "2024-01-09T09:46:14.986713",
     "exception": false,
     "start_time": "2024-01-09T09:46:14.462222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.362051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1.116869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1.114732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.362051\n",
       "1  2222bbbb  1.116869\n",
       "2  4444cccc  1.114732"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats[['id', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2b144",
   "metadata": {
    "papermill": {
     "duration": 0.404063,
     "end_time": "2024-01-09T09:46:15.796259",
     "exception": false,
     "start_time": "2024-01-09T09:46:15.392196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    },
    {
     "datasetId": 3949123,
     "sourceId": 6973319,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4169445,
     "sourceId": 7206892,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 150384981,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 153539261,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154091050,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154250178,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 156990507,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5903.42551,
   "end_time": "2024-01-09T09:46:19.906766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-09T08:07:56.481256",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "039983ae26314ead8501d092b5f87f6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08a346ab18e74c588930f4a7b6e6919c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e69aaf459d524ddabc0fce53b1306993",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_039983ae26314ead8501d092b5f87f6e",
       "value": 3.0
      }
     },
     "0fa4250191f3440eb44469699de1f574": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e213ec6c7604b108a0237dbfa0e148b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8cce7548c2224ec3ac2e322d55272feb",
       "placeholder": "​",
       "style": "IPY_MODEL_f5bd794f442c434d93853677c456bd8d",
       "value": " 3/3 [00:00&lt;00:00, 255.75it/s]"
      }
     },
     "263e49d4b7ab4e90838a7b7549226141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2b57aee5b213404ba39a1f9f46a806bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2ed497b1c96e48628e3ba5630c903312": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3794ebf96f07449f882d3edbc4e7ee5e",
        "IPY_MODEL_08a346ab18e74c588930f4a7b6e6919c",
        "IPY_MODEL_1e213ec6c7604b108a0237dbfa0e148b"
       ],
       "layout": "IPY_MODEL_ff1ad0b2c38449a7a0ffddde55460760"
      }
     },
     "332d22208d354b98a81a2a1c505d491c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8c1ce9c62108448291374076eae6dd81",
        "IPY_MODEL_e968739be37c4e0bbba3cc5b2a85500b",
        "IPY_MODEL_b32b7f51a7044b60aa46318cee388d51"
       ],
       "layout": "IPY_MODEL_b555bc5c59af45a2b1c8371aea5dc06c"
      }
     },
     "3794ebf96f07449f882d3edbc4e7ee5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_870745d59c134620a0baaf20ac374b85",
       "placeholder": "​",
       "style": "IPY_MODEL_a86c4f38732940ec9259639dfed72819",
       "value": "100%"
      }
     },
     "4ddcc0316788412289aa259550f263a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7dbf40db5f5b447aa2472cf7730bdb3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "870745d59c134620a0baaf20ac374b85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c1ce9c62108448291374076eae6dd81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4ddcc0316788412289aa259550f263a5",
       "placeholder": "​",
       "style": "IPY_MODEL_263e49d4b7ab4e90838a7b7549226141",
       "value": "100%"
      }
     },
     "8cce7548c2224ec3ac2e322d55272feb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7e4c16f01304f7480daeb5b63b0fb75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a86c4f38732940ec9259639dfed72819": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b32b7f51a7044b60aa46318cee388d51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a7e4c16f01304f7480daeb5b63b0fb75",
       "placeholder": "​",
       "style": "IPY_MODEL_2b57aee5b213404ba39a1f9f46a806bd",
       "value": " 2471/2471 [00:00&lt;00:00, 165982.69it/s]"
      }
     },
     "b555bc5c59af45a2b1c8371aea5dc06c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e69aaf459d524ddabc0fce53b1306993": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e968739be37c4e0bbba3cc5b2a85500b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0fa4250191f3440eb44469699de1f574",
       "max": 2471.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7dbf40db5f5b447aa2472cf7730bdb3f",
       "value": 2471.0
      }
     },
     "f5bd794f442c434d93853677c456bd8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ff1ad0b2c38449a7a0ffddde55460760": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
