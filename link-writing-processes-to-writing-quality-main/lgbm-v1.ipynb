{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1146b9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:15:39.636156Z",
     "iopub.status.busy": "2023-12-31T03:15:39.635379Z",
     "iopub.status.idle": "2023-12-31T03:15:46.737933Z",
     "shell.execute_reply": "2023-12-31T03:15:46.736644Z"
    },
    "papermill": {
     "duration": 7.118262,
     "end_time": "2023-12-31T03:15:46.740698",
     "exception": false,
     "start_time": "2023-12-31T03:15:39.622436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "from itertools import combinations  \n",
    "from scipy.stats import skew\n",
    "import copy\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings  \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import optuna\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5e617",
   "metadata": {
    "papermill": {
     "duration": 0.011183,
     "end_time": "2023-12-31T03:15:46.762774",
     "exception": false,
     "start_time": "2023-12-31T03:15:46.751591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52009ffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:15:46.786153Z",
     "iopub.status.busy": "2023-12-31T03:15:46.785750Z",
     "iopub.status.idle": "2023-12-31T03:16:03.978766Z",
     "shell.execute_reply": "2023-12-31T03:16:03.977556Z"
    },
    "papermill": {
     "duration": 17.208279,
     "end_time": "2023-12-31T03:16:03.982027",
     "exception": false,
     "start_time": "2023-12-31T03:15:46.773748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "train_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\n",
    "train_essays.index = train_essays[\"Unnamed: 0\"]\n",
    "train_essays.index.name = None\n",
    "train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_essays_with_upper = pd.read_csv('/kaggle/input/essays-generator-with-upper/essays_with_upper.csv') \n",
    "train_essays_with_upper.index = train_essays_with_upper[\"Unnamed: 0\"]\n",
    "train_essays_with_upper.index.name = None\n",
    "train_essays_with_upper.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96710163",
   "metadata": {
    "papermill": {
     "duration": 0.010195,
     "end_time": "2023-12-31T03:16:04.003018",
     "exception": false,
     "start_time": "2023-12-31T03:16:03.992823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a7e59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.026162Z",
     "iopub.status.busy": "2023-12-31T03:16:04.025725Z",
     "iopub.status.idle": "2023-12-31T03:16:04.031545Z",
     "shell.execute_reply": "2023-12-31T03:16:04.030312Z"
    },
    "papermill": {
     "duration": 0.020404,
     "end_time": "2023-12-31T03:16:04.034063",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.013659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocessing(df,dataset='test'):\n",
    "#     if dataset == 'train':\n",
    "#         add_value = 66231-17831+500\n",
    "#         df.loc[(df['id']=='a0c24719')&(df['event_id']>68),'down_time'] += add_value\n",
    "#         df.loc[(df['id']=='a0c24719')&(df['event_id']>68),'up_time'] += add_value\n",
    "#     for i in range(1,4):\n",
    "#         df[f'down_event_shift{i}'] = df.groupby('id')['down_event'].shift(i)\n",
    "#     df['need_drop'] = np.zeros(len(df))\n",
    "#     df.loc[(df['down_event']=='Shift')&(df['down_event_shift1']=='Shift')&(df['down_event_shift2']=='Shift')&(df['down_event_shift3']=='Shift'),'need_drop'] = 1\n",
    "#     df = df[df['need_drop']==0]\n",
    "#     df['event_id'] = df.groupby('id').cumcount()\n",
    "#     return df.drop(columns=['down_event_shift1','down_event_shift2','down_event_shift3','need_drop'])\n",
    "# train_logs = preprocessing(train_logs,dataset='train')\n",
    "# test_logs = preprocessing(test_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8f06a",
   "metadata": {
    "papermill": {
     "duration": 0.010066,
     "end_time": "2023-12-31T03:16:04.054608",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.044542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47db12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.077730Z",
     "iopub.status.busy": "2023-12-31T03:16:04.077319Z",
     "iopub.status.idle": "2023-12-31T03:16:04.115641Z",
     "shell.execute_reply": "2023-12-31T03:16:04.114382Z"
    },
    "papermill": {
     "duration": 0.053482,
     "end_time": "2023-12-31T03:16:04.118507",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.065025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#essay生成：普通版本和带大写版本\n",
    "class EssayConstructor:\n",
    "    \n",
    "    def processingInputs(self,currTextInput):\n",
    "        # Where the essay content will be stored\n",
    "        essayText = \"\"\n",
    "        # Produces the essay\n",
    "        for Input in currTextInput.values:\n",
    "            # Input[0] = activity\n",
    "            # Input[1] = cursor_position\n",
    "            # Input[2] = text_change\n",
    "            # Input[3] = id\n",
    "            # If activity = Replace\n",
    "            if Input[0] == 'Replace':\n",
    "                # splits text_change at ' => '\n",
    "                replaceTxt = Input[2].split(' => ')\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Paste    \n",
    "            if Input[0] == 'Paste':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Remove/Cut\n",
    "            if Input[0] == 'Remove/Cut':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Move...\n",
    "            if \"M\" in Input[0]:\n",
    "                # Gets rid of the \"Move from to\" text\n",
    "                croppedTxt = Input[0][10:]              \n",
    "                # Splits cropped text by ' To '\n",
    "                splitTxt = croppedTxt.split(' To ')              \n",
    "                # Splits split text again by ', ' for each item\n",
    "                valueArr = [item.split(', ') for item in splitTxt]              \n",
    "                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "                # Skip if someone manages to activiate this by moving to same place\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    # Check if they move text forward in essay (they are different)\n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    else:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue                \n",
    "                \n",
    "            # If activity = input\n",
    "            # DONT TOUCH\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        return essayText\n",
    "            \n",
    "            \n",
    "    def getEssays(self,df):\n",
    "        # Copy required columns\n",
    "        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "        # Get rid of text inputs that make no change\n",
    "        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "        # construct essay, fast \n",
    "        tqdm.pandas()\n",
    "        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n",
    "        # to dataframe\n",
    "        essayFrame=essay.to_frame().reset_index()\n",
    "        essayFrame.columns=['id','essay']\n",
    "        # Returns the essay series\n",
    "        return essayFrame\n",
    "\n",
    "def getEssays_with_upper(df):\n",
    "    df['down_event_shift'] = df.groupby('id')['down_event'].shift(1)\n",
    "    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change','down_event','down_event_shift']]\n",
    "    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n",
    "    lastIndex = 0\n",
    "    essaySeries = pd.Series()\n",
    "    for index, valCount in enumerate(tqdm(valCountsArr)):\n",
    "        capital = False\n",
    "        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change','down_event','down_event_shift']].iloc[lastIndex : lastIndex + valCount]\n",
    "        lastIndex += valCount\n",
    "        essayText = \"\"\n",
    "        for Input in currTextInput.values:\n",
    "            if Input[3] == 'CapsLock':\n",
    "                capital = not capital\n",
    "            if Input[0] == 'Nonproduction':\n",
    "                continue\n",
    "            if Input[0] != 'Nonproduction':\n",
    "                if (Input[0] == 'Replace')&(Input[4] == 'Shift'):\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + (replaceTxt[1]).upper() +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Replace':\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Paste':\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                    continue\n",
    "                if Input[0] == 'Remove/Cut':\n",
    "                    essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                    continue\n",
    "                if \"M\" in Input[0]:\n",
    "                    croppedTxt = Input[0][10:]\n",
    "                    splitTxt = croppedTxt.split(' To ')\n",
    "                    valueArr = [item.split(', ') for item in splitTxt]\n",
    "                    moveData = (int(valueArr[0][0][1:]), \n",
    "                                int(valueArr[0][1][:-1]), \n",
    "                                int(valueArr[1][0][1:]), \n",
    "                                int(valueArr[1][1][:-1]))\n",
    "                    if moveData[0] != moveData[2]:\n",
    "                        if moveData[0] < moveData[2]:\n",
    "                            essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                            essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                        else:\n",
    "                            essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                            essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                    continue\n",
    "                if capital|((Input[4]=='Shift')&(Input[3]=='q')):\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2].upper() + essayText[Input[1] - len(Input[2]):]\n",
    "                else:\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        essaySeries[index] = essayText\n",
    "    essaySeries.index =  textInputDf['id'].unique()\n",
    "    return pd.DataFrame(essaySeries, columns=['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f91dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.141281Z",
     "iopub.status.busy": "2023-12-31T03:16:04.140842Z",
     "iopub.status.idle": "2023-12-31T03:16:04.146240Z",
     "shell.execute_reply": "2023-12-31T03:16:04.145294Z"
    },
    "papermill": {
     "duration": 0.019451,
     "end_time": "2023-12-31T03:16:04.148497",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.129046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#两个分位数\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8ff820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.172012Z",
     "iopub.status.busy": "2023-12-31T03:16:04.171391Z",
     "iopub.status.idle": "2023-12-31T03:16:04.177550Z",
     "shell.execute_reply": "2023-12-31T03:16:04.176686Z"
    },
    "papermill": {
     "duration": 0.020596,
     "end_time": "2023-12-31T03:16:04.179975",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.159379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(text):  \n",
    "    # 统计每个字符的出现次数  \n",
    "    char_count = {}  \n",
    "    for char in text:  \n",
    "        if char in char_count:  \n",
    "            char_count[char] += 1  \n",
    "        else:  \n",
    "            char_count[char] = 1  \n",
    "    probabilities = [float(char_count[char]) / len(text) for char in char_count]  \n",
    "    entropy = -sum([p * math.log2(p) for p in probabilities])  \n",
    "    return entropy  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224eb4b4",
   "metadata": {
    "papermill": {
     "duration": 0.00997,
     "end_time": "2023-12-31T03:16:04.201416",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.191446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edd2be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.225635Z",
     "iopub.status.busy": "2023-12-31T03:16:04.224936Z",
     "iopub.status.idle": "2023-12-31T03:16:04.403120Z",
     "shell.execute_reply": "2023-12-31T03:16:04.401776Z"
    },
    "papermill": {
     "duration": 0.194235,
     "end_time": "2023-12-31T03:16:04.406149",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.211914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessor_v1:\n",
    "    def __init__(self,seed,essays,essays_with_upper,train_scores=None,tokenizer=None,method='train',save_cols=None):\n",
    "        self.seed = seed\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_scores = train_scores\n",
    "        self.save_cols = save_cols\n",
    "        self.essays = essays\n",
    "        self.essays_with_upper = essays_with_upper\n",
    "        self.method =method \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes = ['q', ' ', 'NoChange', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/','@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "        self.idf = defaultdict(float)\n",
    "        self.text_changes_dict = {\n",
    "            'q': 'q', \n",
    "            ' ': 'space', \n",
    "            'NoChange': 'NoChange', \n",
    "            '.': 'full_stop', \n",
    "            ',': 'comma', \n",
    "            '\\n': 'newline', \n",
    "            \"'\": 'single_quote', \n",
    "            '\"': 'double_quote', \n",
    "            '-': 'dash', \n",
    "            '?': 'question_mark', \n",
    "            ';': 'semicolon', \n",
    "            '=': 'equals', \n",
    "            '/': 'slash', \n",
    "            '\\\\': 'double_backslash', \n",
    "            ':': 'colon'\n",
    "        }\n",
    "        self.AGGREGATIONS =  ['nunique','count', 'mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "        self.AGGREGATIONS2 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS3 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS4 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "        self.AGGREGATIONS5 = ['nunique', 'mean', 'std', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "                \n",
    "            di[\"move_to\"] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "                else:\n",
    "                    di[\"move_to\"] += v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'activity_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'{colname}_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "        ret = pd.DataFrame(ret)\n",
    "        cols = [f'text_change_{i}_count' for i in range(len(ret.columns))]\n",
    "        ret.columns = cols\n",
    "\n",
    "        cnts = ret.sum(1)\n",
    "\n",
    "        for col in cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "            \n",
    "            ret[col] = 1 + np.log(ret[col] / cnts)\n",
    "            ret[col] *= idf\n",
    "            \n",
    "        return ret\n",
    "\n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "    \n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def calculate_pauses(self, df, pause_threshold=2000):\n",
    "        # Compute IKI within each 'id' group\n",
    "        df['IKI'] = df.groupby('id')['down_time'].diff()\n",
    "\n",
    "        # Define pauses\n",
    "        df['is_pause'] = (df['IKI'] > pause_threshold)\n",
    "\n",
    "        # Compute statistics for IKI\n",
    "        iki_stats = df.groupby('id')['IKI'].agg(['mean', 'median', 'std', 'max']).reset_index().rename(columns={\n",
    "            'mean': 'iki_mean',\n",
    "            'median': 'iki_median',\n",
    "            'std': 'iki_std',\n",
    "            'max': 'iki_max'\n",
    "        })\n",
    "\n",
    "        # Compute pause counts\n",
    "        pause_counts = df.groupby('id')['is_pause'].sum().reset_index(name='pause_count')\n",
    "\n",
    "        # Compute average pause time excluding NaNs\n",
    "        pause_times = df[df['is_pause']].groupby('id')['IKI'].mean().reset_index(name='average_pause_time')\n",
    "\n",
    "        # Compute total pause time for paragraph\n",
    "        para_pause_duration = df.groupby('id').apply(lambda group: group['IKI'].where(group['text_change'] == '\\n').sum()).reset_index(name='para_pause_duration')\n",
    "\n",
    "        # Merge pause features\n",
    "        pause_features = pause_counts.merge(pause_times, on='id', how='left')\n",
    "        pause_features = pause_features.merge(para_pause_duration, on='id', how='left')\n",
    "        pause_features = pause_features.merge(iki_stats, on='id', how='left')\n",
    "\n",
    "        # Compute total IKI time and exclude NaNs\n",
    "        total_time = df.groupby('id')['IKI'].sum().reset_index(name='total_time')\n",
    "        \n",
    "        # Merge the total time into pause_features\n",
    "        pause_features = pause_features.merge(total_time, on='id', how='left')\n",
    "\n",
    "        # Calculate pause time ratio\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_count'] * pause_features['average_pause_time']\n",
    "        pause_features['pause_time_ratio'] = pause_features['pause_time_ratio'] / pause_features['total_time'].replace(0, np.nan)\n",
    "\n",
    "        # Calculate times between sentences within each 'id' group\n",
    "        df['sentence_end_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].isin(['.', '?', '!']))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for times between sentences\n",
    "        between_sentences_stats = df.groupby('id')['sentence_end_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_sentences_IKI', 'std': 'std_between_sentences_IKI'})\n",
    "\n",
    "        # Calculate within-word IKI for 'q' characters within each 'id'\n",
    "        df['within_word_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'] == 'q')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for within-word IKI\n",
    "        within_word_stats = df.groupby('id')['within_word_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_within_word_IKI', 'std': 'std_within_word_IKI'})\n",
    "\n",
    "        # Calculate between-words IKI for spaces or punctuation followed by 'q'\n",
    "        df['between_words_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].shift().isin([' '] + self.punctuations) & (group['text_change'] == 'q'))).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Calculate statistics for between-words IKI\n",
    "        between_words_stats = df.groupby('id')['between_words_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_words_IKI', 'std': 'std_between_words_IKI'})\n",
    "\n",
    "        # Combine all the IKI related features into one DataFrame\n",
    "        pause_features = pause_features.merge(between_sentences_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(within_word_stats, on='id', how='left')\n",
    "        pause_features = pause_features.merge(between_words_stats, on='id', how='left')\n",
    "\n",
    "        return pause_features\n",
    "\n",
    "    def brute_force_agg(self,df):\n",
    "        #bruteforce agg\n",
    "        agg_fe_df = df.groupby(\"id\")[['down_time', 'cursor_position', 'word_count']].agg(\n",
    "            ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "        agg_fe_df.columns = ['_'.join(x) for x in agg_fe_df.columns]\n",
    "        agg_fe_df = agg_fe_df.add_prefix(\"tmp_\")\n",
    "        agg_fe_df.reset_index(inplace=True)\n",
    "        return agg_fe_df\n",
    "    \n",
    "    def duration_features(self,df):\n",
    "        logs = copy.deepcopy(df)\n",
    "        logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "        logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "\n",
    "        group = logs.groupby('id')['time_diff']\n",
    "        initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        data = pd.DataFrame({\n",
    "            'id': logs['id'].unique(),\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True)\n",
    "        return data\n",
    "    \n",
    "    def essay_CountVectorizer_and_tfidf(self):\n",
    "        if self.method=='train':\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            merged_data = essaysdf.merge(self.train_scores, on='id')\n",
    "            count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "            tokenizer = count_vectorizer.fit_transform(merged_data['essay'])\n",
    "            y = merged_data['score']\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]) : \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            \n",
    "            save_cols = []\n",
    "            for i in count_vector.columns:\n",
    "                if sum(count_vector[i]==0)/len(count_vector)<0.1:\n",
    "                    save_cols.append(i)\n",
    "\n",
    "            return count_vector[save_cols],count_vectorizer,save_cols\n",
    "\n",
    "        else:\n",
    "            essaysdf = copy.deepcopy(self.essays['essay'])\n",
    "            essaysdf = pd.DataFrame({'id': essaysdf.index, 'essay': essaysdf.values})\n",
    "            tokenizer = self.tokenizer.transform(essaysdf['essay'])\n",
    "            tokenizer = tokenizer.todense()\n",
    "            count_vector = pd.DataFrame()\n",
    "            for i in range(tokenizer.shape[1]): \n",
    "                L = list(tokenizer[:,i])\n",
    "                li = [int(x) for x in L ]\n",
    "                count_vector[f'feature {i}'] = li\n",
    "            df_index = essaysdf['id']\n",
    "            count_vector.loc[:, 'id'] = df_index\n",
    "            return count_vector[self.save_cols]       \n",
    "        \n",
    "    def other_features(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        a['Input_all_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['activity']!='Input'))/df.groupby(['id']).apply(lambda x:sum(x['activity']=='Input'))\n",
    "        a['all_q_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['down_event']!='q'))/df.groupby(['id']).apply(lambda x:sum(x['down_event']=='q'))\n",
    "        activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        events_dict = {\n",
    "                        'q':'q', \n",
    "                        'Space':'Space', \n",
    "                        'Backspace':'Backspace', \n",
    "                        'Shift':'Shift', \n",
    "                        'ArrowRight':'ArrowRight', \n",
    "                        'Leftclick':'Leftclick', \n",
    "                        'ArrowLeft':'ArrowLeft', \n",
    "                        '.':'fullstop', \n",
    "                        ',':'comma', \n",
    "                        'ArrowDown':'ArrowDown', \n",
    "                        'ArrowUp':'ArrowUp', \n",
    "                        'Enter':'Enter', \n",
    "                        'CapsLock':'CapsLock', \n",
    "                        \"'\":'single_quote', \n",
    "                        'Delete':'Delete', \n",
    "                        'Unidentified':'Unidentified',\n",
    "                      }\n",
    "        for i in tqdm(activities):\n",
    "            for j in events_dict:\n",
    "                a[f'{i}_{events_dict[j]}_count'] = df.groupby('id').apply(lambda x:len(x[(x['activity']==i)&(x['down_event']==j)]))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df['down_event_shift'] = df.groupby('id')['down_event'].shift(-1)\n",
    "        letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')|((x['down_event']=='Shift')&(x['down_event_shift']=='q'))]))\n",
    "        a['letter_big_count'] = letter_upper.values\n",
    "        a['id'] = df['id'].unique()\n",
    "\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df['sent_len']!=0]\n",
    "        errors_num = (essay_df.groupby('id').apply(len)-letter_upper).values\n",
    "        a['error_num'] = errors_num                          #如果句子个数大于大写字母按键次数，那么文章会有语法错误\n",
    "\n",
    "        return a \n",
    "\n",
    "    def sentence_error(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        essay_df = essay_df[essay_df['paragraph_len']!=0]\n",
    "        essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        essay_df = essay_df[essay_df['only_space']==False]\n",
    "        a = pd.DataFrame()\n",
    "        a['para_error'] = essay_df.groupby('id').apply(lambda x:len(x[x['paragraph_len']<25]))   #一个段落字符过少可能不是完整的一句话，可能存在语法错误\n",
    "\n",
    "        return a.reset_index()\n",
    "\n",
    "    def language_error_letter(self):\n",
    "        essay_df = copy.deepcopy(self.essays_with_upper)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: str(x).replace('\\n','').strip()) \n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        essay_df['language_error_letter'] = essay_df['sent'].apply(lambda x:x[0])\n",
    "        essay_df['if_q'] = essay_df['language_error_letter'].apply(lambda x:x.lower()=='q')\n",
    "        essay_df = essay_df[essay_df['if_q']==True]\n",
    "        a = pd.DataFrame()\n",
    "        a['language_error_letter'] = essay_df.groupby('id').apply(lambda x:len(x[x['language_error_letter']=='q']))\n",
    "        return a.reset_index()\n",
    "\n",
    "    def R_burst(self,df):\n",
    "        a = pd.DataFrame()\n",
    "        df = df[(df['activity']=='Input')|(df['activity']=='Remove/Cut')].reset_index(drop=True)\n",
    "        df['activity_shift'] = df.groupby('id')['activity'].shift().fillna(method='bfill')\n",
    "        df['is_R_burst'] = df['activity'] != df['activity_shift']\n",
    "        a['revision_count'] = df.groupby('id').apply(lambda x:x['is_R_burst'].sum())\n",
    "        df['keystroke_duration'] = df.groupby('id')['down_time'].diff()\n",
    "        df = df[df['keystroke_duration'].notnull()]\n",
    "\n",
    "        a['revision_count_above2s'] = df.groupby('id').apply(lambda x:x[(x['is_R_burst']==True)&(x['keystroke_duration']>2)]['is_R_burst'].sum()).values\n",
    "        Rburst =  df[(df['is_R_burst']==True)&(df['keystroke_duration']>2)]   #&(df['keystroke_duration']>2)\n",
    "        Rburst_statistic = Rburst.groupby('id').agg({'keystroke_duration':['mean','max','sum','median']})\n",
    "        Rburst_statistic.columns = ['_'.join(x) for x in Rburst_statistic.columns]\n",
    "        \n",
    "        return a.merge(Rburst_statistic.reset_index(),on='id',how='left')\n",
    "\n",
    "\n",
    "    def split_essays_into_sentences(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "\n",
    "        #避免将qqq.).切分成多个句子\n",
    "        #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "\n",
    "        # Number of words in sentences\n",
    "        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['sent_word_count_diff'] = essay_df.groupby(['id'])['sent_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "        essay_df['words_len_above10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_above10'] = essay_df['words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['words_len_5-10'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_5-10'] = essay_df['words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['words_len_first'] = essay_df['sent'].apply(lambda x: x.split(' '))\n",
    "        essay_df['words_len_first'] = essay_df['words_len_first'].apply(lambda x:len(x[0]))\n",
    "\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "    \n",
    "    def compute_sentence_aggregations(self,df):\n",
    "        sent_agg_df = pd.concat(\n",
    "            [df[['id','sent_len']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','sent_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ],\n",
    "             axis=1)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index    \n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "        sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "        return sent_agg_df\n",
    "\n",
    "    def split_essays_into_paragraphs(self):\n",
    "        essay_df = copy.deepcopy(self.essays)\n",
    "        essay_df['id'] = essay_df.index\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        \n",
    "        # Number of words in paragraphs\n",
    "        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df['paragraph_word_count_diff'] = essay_df.groupby(['id'])['paragraph_word_count'].transform(lambda x:np.abs(x.diff()))\n",
    "\n",
    "        essay_df['para_words_len_above10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_above10'] = essay_df['para_words_len_above10'].apply(lambda x:sum(len(y)>10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_5-10'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_5-10'] = essay_df['para_words_len_5-10'].apply(lambda x:sum(5<=len(y)<=10 for y in x))\n",
    "\n",
    "        essay_df['para_words_len_first'] = essay_df['paragraph'].apply(lambda x: x.split(' '))\n",
    "        essay_df['para_words_len_first'] = essay_df['para_words_len_first'].apply(lambda x:len(x[0]))\n",
    "        \n",
    "        essay_df['num_question'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\?', x)))\n",
    "        essay_df['num_yinyong'] = essay_df['paragraph'].apply(lambda x: len(re.findall(r'\\\"', x)))\n",
    "\n",
    "        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "        #有些段落可能全部是空格，类似于：'    '\n",
    "        #essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "        #essay_df = essay_df[essay_df['only_space']==False]\n",
    "        return essay_df\n",
    "\n",
    "    def compute_paragraph_aggregations(self,df):\n",
    "        paragraph_agg_df = pd.concat(\n",
    "            [df[['id','paragraph_len']].groupby(['id']).agg(self.AGGREGATIONS),\\\n",
    "             df[['id','paragraph_word_count']].groupby(['id']).agg(self.AGGREGATIONS),\n",
    "             df[['id','paragraph_word_count_diff']].groupby(['id']).agg(self.AGGREGATIONS2),\n",
    "             df[['id','para_words_len_above10']].groupby(['id']).agg(self.AGGREGATIONS3),\n",
    "             df[['id','para_words_len_first']].groupby(['id']).agg(self.AGGREGATIONS4),\n",
    "             df[['id','para_words_len_5-10']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_question']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "             df[['id','num_yinyong']].groupby(['id']).agg(self.AGGREGATIONS5),\n",
    "\n",
    "             ], axis=1) \n",
    "        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "        paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "        paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "        paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "        return paragraph_agg_df\n",
    "    \n",
    "    def difficulty(self):\n",
    "        df = copy.deepcopy(self.essays)\n",
    "        df['token'] = [word_tokenize(p) for p in df[\"essay\"]]\n",
    "        df['token_len'] = df['token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df['verylong']  = df['token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df['long']      = df['token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df['mid']       = df['token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df['difficulty'] = df['verylong']*5 + df['long']*3 + df['mid']*1\n",
    "        df['long_words'] = df['verylong']+df['long']\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index':'id'},inplace=True)\n",
    "\n",
    "        #sentence\n",
    "        df_sentence = copy.deepcopy(self.essays)\n",
    "        df_sentence['id'] = df_sentence.index\n",
    "        #避免将类似于i.e.切分成多个句子\n",
    "        df_sentence['essay'] = df_sentence['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "        df_sentence['sent'] = df_sentence['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        df_sentence = df_sentence.explode('sent')   #explode将列表里的元素展开\n",
    "        df_sentence['sent'] = df_sentence['sent'].apply(lambda x: x.replace('\\n','').strip())    #strip会删除字符串两端的空格\n",
    "        # Number of characters in sentences\n",
    "        df_sentence['sent_len'] = df_sentence['sent'].apply(lambda x: len(x))\n",
    "        df_sentence['sent_word_count'] = df_sentence['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        df_sentence = df_sentence[df_sentence['sent_len']!=0]\n",
    "\n",
    "        df_sentence['sentence_token'] = [word_tokenize(p) for p in df_sentence[\"sent\"]]\n",
    "        df_sentence['sentence_token_len'] = df_sentence['sentence_token'].apply(lambda x : list(len(word) for word in x))\n",
    "        df_sentence['sentence_verylong']  = df_sentence['sentence_token_len'].apply(lambda x : sum(c>=9 for c in x))\n",
    "        df_sentence['sentence_long']      = df_sentence['sentence_token_len'].apply(lambda x : sum(c==7 or c==8 for c in x))\n",
    "        df_sentence['sentence_mid']       = df_sentence['sentence_token_len'].apply(lambda x : sum(c==5 or c==6 for c in x))\n",
    "        df_sentence['sentence_difficulty'] = df_sentence['sentence_verylong']*5 + df_sentence['sentence_long']*3 + df_sentence['sentence_mid']*1\n",
    "        df_sentence['sentence_long_words'] = df_sentence['sentence_verylong']+df_sentence['sentence_long']\n",
    "        a = df_sentence.groupby('id')[['sentence_verylong','sentence_long','sentence_mid','sentence_difficulty','sentence_long_words']].agg(['max','mean','sum'])\n",
    "        a.columns = ['_'.join(x) for x in a.columns]\n",
    "\n",
    "        return (df[['id','verylong','long','mid','difficulty','long_words']]).merge(a,on='id',how='left')\n",
    "    \n",
    "    def make_feats(self, df):\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "            \n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        print(\"Engineering cursor position data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "\n",
    "        print(\"Engineering word count data\")\n",
    "        for gap in self.gaps:\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)        \n",
    "        \n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        feats_stat = [\n",
    "            ('event_id', ['max']),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', ['nunique']),\n",
    "            ('up_event', ['nunique']),\n",
    "            ('text_change', ['nunique']),\n",
    "            ]\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['max', 'min', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'cursor_position_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'quantile', 'sem', 'sum', 'skew', pd.DataFrame.kurt]),\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Calculating pause features\")\n",
    "        tmp_df = self.calculate_pauses(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge brute force agg.>')\n",
    "        tmp_df = self.brute_force_agg(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Engineering ratios data\")\n",
    "        feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
    "        feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
    "        feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
    "        feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n",
    "\n",
    "        print('<merge duration_features.>')\n",
    "        tmp_df = self.duration_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            feats = feats.merge(self.train_scores, on='id', how='left')\n",
    "        \n",
    "        print('<merge countvectorizer_and_tfidf_features.>')\n",
    "        if self.method == 'train':\n",
    "            tmp_df,tokenizer,save_cols = self.essay_CountVectorizer_and_tfidf()\n",
    "        else:\n",
    "            tmp_df = self.essay_CountVectorizer_and_tfidf()\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge other features.>')\n",
    "        if self.method == 'train':\n",
    "            if os.path.exists('/kaggle/input/lgbm-and-nn-on-sentences'):  \n",
    "                tmp_df = pd.read_csv('/kaggle/input/lgbm-and-nn-on-sentences/train_agg_ratio.csv')\n",
    "            else:\n",
    "                tmp_df = self.other_features(df)\n",
    "        else:\n",
    "            tmp_df = self.other_features(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('<merge errors features.>')\n",
    "        tmp_df = self.language_error(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        tmp_df = self.sentence_error()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        if self.method == 'train':\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        else:\n",
    "            essays_upper = getEssays_with_upper(df)\n",
    "            tmp_df =  self.language_error_letter()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        print('merge sentence and paragraph agg features')\n",
    "        sent_df = self.split_essays_into_sentences()\n",
    "        tmp_df = self.compute_sentence_aggregations(sent_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        paragraph_df = self.split_essays_into_paragraphs()\n",
    "        tmp_df = self.compute_paragraph_aggregations(paragraph_df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge R burst features')\n",
    "        tmp_df = self.R_burst(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "\n",
    "        print('merge difficulty agg features')\n",
    "        tmp_df = self.difficulty()\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        if self.method == 'train':\n",
    "            return feats,tokenizer,save_cols\n",
    "        else:\n",
    "            return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f48802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:16:04.429800Z",
     "iopub.status.busy": "2023-12-31T03:16:04.429017Z",
     "iopub.status.idle": "2023-12-31T03:24:54.216977Z",
     "shell.execute_reply": "2023-12-31T03:24:54.215470Z"
    },
    "papermill": {
     "duration": 529.803203,
     "end_time": "2023-12-31T03:24:54.220123",
     "exception": false,
     "start_time": "2023-12-31T03:16:04.416920",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [03:55<00:00,  8.11s/it, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 4901.70it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 4460.77it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 4421.53it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 4510.83it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 4389.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n",
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/146573151.py:620: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_19/146573151.py:621: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_19/146573151.py:622: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_19/146573151.py:623: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:275: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:277: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n",
      "<merge errors features.>\n",
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1168.87it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 425.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering time data\n",
      "Engineering cursor position data\n",
      "Engineering word count data\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:02<00:00, 14.38it/s, column=word_count_change100, method=kurt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17476.27it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7273.36it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "100%|██████████| 3/3 [00:00<00:00, 20327.81it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 15968.16it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 15572.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Calculating pause features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/146573151.py:620: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_time_ratio'] = feats['tmp_word_count_max'] / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_19/146573151.py:621: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['word_event_ratio'] = feats['tmp_word_count_max'] / feats['event_id_max']\n",
      "/tmp/ipykernel_19/146573151.py:622: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['event_time_ratio'] = feats['event_id_max']  / feats['tmp_down_time_max']\n",
      "/tmp/ipykernel_19/146573151.py:623: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feats['idle_time_ratio'] = feats['action_time_gap1_sum'] / feats['tmp_down_time_max']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge brute force agg.>\n",
      "Engineering ratios data\n",
      "<merge duration_features.>\n",
      "<merge countvectorizer_and_tfidf_features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_19/146573151.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  count_vector.loc[:, 'id'] = df_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge errors features.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 501.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge sentence and paragraph agg features\n",
      "merge R burst features\n",
      "merge difficulty agg features\n"
     ]
    }
   ],
   "source": [
    "#LGBM V1 train_feats and test_feats\n",
    "preprocessor = Preprocessor_v1(42,train_essays,train_essays_with_upper,train_scores = train_scores)\n",
    "train_feats,tokenizer,save_cols = preprocessor.make_feats(train_logs)\n",
    "\n",
    "test_essays = EssayConstructor().getEssays(test_logs)\n",
    "test_essays.set_index('id',inplace=True)\n",
    "test_essays.index.name = None\n",
    "test_essays_with_upper = getEssays_with_upper(test_logs)\n",
    "preprocessor = Preprocessor_v1(42,test_essays,test_essays_with_upper,tokenizer=tokenizer,method='test',save_cols=save_cols)\n",
    "test_feats = preprocessor.make_feats(test_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f94df",
   "metadata": {
    "papermill": {
     "duration": 0.103895,
     "end_time": "2023-12-31T03:24:54.430510",
     "exception": false,
     "start_time": "2023-12-31T03:24:54.326615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## select features for lgbm v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13eebf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:24:54.645052Z",
     "iopub.status.busy": "2023-12-31T03:24:54.644620Z",
     "iopub.status.idle": "2023-12-31T03:24:54.672415Z",
     "shell.execute_reply": "2023-12-31T03:24:54.671159Z"
    },
    "papermill": {
     "duration": 0.139748,
     "end_time": "2023-12-31T03:24:54.675573",
     "exception": false,
     "start_time": "2023-12-31T03:24:54.535825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "f_read = open('/kaggle/input/select-features/feats_dict.pkl', 'rb')\n",
    "lgb_cols_v1 = pickle.load(f_read)\n",
    "f_read.close()\n",
    "\n",
    "f_read = open('/kaggle/input/features-select/feats_dict.pkl', 'rb')\n",
    "lgb_cols_v2 = pickle.load(f_read)\n",
    "f_read.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e3864",
   "metadata": {
    "papermill": {
     "duration": 0.104069,
     "end_time": "2023-12-31T03:24:54.885429",
     "exception": false,
     "start_time": "2023-12-31T03:24:54.781360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LGBM  V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c240da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:24:55.098176Z",
     "iopub.status.busy": "2023-12-31T03:24:55.097776Z",
     "iopub.status.idle": "2023-12-31T03:26:03.537289Z",
     "shell.execute_reply": "2023-12-31T03:26:03.536324Z"
    },
    "papermill": {
     "duration": 68.551269,
     "end_time": "2023-12-31T03:26:03.540043",
     "exception": false,
     "start_time": "2023-12-31T03:24:54.988774",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.560866\n",
      "[200]\tvalid_0's rmse: 0.558959\n",
      "[300]\tvalid_0's rmse: 0.559198\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.644847\n",
      "[200]\tvalid_0's rmse: 0.614789\n",
      "[300]\tvalid_0's rmse: 0.610724\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.596754\n",
      "[200]\tvalid_0's rmse: 0.582304\n",
      "[300]\tvalid_0's rmse: 0.585804\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.593375\n",
      "[200]\tvalid_0's rmse: 0.581715\n",
      "[300]\tvalid_0's rmse: 0.57927\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.615054\n",
      "[200]\tvalid_0's rmse: 0.603683\n",
      "[300]\tvalid_0's rmse: 0.597717\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.631984\n",
      "[200]\tvalid_0's rmse: 0.623964\n",
      "[300]\tvalid_0's rmse: 0.623121\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.593616\n",
      "[200]\tvalid_0's rmse: 0.580042\n",
      "[300]\tvalid_0's rmse: 0.580825\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.634697\n",
      "[200]\tvalid_0's rmse: 0.614856\n",
      "[300]\tvalid_0's rmse: 0.611223\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.62652\n",
      "[200]\tvalid_0's rmse: 0.605585\n",
      "[300]\tvalid_0's rmse: 0.599917\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.699914\n",
      "[200]\tvalid_0's rmse: 0.687379\n",
      "[300]\tvalid_0's rmse: 0.680328\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.669763\n",
      "[200]\tvalid_0's rmse: 0.665024\n",
      "[300]\tvalid_0's rmse: 0.670422\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.635449\n",
      "[200]\tvalid_0's rmse: 0.614573\n",
      "[300]\tvalid_0's rmse: 0.617301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.603289\n",
      "[200]\tvalid_0's rmse: 0.58905\n",
      "[300]\tvalid_0's rmse: 0.584578\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.611655\n",
      "[200]\tvalid_0's rmse: 0.59869\n",
      "[300]\tvalid_0's rmse: 0.596508\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.662507\n",
      "[200]\tvalid_0's rmse: 0.63202\n",
      "[300]\tvalid_0's rmse: 0.62816\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.601038\n",
      "[200]\tvalid_0's rmse: 0.596974\n",
      "[300]\tvalid_0's rmse: 0.596496\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.640939\n",
      "[200]\tvalid_0's rmse: 0.619372\n",
      "[300]\tvalid_0's rmse: 0.621448\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.565518\n",
      "[200]\tvalid_0's rmse: 0.553495\n",
      "[300]\tvalid_0's rmse: 0.559743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.622855\n",
      "[200]\tvalid_0's rmse: 0.586092\n",
      "[300]\tvalid_0's rmse: 0.588642\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.609955\n",
      "[200]\tvalid_0's rmse: 0.592111\n",
      "[300]\tvalid_0's rmse: 0.588881\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.630563\n",
      "[200]\tvalid_0's rmse: 0.625593\n",
      "[300]\tvalid_0's rmse: 0.627845\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.597625\n",
      "[200]\tvalid_0's rmse: 0.587406\n",
      "[300]\tvalid_0's rmse: 0.58128\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.622648\n",
      "[200]\tvalid_0's rmse: 0.604641\n",
      "[300]\tvalid_0's rmse: 0.608754\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.627927\n",
      "[200]\tvalid_0's rmse: 0.614386\n",
      "[300]\tvalid_0's rmse: 0.608574\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.626973\n",
      "[200]\tvalid_0's rmse: 0.60527\n",
      "[300]\tvalid_0's rmse: 0.598402\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.610027\n",
      "[200]\tvalid_0's rmse: 0.589721\n",
      "[300]\tvalid_0's rmse: 0.585106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.636277\n",
      "[200]\tvalid_0's rmse: 0.632165\n",
      "[300]\tvalid_0's rmse: 0.63065\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.611415\n",
      "[200]\tvalid_0's rmse: 0.601693\n",
      "[300]\tvalid_0's rmse: 0.594805\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.660306\n",
      "[200]\tvalid_0's rmse: 0.637464\n",
      "[300]\tvalid_0's rmse: 0.634752\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.56982\n",
      "[200]\tvalid_0's rmse: 0.559314\n",
      "[300]\tvalid_0's rmse: 0.554191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.622017\n",
      "[200]\tvalid_0's rmse: 0.606456\n",
      "[300]\tvalid_0's rmse: 0.606168\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.508867\n",
      "[200]\tvalid_0's rmse: 0.502795\n",
      "[300]\tvalid_0's rmse: 0.496861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.561007\n",
      "[200]\tvalid_0's rmse: 0.54856\n",
      "[300]\tvalid_0's rmse: 0.555478\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.627974\n",
      "[200]\tvalid_0's rmse: 0.608806\n",
      "[300]\tvalid_0's rmse: 0.600871\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.64088\n",
      "[200]\tvalid_0's rmse: 0.618286\n",
      "[300]\tvalid_0's rmse: 0.620899\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.649308\n",
      "[200]\tvalid_0's rmse: 0.646627\n",
      "[300]\tvalid_0's rmse: 0.646779\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.661846\n",
      "[200]\tvalid_0's rmse: 0.651705\n",
      "[300]\tvalid_0's rmse: 0.649696\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.646435\n",
      "[200]\tvalid_0's rmse: 0.637415\n",
      "[300]\tvalid_0's rmse: 0.644037\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.614203\n",
      "[200]\tvalid_0's rmse: 0.595453\n",
      "[300]\tvalid_0's rmse: 0.593115\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.642159\n",
      "[200]\tvalid_0's rmse: 0.609233\n",
      "[300]\tvalid_0's rmse: 0.60414\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.607889\n",
      "[200]\tvalid_0's rmse: 0.59094\n",
      "[300]\tvalid_0's rmse: 0.587405\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.60135\n",
      "[200]\tvalid_0's rmse: 0.595571\n",
      "[300]\tvalid_0's rmse: 0.594917\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.578259\n",
      "[200]\tvalid_0's rmse: 0.554866\n",
      "[300]\tvalid_0's rmse: 0.559832\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.597399\n",
      "[200]\tvalid_0's rmse: 0.58794\n",
      "[300]\tvalid_0's rmse: 0.586221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.607129\n",
      "[200]\tvalid_0's rmse: 0.601802\n",
      "[300]\tvalid_0's rmse: 0.605899\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.646435\n",
      "[200]\tvalid_0's rmse: 0.641451\n",
      "[300]\tvalid_0's rmse: 0.648814\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.650437\n",
      "[200]\tvalid_0's rmse: 0.622982\n",
      "[300]\tvalid_0's rmse: 0.617639\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.606398\n",
      "[200]\tvalid_0's rmse: 0.589998\n",
      "[300]\tvalid_0's rmse: 0.58211\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.664626\n",
      "[200]\tvalid_0's rmse: 0.653758\n",
      "[300]\tvalid_0's rmse: 0.64939\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20627717879176471, colsample_bytree=0.404204345392843 will be ignored. Current value: feature_fraction=0.20627717879176471\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1934830872644402, subsample=0.8488883899692615 will be ignored. Current value: bagging_fraction=0.1934830872644402\n",
      "[100]\tvalid_0's rmse: 0.640465\n",
      "[200]\tvalid_0's rmse: 0.622818\n",
      "[300]\tvalid_0's rmse: 0.618842\n"
     ]
    }
   ],
   "source": [
    "def LGBM_train_and_test_v1(features,params):\n",
    "    OOF_PREDS = np.zeros(len(train_feats))\n",
    "    TEST_PREDS = np.zeros(len(test_feats))\n",
    "    best_iters_dict = defaultdict(list)\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = params\n",
    "    best_iterations = [340, 318, 325, 301, 361]\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                'random_state': 42,\n",
    "                \"n_estimators\" : best_iterations[i],\n",
    "                \"verbosity\": -1,\n",
    "                **best_params\n",
    "            }\n",
    "            \n",
    "#             X_train_pre, y_train_pre = train_feats.iloc[train_idx][features], train_feats.iloc[train_idx][target_col]\n",
    "#             X_valid_pre, y_valid_pre = train_feats.iloc[valid_idx][features], train_feats.iloc[valid_idx][target_col]\n",
    "#             pre_model = lgb.LGBMRegressor(**params)\n",
    "#             pre_model.fit(X_train_pre, y_train_pre, eval_set=[(X_valid_pre, y_valid_pre)],verbose=100)\n",
    "#             imp_df = pd.DataFrame()\n",
    "#             imp_df[\"feature\"] = features       \n",
    "#             imp_df[\"importance\"] = pre_model.feature_importances_\n",
    "#             imp_df = imp_df.sort_values(by='importance',ascending=False)\n",
    "#             features_select = list(imp_df[imp_df['importance']!=0]['feature'].values)\n",
    "#             print('-'*50)\n",
    "            \n",
    "            X_train, y_train = train_feats.iloc[train_idx][features], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][features], train_feats.iloc[valid_idx][target_col]\n",
    "            model = lgb.LGBMRegressor(**params)    \n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=100)\n",
    "            best_iters_dict[str(seed)].append(model.best_iteration_)\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[features])\n",
    "            TEST_PREDS += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "        scores.append(oof_score)\n",
    "    return OOF_PREDS,TEST_PREDS\n",
    "\n",
    "params1 =   {'reg_alpha': 0.885805244981124,\n",
    "             'reg_lambda': 0.14935461337828307,\n",
    "             'bagging_freq': 2,\n",
    "             'bagging_fraction': 0.1934830872644402,\n",
    "             'colsample_bytree': 0.404204345392843,\n",
    "             'subsample': 0.8488883899692615,\n",
    "             'feature_fraction': 0.20627717879176471,\n",
    "             'learning_rate': 0.030830005210447875,\n",
    "             'num_leaves': 7,\n",
    "             'min_child_samples': 2}\n",
    "OOF_PREDS_v1,TEST_PREDS_v1 = LGBM_train_and_test_v1(lgb_cols_v1,params1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f10d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:26:03.787889Z",
     "iopub.status.busy": "2023-12-31T03:26:03.786889Z",
     "iopub.status.idle": "2023-12-31T03:26:03.795866Z",
     "shell.execute_reply": "2023-12-31T03:26:03.794709Z"
    },
    "papermill": {
     "duration": 0.136006,
     "end_time": "2023-12-31T03:26:03.797967",
     "exception": false,
     "start_time": "2023-12-31T03:26:03.661961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric LGBM v1 = 0.59698\n"
     ]
    }
   ],
   "source": [
    "print('OOF metric LGBM v1 = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS_v1, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016e836a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:26:04.043398Z",
     "iopub.status.busy": "2023-12-31T03:26:04.042680Z",
     "iopub.status.idle": "2023-12-31T03:26:04.059529Z",
     "shell.execute_reply": "2023-12-31T03:26:04.058489Z"
    },
    "papermill": {
     "duration": 0.142477,
     "end_time": "2023-12-31T03:26:04.062008",
     "exception": false,
     "start_time": "2023-12-31T03:26:03.919531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LGBM v1 参数调优\n",
    "def objective(trial):\n",
    "    OOF_PREDS = np.zeros((len(train_feats), 2))\n",
    "    TEST_PREDS = np.zeros((len(test_feats), 2))\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = {\n",
    "        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5,1),\n",
    "        'bagging_freq': trial.suggest_categorical('bagging_freq',[1,2,3,4,5,6,7,8,9,10]),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 1e-1, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 1e-1, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 1e-1,1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction',1e-1,1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-2,1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1,50),\n",
    "    }\n",
    "    \n",
    "    best_iterations = [340, 318, 325, 301, 361]\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                'random_state': 42,\n",
    "                \"n_estimators\" : best_iterations[i],\n",
    "                \"verbosity\": -1,\n",
    "                **best_params\n",
    "            }\n",
    "            X_train, y_train = train_feats.iloc[train_idx][lgb_cols_v1], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][lgb_cols_v1], train_feats.iloc[valid_idx][target_col]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=100)\n",
    "\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx, 0] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[lgb_cols_v1])\n",
    "            TEST_PREDS[:, 0] += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        lgbm_rmse = metrics.mean_squared_error(train_feats[target_col], OOF_PREDS[:, 0], squared=False)\n",
    "    return lgbm_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e219fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:26:04.306897Z",
     "iopub.status.busy": "2023-12-31T03:26:04.306161Z",
     "iopub.status.idle": "2023-12-31T03:26:04.310501Z",
     "shell.execute_reply": "2023-12-31T03:26:04.309553Z"
    },
    "papermill": {
     "duration": 0.131047,
     "end_time": "2023-12-31T03:26:04.313313",
     "exception": false,
     "start_time": "2023-12-31T03:26:04.182266",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)\n",
    "# best_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16a68d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:26:04.556599Z",
     "iopub.status.busy": "2023-12-31T03:26:04.556135Z",
     "iopub.status.idle": "2023-12-31T03:27:34.650498Z",
     "shell.execute_reply": "2023-12-31T03:27:34.649138Z"
    },
    "papermill": {
     "duration": 90.219292,
     "end_time": "2023-12-31T03:27:34.653349",
     "exception": false,
     "start_time": "2023-12-31T03:26:04.434057",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.551417\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.621878\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.590484\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.575461\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.607355\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.630288\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.571412\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.61194\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.600662\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.676888\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.666301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.621273\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.589007\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.58038\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.637311\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.593366\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.616149\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.547937\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.591897\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.582068\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.620324\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.582117\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.590483\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.602344\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.603007\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.598234\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.627539\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.610072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.641622\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.557112\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.607915\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.496278\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.549013\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.619449\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.617148\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.635852\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.651549\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.634769\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.589874\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.614044\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.589838\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.57987\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.565453\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.5766\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.593479\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.642414\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.629597\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.579538\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.648579\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4129923391793148, colsample_bytree=0.3602912391579135 will be ignored. Current value: feature_fraction=0.4129923391793148\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6318927381128606, subsample=0.6072322983043791 will be ignored. Current value: bagging_fraction=0.6318927381128606\n",
      "[200]\tvalid_0's rmse: 0.629544\n"
     ]
    }
   ],
   "source": [
    "def LGBM_train_and_test_v2(features,params):\n",
    "    OOF_PREDS = np.zeros(len(train_feats))\n",
    "    TEST_PREDS = np.zeros(len(test_feats))\n",
    "    best_iters_dict = defaultdict(list)\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = params\n",
    "    best_iterations = [342, 340, 359, 339, 330]\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                'random_state': 42,\n",
    "                \"n_estimators\" : best_iterations[i],\n",
    "                \"verbosity\": -1,\n",
    "                **best_params\n",
    "            }\n",
    "            \n",
    "            X_train, y_train = train_feats.iloc[train_idx][features], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][features], train_feats.iloc[valid_idx][target_col]\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=200)\n",
    "            best_iters_dict[str(seed)].append(model.best_iteration_)\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[features])\n",
    "            TEST_PREDS += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        oof_score = metrics.mean_squared_error(train_feats[target_col], oof_valid_preds, squared=False)\n",
    "        scores.append(oof_score)\n",
    "    return OOF_PREDS,TEST_PREDS,best_iters_dict\n",
    "\n",
    "params2 =   {'reg_alpha': 0.03780986639910007,\n",
    "             'reg_lambda': 0.5842850261135902,\n",
    "             'bagging_freq': 1,\n",
    "             'bagging_fraction': 0.6318927381128606,\n",
    "             'colsample_bytree': 0.3602912391579135,\n",
    "             'subsample': 0.6072322983043791,\n",
    "             'feature_fraction': 0.4129923391793148,\n",
    "             'learning_rate': 0.02972037888075976,\n",
    "             'num_leaves': 7,\n",
    "             'min_child_samples': 14}\n",
    "OOF_PREDS_v2,TEST_PREDS_v2,iterations = LGBM_train_and_test_v2(lgb_cols_v2,params2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274f9626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:34.924188Z",
     "iopub.status.busy": "2023-12-31T03:27:34.923770Z",
     "iopub.status.idle": "2023-12-31T03:27:34.936779Z",
     "shell.execute_reply": "2023-12-31T03:27:34.935560Z"
    },
    "papermill": {
     "duration": 0.148367,
     "end_time": "2023-12-31T03:27:34.939204",
     "exception": false,
     "start_time": "2023-12-31T03:27:34.790837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metric LGBM v1 = 0.59854\n"
     ]
    }
   ],
   "source": [
    "print('OOF metric LGBM v1 = {:.5f}'.format(metrics.mean_squared_error(train_feats[target_col], OOF_PREDS_v2, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9161e80f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:35.206400Z",
     "iopub.status.busy": "2023-12-31T03:27:35.205938Z",
     "iopub.status.idle": "2023-12-31T03:27:35.231050Z",
     "shell.execute_reply": "2023-12-31T03:27:35.229730Z"
    },
    "papermill": {
     "duration": 0.162453,
     "end_time": "2023-12-31T03:27:35.233849",
     "exception": false,
     "start_time": "2023-12-31T03:27:35.071396",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    OOF_PREDS = np.zeros((len(train_feats), 2))\n",
    "    TEST_PREDS = np.zeros((len(test_feats), 2))\n",
    "    models_dict = {}\n",
    "    scores = []\n",
    "    test_predict_list = []\n",
    "    best_params = {\n",
    "        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5,1),\n",
    "        'bagging_freq': trial.suggest_categorical('bagging_freq',[1,2,3,4,5,6,7,8,9,10]),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 1e-1, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 1e-1, 1),\n",
    "        'subsample': trial.suggest_float('subsample', 1e-1,1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction',1e-1,1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-2,1e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1,50),\n",
    "    }\n",
    "    \n",
    "    best_iterations = [342, 340, 359, 339, 330]\n",
    "    for i in range(5): \n",
    "        seeds = [3,6,38,39,43]\n",
    "        seed = seeds[i]\n",
    "        kf = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    "        oof_valid_preds = np.zeros(train_feats.shape[0])\n",
    "        X_test = test_feats\n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_feats)):\n",
    "\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                'random_state': 42,\n",
    "                \"n_estimators\" : best_iterations[i],\n",
    "                \"verbosity\": -1,\n",
    "                **best_params\n",
    "            }\n",
    "            \n",
    "            X_train, y_train = train_feats.iloc[train_idx][lgb_cols_v2], train_feats.iloc[train_idx][target_col]\n",
    "            X_valid, y_valid = train_feats.iloc[valid_idx][lgb_cols_v2], train_feats.iloc[valid_idx][target_col]\n",
    "\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose=100)\n",
    "\n",
    "            valid_predict = model.predict(X_valid)\n",
    "            oof_valid_preds[valid_idx] = valid_predict\n",
    "            OOF_PREDS[valid_idx, 0] += valid_predict / len(seeds)\n",
    "            test_predict = model.predict(X_test[lgb_cols_v2])\n",
    "            TEST_PREDS[:, 0] += test_predict / len(seeds) / 10\n",
    "            test_predict_list.append(test_predict)\n",
    "            score = metrics.mean_squared_error(y_valid, valid_predict, squared=False)\n",
    "            models_dict[f'{fold}_{i}'] = model\n",
    "        lgbm_rmse = metrics.mean_squared_error(train_feats[target_col], OOF_PREDS[:, 0], squared=False)\n",
    "    return lgbm_rmse\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=70)\n",
    "# print('Number of finished trials:', len(study.trials))\n",
    "# print('Best trial:', study.best_trial.params)\n",
    "# best_params = study.best_trial.params\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b816d89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:35.499047Z",
     "iopub.status.busy": "2023-12-31T03:27:35.498609Z",
     "iopub.status.idle": "2023-12-31T03:27:37.775532Z",
     "shell.execute_reply": "2023-12-31T03:27:37.773945Z"
    },
    "papermill": {
     "duration": 2.412866,
     "end_time": "2023-12-31T03:27:37.778399",
     "exception": false,
     "start_time": "2023-12-31T03:27:35.365533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition OOF score = 0.59641\n",
      "Composition best W = 0.660\n"
     ]
    }
   ],
   "source": [
    "best_sc = 1\n",
    "for w in np.arange(0, 1.01, 0.001):\n",
    "    sc = metrics.mean_squared_error(train_feats[target_col], \n",
    "                                    w * OOF_PREDS_v1 + (1-w) * OOF_PREDS_v2, \n",
    "                                    squared=False)\n",
    "    if sc < best_sc:\n",
    "        best_sc = sc\n",
    "        best_w = w\n",
    "print('Composition OOF score = {:.5f}'.format(best_sc))\n",
    "print('Composition best W = {:.3f}'.format(best_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34e6e27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:38.050556Z",
     "iopub.status.busy": "2023-12-31T03:27:38.050098Z",
     "iopub.status.idle": "2023-12-31T03:27:38.060246Z",
     "shell.execute_reply": "2023-12-31T03:27:38.059001Z"
    },
    "papermill": {
     "duration": 0.147587,
     "end_time": "2023-12-31T03:27:38.062768",
     "exception": false,
     "start_time": "2023-12-31T03:27:37.915181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12342756, 1.06594115, 1.07543807])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = [best_w,1 - best_w]\n",
    "TEST_PREDS = TEST_PREDS_v1 * W[0] + TEST_PREDS_v2 * W[1]\n",
    "TEST_PREDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a92fe975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:38.334171Z",
     "iopub.status.busy": "2023-12-31T03:27:38.333703Z",
     "iopub.status.idle": "2023-12-31T03:27:38.345213Z",
     "shell.execute_reply": "2023-12-31T03:27:38.344284Z"
    },
    "papermill": {
     "duration": 0.149088,
     "end_time": "2023-12-31T03:27:38.347420",
     "exception": false,
     "start_time": "2023-12-31T03:27:38.198332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/1114240514.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_feats['score'] = TEST_PREDS\n"
     ]
    }
   ],
   "source": [
    "test_feats['score'] = TEST_PREDS\n",
    "test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14aa0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T03:27:38.612797Z",
     "iopub.status.busy": "2023-12-31T03:27:38.612313Z",
     "iopub.status.idle": "2023-12-31T03:27:38.629484Z",
     "shell.execute_reply": "2023-12-31T03:27:38.628191Z"
    },
    "papermill": {
     "duration": 0.153003,
     "end_time": "2023-12-31T03:27:38.632289",
     "exception": false,
     "start_time": "2023-12-31T03:27:38.479286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.123428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1.065941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1.075438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.123428\n",
       "1  2222bbbb  1.065941\n",
       "2  4444cccc  1.075438"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats[['id', 'score']]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    },
    {
     "datasetId": 3992884,
     "sourceId": 6971449,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949123,
     "sourceId": 6973319,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4169445,
     "sourceId": 7206892,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 150384981,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154091050,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154250178,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 156990507,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 724.296571,
   "end_time": "2023-12-31T03:27:39.997849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-31T03:15:35.701278",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
