{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8206d4",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-01-05T03:49:39.723056Z",
     "iopub.status.busy": "2024-01-05T03:49:39.722771Z",
     "iopub.status.idle": "2024-01-05T03:50:27.119175Z",
     "shell.execute_reply": "2024-01-05T03:50:27.118159Z"
    },
    "papermill": {
     "duration": 47.409599,
     "end_time": "2024-01-05T03:50:27.121467",
     "exception": false,
     "start_time": "2024-01-05T03:49:39.711868",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lightautoml-038-dependecies\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/lightautoml-0.3.8-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/AutoWoE-1.3.2-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: catboost>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/cmaes-0.10.0-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: holidays in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.24)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1.2)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/joblib-1.2.0-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/json2html-1.3.0.tar.gz (from lightautoml==0.3.8)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hProcessing /kaggle/input/lightautoml-038-dependecies/lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.1)\r\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.24.3)\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (3.5.0)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/poetry_core-1.8.1-py3-none-any.whl (from lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (6.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (1.2.2)\r\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.12.2)\r\n",
      "Requirement already satisfied: statsmodels<=0.14.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (0.14.0)\r\n",
      "Requirement already satisfied: torch<=2.0.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (2.0.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lightautoml==0.3.8) (4.66.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/StrEnum-0.4.15-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (3.7.4)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (7.4.3)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (2023.3)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (1.11.4)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinx-7.2.6-py3-none-any.whl (from autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from autowoe>=1.2->lightautoml==0.3.8) (0.2.4)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (0.20.1)\r\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (5.16.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost>=0.26.1->lightautoml==0.3.8) (1.16.0)\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm<=3.2.1,>=2.3->lightautoml==0.3.8) (0.41.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0->lightautoml==0.3.8) (2.8.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->lightautoml==0.3.8) (3.2.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (0.5.3)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels<=0.14.0->lightautoml==0.3.8) (21.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.12)\r\n",
      "Requirement already satisfied: hijri-converter in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (2.3.1)\r\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/conda/lib/python3.10/site-packages (from holidays->lightautoml==0.3.8) (0.3.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->lightautoml==0.3.8) (2.1.3)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (1.13.0)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (6.8.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna->lightautoml==0.3.8) (2.0.20)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna->lightautoml==0.3.8) (1.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (4.42.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (10.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autowoe>=1.2->lightautoml==0.3.8) (3.0.9)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna->lightautoml==0.3.8) (2.0.2)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost>=0.26.1->lightautoml==0.3.8) (8.2.3)\r\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.0)\r\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.2.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (1.1.3)\r\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest->autowoe>=1.2->lightautoml==0.3.8) (2.0.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_applehelp-1.0.7-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_devhelp-1.0.5-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/sphinxcontrib_qthelp-1.0.6-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.16.1)\r\n",
      "Requirement already satisfied: docutils<0.21,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (0.20.1)\r\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.2.0)\r\n",
      "Requirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.12.1)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/alabaster-0.7.13-py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/imagesize-1.4.1-py2.py3-none-any.whl (from sphinx->autowoe>=1.2->lightautoml==0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx->autowoe>=1.2->lightautoml==0.3.8) (2.31.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<=2.0.0,>=1.9.0->lightautoml==0.3.8) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx->autowoe>=1.2->lightautoml==0.3.8) (2023.11.17)\r\n",
      "Building wheels for collected packages: json2html\r\n",
      "  Building wheel for json2html (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for json2html: filename=json2html-1.3.0-py3-none-any.whl size=7591 sha256=002ffbddb09e5f55c8a612d03614fdafe01e3635034d67316fb8de26d7301ec9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/04/0d/34912ecabd9128a537a032c0fc15c6c46e734fb5fe3a14536c\r\n",
      "Successfully built json2html\r\n",
      "Installing collected packages: StrEnum, json2html, sphinxcontrib-jsmath, poetry-core, joblib, imagesize, cmaes, alabaster, pandas, lightgbm, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sphinx, autowoe, lightautoml\r\n",
      "  Attempting uninstall: joblib\r\n",
      "    Found existing installation: joblib 1.3.2\r\n",
      "    Uninstalling joblib-1.3.2:\r\n",
      "      Successfully uninstalled joblib-1.3.2\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.0.3\r\n",
      "    Uninstalling pandas-2.0.3:\r\n",
      "      Successfully uninstalled pandas-2.0.3\r\n",
      "  Attempting uninstall: lightgbm\r\n",
      "    Found existing installation: lightgbm 3.3.2\r\n",
      "    Uninstalling lightgbm-3.3.2:\r\n",
      "      Successfully uninstalled lightgbm-3.3.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "fitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\r\n",
      "fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed StrEnum-0.4.15 alabaster-0.7.13 autowoe-1.3.2 cmaes-0.10.0 imagesize-1.4.1 joblib-1.2.0 json2html-1.3.0 lightautoml-0.3.8 lightgbm-3.2.1 pandas-1.5.3 poetry-core-1.8.1 sphinx-7.2.6 sphinxcontrib-applehelp-1.0.7 sphinxcontrib-devhelp-1.0.5 sphinxcontrib-htmlhelp-2.0.4 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.6 sphinxcontrib-serializinghtml-1.1.9\r\n",
      "Looking in links: /kaggle/input/lightautoml-038-dependecies\r\n",
      "Processing /kaggle/input/lightautoml-038-dependecies/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\r\n",
      "Installing collected packages: pandas\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 1.5.3\r\n",
      "    Uninstalling pandas-1.5.3:\r\n",
      "      Successfully uninstalled pandas-1.5.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "fitter 1.6.0 requires joblib<2.0.0,>=1.3.1, but you have joblib 1.2.0 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "lightautoml 0.3.8 requires pandas<2.0.0, but you have pandas 2.0.3 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed pandas-2.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies lightautoml==0.3.8\n",
    "!pip install --no-index -U --find-links=/kaggle/input/lightautoml-038-dependecies pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e202ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:50:27.149077Z",
     "iopub.status.busy": "2024-01-05T03:50:27.148794Z",
     "iopub.status.idle": "2024-01-05T03:50:58.607443Z",
     "shell.execute_reply": "2024-01-05T03:50:58.606537Z"
    },
    "papermill": {
     "duration": 31.475075,
     "end_time": "2024-01-05T03:50:58.609873",
     "exception": false,
     "start_time": "2024-01-05T03:50:27.134798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from random import choice, choices\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from itertools import cycle\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn import metrics, model_selection, preprocessing, linear_model, ensemble, decomposition, tree\n",
    "import lightgbm as lgb\n",
    "import copy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import random\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from itertools import combinations  \n",
    "from scipy.stats import skew\n",
    "import copy\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings  \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5fb5225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:50:58.638763Z",
     "iopub.status.busy": "2024-01-05T03:50:58.637976Z",
     "iopub.status.idle": "2024-01-05T03:51:13.249517Z",
     "shell.execute_reply": "2024-01-05T03:51:13.248546Z"
    },
    "papermill": {
     "duration": 14.628406,
     "end_time": "2024-01-05T03:51:13.252077",
     "exception": false,
     "start_time": "2024-01-05T03:50:58.623671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/linking-writing-processes-to-writing-quality'\n",
    "train_logs = pd.read_csv(f'{INPUT_DIR}/train_logs.csv')\n",
    "train_scores = pd.read_csv(f'{INPUT_DIR}/train_scores.csv')\n",
    "test_logs = pd.read_csv(f'{INPUT_DIR}/test_logs.csv')\n",
    "ss_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "train_essays = pd.read_csv('../input/writing-quality-challenge-constructed-essays/train_essays_02.csv')\n",
    "train_essays.index = train_essays[\"Unnamed: 0\"]\n",
    "train_essays.index.name = None\n",
    "train_essays.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "train_essays_with_upper = pd.read_csv('/kaggle/input/essays-generator-with-upper/essays_with_upper.csv') \n",
    "train_essays_with_upper.index = train_essays_with_upper[\"Unnamed: 0\"]\n",
    "train_essays_with_upper.index.name = None\n",
    "train_essays_with_upper.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d1c6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:13.280159Z",
     "iopub.status.busy": "2024-01-05T03:51:13.279833Z",
     "iopub.status.idle": "2024-01-05T03:51:13.310416Z",
     "shell.execute_reply": "2024-01-05T03:51:13.309560Z"
    },
    "papermill": {
     "duration": 0.046473,
     "end_time": "2024-01-05T03:51:13.312234",
     "exception": false,
     "start_time": "2024-01-05T03:51:13.265761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#essay生成：普通版本和带大写版本\n",
    "class EssayConstructor:\n",
    "    \n",
    "    def processingInputs(self,currTextInput):\n",
    "        # Where the essay content will be stored\n",
    "        essayText = \"\"\n",
    "        # Produces the essay\n",
    "        for Input in currTextInput.values:\n",
    "            # Input[0] = activity\n",
    "            # Input[1] = cursor_position\n",
    "            # Input[2] = text_change\n",
    "            # Input[3] = id\n",
    "            # If activity = Replace\n",
    "            if Input[0] == 'Replace':\n",
    "                # splits text_change at ' => '\n",
    "                replaceTxt = Input[2].split(' => ')\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Paste    \n",
    "            if Input[0] == 'Paste':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Remove/Cut\n",
    "            if Input[0] == 'Remove/Cut':\n",
    "                # DONT TOUCH\n",
    "                essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                continue\n",
    "\n",
    "            # If activity = Move...\n",
    "            if \"M\" in Input[0]:\n",
    "                # Gets rid of the \"Move from to\" text\n",
    "                croppedTxt = Input[0][10:]              \n",
    "                # Splits cropped text by ' To '\n",
    "                splitTxt = croppedTxt.split(' To ')              \n",
    "                # Splits split text again by ', ' for each item\n",
    "                valueArr = [item.split(', ') for item in splitTxt]              \n",
    "                # Move from [2, 4] To [5, 7] = (2, 4, 5, 7)\n",
    "                moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "                # Skip if someone manages to activiate this by moving to same place\n",
    "                if moveData[0] != moveData[2]:\n",
    "                    # Check if they move text forward in essay (they are different)\n",
    "                    if moveData[0] < moveData[2]:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                    else:\n",
    "                        # DONT TOUCH\n",
    "                        essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                continue                \n",
    "                \n",
    "            # If activity = input\n",
    "            # DONT TOUCH\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        return essayText\n",
    "            \n",
    "            \n",
    "    def getEssays(self,df):\n",
    "        # Copy required columns\n",
    "        textInputDf = copy.deepcopy(df[['id', 'activity', 'cursor_position', 'text_change']])\n",
    "        # Get rid of text inputs that make no change\n",
    "        textInputDf = textInputDf[textInputDf.activity != 'Nonproduction']     \n",
    "        # construct essay, fast \n",
    "        tqdm.pandas()\n",
    "        essay=textInputDf.groupby('id')[['activity','cursor_position', 'text_change']].progress_apply(lambda x: self.processingInputs(x))      \n",
    "        # to dataframe\n",
    "        essayFrame=essay.to_frame().reset_index()\n",
    "        essayFrame.columns=['id','essay']\n",
    "        # Returns the essay series\n",
    "        return essayFrame\n",
    "\n",
    "def getEssays_with_upper(df):\n",
    "    df['down_event_shift'] = df.groupby('id')['down_event'].shift(1)\n",
    "    textInputDf = df[['id', 'activity', 'cursor_position', 'text_change','down_event','down_event_shift']]\n",
    "    valCountsArr = textInputDf['id'].value_counts(sort=False).values\n",
    "    lastIndex = 0\n",
    "    essaySeries = pd.Series()\n",
    "    for index, valCount in enumerate(tqdm(valCountsArr)):\n",
    "        capital = False\n",
    "        currTextInput = textInputDf[['activity', 'cursor_position', 'text_change','down_event','down_event_shift']].iloc[lastIndex : lastIndex + valCount]\n",
    "        lastIndex += valCount\n",
    "        essayText = \"\"\n",
    "        for Input in currTextInput.values:\n",
    "            if Input[3] == 'CapsLock':\n",
    "                capital = not capital\n",
    "            if Input[0] == 'Nonproduction':\n",
    "                continue\n",
    "            if Input[0] != 'Nonproduction':\n",
    "                if (Input[0] == 'Replace')&(Input[4] == 'Shift'):\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + (replaceTxt[1]).upper() +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Replace':\n",
    "                    replaceTxt = Input[2].split(' => ')\n",
    "                    essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +\\\n",
    "                    essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "                    continue\n",
    "                    \n",
    "                if Input[0] == 'Paste':\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "                    continue\n",
    "                if Input[0] == 'Remove/Cut':\n",
    "                    essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "                    continue\n",
    "                if \"M\" in Input[0]:\n",
    "                    croppedTxt = Input[0][10:]\n",
    "                    splitTxt = croppedTxt.split(' To ')\n",
    "                    valueArr = [item.split(', ') for item in splitTxt]\n",
    "                    moveData = (int(valueArr[0][0][1:]), \n",
    "                                int(valueArr[0][1][:-1]), \n",
    "                                int(valueArr[1][0][1:]), \n",
    "                                int(valueArr[1][1][:-1]))\n",
    "                    if moveData[0] != moveData[2]:\n",
    "                        if moveData[0] < moveData[2]:\n",
    "                            essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                            essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                        else:\n",
    "                            essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                            essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "                    continue\n",
    "                if capital|((Input[4]=='Shift')&(Input[3]=='q')):\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2].upper() + essayText[Input[1] - len(Input[2]):]\n",
    "                else:\n",
    "                    essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "        essaySeries[index] = essayText\n",
    "    essaySeries.index =  textInputDf['id'].unique()\n",
    "    return pd.DataFrame(essaySeries, columns=['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1dda9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:13.339623Z",
     "iopub.status.busy": "2024-01-05T03:51:13.338957Z",
     "iopub.status.idle": "2024-01-05T03:51:13.343951Z",
     "shell.execute_reply": "2024-01-05T03:51:13.343161Z"
    },
    "papermill": {
     "duration": 0.020356,
     "end_time": "2024-01-05T03:51:13.345804",
     "exception": false,
     "start_time": "2024-01-05T03:51:13.325448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return x.quantile(n/100)\n",
    "    percentile_.__name__ = 'pct_{:02.0f}'.format(n)\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8ad211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:13.372630Z",
     "iopub.status.busy": "2024-01-05T03:51:13.372383Z",
     "iopub.status.idle": "2024-01-05T03:51:13.390363Z",
     "shell.execute_reply": "2024-01-05T03:51:13.389541Z"
    },
    "papermill": {
     "duration": 0.033654,
     "end_time": "2024-01-05T03:51:13.392186",
     "exception": false,
     "start_time": "2024-01-05T03:51:13.358532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_essay_aggregations(essay_df):\n",
    "    cols_to_drop = ['essay']\n",
    "    # Total essay length\n",
    "    essay_df['essay_len'] = essay_df['essay'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df.drop(columns=cols_to_drop)\n",
    "    return essay_df\n",
    "\n",
    "def split_essays_into_words(essay_df):\n",
    "    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('word')\n",
    "    # Word length (number of characters in word)\n",
    "    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "    return essay_df\n",
    "\n",
    "def compute_word_aggregations(word_df):\n",
    "    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(self.aggregations)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    # New features: computing the # of words whose length exceed word_l\n",
    "    for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "        word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "def split_essays_into_sentences(essay_df):\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df['sent_word_count_diff'] = essay_df.groupby(level=0)['sent_word_count'].transform(lambda x:np.abs(x.diff()))    \n",
    "\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_sentence_aggregations(sent_df):\n",
    "    sent_agg_df = sent_df[['id','sent_len','sent_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    # New features: computing the # of sentences whose (character) length exceed sent_l\n",
    "    for sent_l in [50, 60, 75, 100]:\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_df[sent_df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "        sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    return sent_agg_df\n",
    "\n",
    "def split_essays_into_paragraphs(essay_df):\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    # Number of sentences in paragraphs\n",
    "    essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(x.split('\\\\.|\\\\?|\\\\!')))\n",
    "    # Number of words in paragraphs\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def compute_paragraph_aggregations(paragraph_df):\n",
    "    paragraph_agg_df = paragraph_df[['id','paragraph_len', 'paragraph_sent_count', 'paragraph_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fce4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:13.419028Z",
     "iopub.status.busy": "2024-01-05T03:51:13.418780Z",
     "iopub.status.idle": "2024-01-05T03:51:13.495858Z",
     "shell.execute_reply": "2024-01-05T03:51:13.495040Z"
    },
    "papermill": {
     "duration": 0.092958,
     "end_time": "2024-01-05T03:51:13.497706",
     "exception": false,
     "start_time": "2024-01-05T03:51:13.404748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nth percentile function for agg\n",
    "class Preprocessor:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "        self.events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', \n",
    "              'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "        self.text_changes_dict = {\n",
    "            'q': 'q', \n",
    "            ' ': 'space', \n",
    "            'NoChange': 'NoChange', \n",
    "            '.': 'full_stop', \n",
    "            ',': 'comma', \n",
    "            '\\n': 'newline', \n",
    "            \"'\": 'single_quote', \n",
    "            '\"': 'double_quote', \n",
    "            '-': 'dash', \n",
    "            '?': 'question_mark', \n",
    "            ';': 'semicolon', \n",
    "            '=': 'equals', \n",
    "            '/': 'slash', \n",
    "            '\\\\': 'double_backslash', \n",
    "            ':': 'colon'\n",
    "        }\n",
    "        self.punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/',\n",
    "                        '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "        self.gaps = [1, 2, 3, 5, 10, 20, 50, 70, 100]\n",
    "        self.percentiles = [5, 10, 25, 50, 75, 90, 95]\n",
    "        self.percentiles_cols = [percentile(n) for n in self.percentiles]\n",
    "        self.aggregations = ['mean', 'std', 'min', 'max', 'first', 'last', 'sem', q1, 'median', q3, 'skew', pd.DataFrame.kurt, 'sum']\n",
    "        self.idf = defaultdict(float)\n",
    "        \n",
    "        self.essay_constructor = EssayConstructor()\n",
    "    \n",
    "    def get_essay_aggregations(self, essay_df):\n",
    "        cols_to_drop = ['essay']\n",
    "        # Total essay length\n",
    "        essay_df['essay_len'] = essay_df['essay'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df.drop(columns=cols_to_drop)\n",
    "        return essay_df\n",
    "    \n",
    "    def split_essays_into_words(self, essay_df):\n",
    "        essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('word')\n",
    "        # Word length (number of characters in word)\n",
    "        essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "        essay_df = essay_df[essay_df['word_len'] != 0]\n",
    "        return essay_df\n",
    "    \n",
    "    def compute_word_aggregations(self, word_df):\n",
    "        word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(self.aggregations)\n",
    "        word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "        word_agg_df['id'] = word_agg_df.index\n",
    "        # New features: computing the # of words whose length exceed word_l\n",
    "        for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "            word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "            word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "        word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "        return word_agg_df\n",
    "    \n",
    "    def split_essays_into_sentences(self, essay_df):\n",
    "        essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "        essay_df = essay_df.explode('sent')\n",
    "        essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "        # Number of characters in sentences\n",
    "        essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "        # Number of words in sentences\n",
    "        essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "        essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "\n",
    "    def compute_sentence_aggregations(self, sent_df):\n",
    "        sent_agg_df = sent_df[['id','sent_len','sent_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "        sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "        sent_agg_df['id'] = sent_agg_df.index\n",
    "        # New features: computing the # of sentences whose (character) length exceed sent_l\n",
    "        for sent_l in [50, 60, 75, 100]:\n",
    "            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_df[sent_df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "            sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "        sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "        return sent_agg_df\n",
    "\n",
    "    def split_essays_into_paragraphs(self, essay_df):\n",
    "        essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "        essay_df = essay_df.explode('paragraph')\n",
    "        # Number of characters in paragraphs\n",
    "        essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "        # Number of sentences in paragraphs\n",
    "        essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(x.split('\\\\.|\\\\?|\\\\!')))\n",
    "        # Number of words in paragraphs\n",
    "        essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "        \n",
    "        essay_df = essay_df[essay_df.paragraph_len!=0].reset_index(drop=True)\n",
    "        return essay_df\n",
    "\n",
    "    def compute_paragraph_aggregations(self, paragraph_df):\n",
    "        paragraph_agg_df = paragraph_df[['id','paragraph_len', 'paragraph_sent_count', 'paragraph_word_count']].groupby(['id']).agg(self.aggregations)\n",
    "        paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "        paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "        paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "        return paragraph_agg_df\n",
    "        \n",
    "    def activity_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'activity': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['activity'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.activities:\n",
    "                di[k] = 0\n",
    "            # make dictionary entry for \"move from X to Y\"\n",
    "            di[\"move_to\"] = 0\n",
    "            \n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "                else:\n",
    "                    # we can do this because there are no missing values\n",
    "                    di[\"move_to\"] += v\n",
    "            ret.append(di)\n",
    "        \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'activity_{act}_tfidf_count' for act in ret.columns]\n",
    "        normal_cols = [f'activity_{act}_normal_count' for act in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "\n",
    "    def event_counts(self, df, colname):\n",
    "        tmp_df = df.groupby('id').agg({colname: list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df[colname].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.events:\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "            \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'{colname}_{event}_tfidf_count' for event in ret.columns]\n",
    "        normal_cols = [f'{colname}_{event}_normal_count' for event in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "\n",
    "    def text_change_counts(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['text_change'].values):\n",
    "            items = list(Counter(li).items())\n",
    "            di = dict()\n",
    "            for k in self.text_changes_dict.keys():\n",
    "                di[k] = 0\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in di:\n",
    "                    di[k] = v\n",
    "            ret.append(di)\n",
    "            \n",
    "        ret = pd.DataFrame(ret)\n",
    "        # using tfidf\n",
    "        ret_tfidf = pd.DataFrame(ret)\n",
    "        # returning counts as is\n",
    "        ret_normal = pd.DataFrame(ret)\n",
    "        \n",
    "        tfidf_cols = [f'text_change_{self.text_changes_dict[txt_change]}_tfidf_count' for txt_change in ret.columns]\n",
    "        normal_cols = [f'text_change_{self.text_changes_dict[txt_change]}_normal_count' for txt_change in ret.columns]\n",
    "        \n",
    "        ret_tfidf.columns = tfidf_cols\n",
    "        ret_normal.columns = normal_cols\n",
    "        \n",
    "        '''\n",
    "        Credit: https://www.kaggle.com/code/olyatsimboy/towards-tf-idf-in-logs-features\n",
    "        '''\n",
    "        cnts = ret_tfidf.sum(1)\n",
    "\n",
    "        for col in tfidf_cols:\n",
    "            if col in self.idf.keys():\n",
    "                idf = self.idf[col]\n",
    "            else:\n",
    "                idf = df.shape[0] / (ret_tfidf[col].sum() + 1)\n",
    "                idf = np.log(idf)\n",
    "                self.idf[col] = idf\n",
    "\n",
    "            ret_tfidf[col] = 1 + np.log(ret_tfidf[col] / cnts)\n",
    "            ret_tfidf[col] *= idf\n",
    "        \n",
    "        ret_agg = pd.concat([ret_tfidf, ret_normal], axis=1)\n",
    "        return ret_agg\n",
    "    \n",
    "    def match_punctuations(self, df):\n",
    "        tmp_df = df.groupby('id').agg({'down_event': list}).reset_index()\n",
    "        ret = list()\n",
    "        for li in tqdm(tmp_df['down_event'].values):\n",
    "            cnt = 0\n",
    "            items = list(Counter(li).items())\n",
    "            for item in items:\n",
    "                k, v = item[0], item[1]\n",
    "                if k in self.punctuations:\n",
    "                    cnt += v\n",
    "            ret.append(cnt)\n",
    "        ret = pd.DataFrame({'punct_cnt': ret})\n",
    "        return ret\n",
    "\n",
    "    # Credit: https://www.kaggle.com/code/abdullahmeda/enter-ing-the-timeseries-space-sec-3-new-aggs/notebook\n",
    "    def make_space_features(self, df):\n",
    "        df['up_time_lagged'] = df.groupby('id')['up_time'].shift(1).fillna(df['down_time'])\n",
    "        df['time_diff'] = abs(df['down_time'] - df['up_time_lagged']) / 1000\n",
    "\n",
    "        group = df.groupby('id')['time_diff']\n",
    "        largest_lantency = group.max()\n",
    "        smallest_lantency = group.min()\n",
    "        median_lantency = group.median()\n",
    "        initial_pause = df.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        \n",
    "        result = pd.DataFrame({\n",
    "            'id': df['id'].unique(),\n",
    "            'largest_lantency': largest_lantency,\n",
    "            'smallest_lantency': smallest_lantency,\n",
    "            'median_lantency': median_lantency,\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True)\n",
    "        return result\n",
    "    \n",
    "    def get_input_words(self, df):\n",
    "        tmp_df = df[(~df['text_change'].str.contains('=>'))&(df['text_change'] != 'NoChange')].reset_index(drop=True)\n",
    "        tmp_df = tmp_df.groupby('id').agg({'text_change': list}).reset_index()\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: ''.join(x))\n",
    "        tmp_df['text_change'] = tmp_df['text_change'].apply(lambda x: re.findall(r'q+', x))\n",
    "        tmp_df['input_word_count'] = tmp_df['text_change'].apply(len)\n",
    "        tmp_df['input_word_length_mean'] = tmp_df['text_change'].apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        for percentile in self.percentiles:\n",
    "            tmp_df[f'input_word_length_pct_{percentile}'] = tmp_df['text_change'].apply(lambda x: np.percentile([len(i) for i in x] if len(x) > 0 else 0, \n",
    "                                                                                                               percentile))\n",
    "        tmp_df['input_word_length_max'] = tmp_df['text_change'].apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df['input_word_length_std'] = tmp_df['text_change'].apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0))\n",
    "        tmp_df.drop(['text_change'], axis=1, inplace=True)\n",
    "        return tmp_df\n",
    "    \n",
    "    def make_feats(self, df: pd.DataFrame, save_essays_path: str):\n",
    "        \n",
    "        print(\"Starting to engineer features\")\n",
    "        \n",
    "        # initialize features dataframe\n",
    "        feats = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "        \n",
    "        # get essay feats\n",
    "        print(\"Getting essays\")\n",
    "        essay_df = self.essay_constructor.getEssays(df)\n",
    "        essay_df.to_csv(save_essays_path, index=False)\n",
    "\n",
    "        print(\"Getting essay aggregations data\")\n",
    "        essay_agg_df = self.get_essay_aggregations(essay_df)\n",
    "        feats = feats.merge(essay_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay word aggregations data\")\n",
    "        word_df = self.split_essays_into_words(essay_df)\n",
    "        word_agg_df = self.compute_word_aggregations(word_df)\n",
    "        feats = feats.merge(word_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay sentence aggregations data\")\n",
    "        sent_df = self.split_essays_into_sentences(essay_df)\n",
    "        sent_agg_df = self.compute_sentence_aggregations(sent_df)\n",
    "        feats = feats.merge(sent_agg_df, on='id', how='left')\n",
    "\n",
    "        print(\"Getting essay paragraph aggregations data\")\n",
    "        paragraph_df = self.split_essays_into_paragraphs(essay_df)\n",
    "        paragraph_agg_df = self.compute_paragraph_aggregations(paragraph_df)\n",
    "        feats = feats.merge(paragraph_agg_df, on='id', how='left')\n",
    "        \n",
    "        # engineer counts data\n",
    "        print(\"Engineering activity counts data\")\n",
    "        tmp_df = self.activity_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering event counts data\")\n",
    "        tmp_df = self.event_counts(df, 'down_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        tmp_df = self.event_counts(df, 'up_event')\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering text change counts data\")\n",
    "        tmp_df = self.text_change_counts(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        print(\"Engineering punctuation counts data\")\n",
    "        tmp_df = self.match_punctuations(df)\n",
    "        feats = pd.concat([feats, tmp_df], axis=1)\n",
    "        \n",
    "        # space features\n",
    "        print(\"Engineering space-related data\")\n",
    "        tmp_df = self.make_space_features(df)\n",
    "        feats = feats.merge(tmp_df, on='id', how='left')\n",
    "        \n",
    "        # get shifted features\n",
    "        # time shift\n",
    "        print(\"Engineering time data\")\n",
    "        for gap in self.gaps:\n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'up_time_shift{gap}'] = df.groupby('id')['up_time'].shift(gap)\n",
    "            df[f'action_time_gap{gap}'] = df['down_time'] - df[f'up_time_shift{gap}']\n",
    "        df.drop(columns=[f'up_time_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # cursor position shift\n",
    "        print(\"Engineering cursor position data - gaps\")\n",
    "        for gap in self.gaps: \n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'cursor_position_shift{gap}'] = df.groupby('id')['cursor_position'].shift(gap)\n",
    "            df[f'cursor_position_change{gap}'] = df['cursor_position'] - df[f'cursor_position_shift{gap}']\n",
    "            df[f'cursor_position_abs_change{gap}'] = np.abs(df[f'cursor_position_change{gap}'])\n",
    "        df.drop(columns=[f'cursor_position_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # word count shift\n",
    "        print(\"Engineering word count data - gaps\")\n",
    "        for gap in self.gaps: \n",
    "            print(f\"> for gap {gap}\")\n",
    "            df[f'word_count_shift{gap}'] = df.groupby('id')['word_count'].shift(gap)\n",
    "            df[f'word_count_change{gap}'] = df['word_count'] - df[f'word_count_shift{gap}']\n",
    "            df[f'word_count_abs_change{gap}'] = np.abs(df[f'word_count_change{gap}'])\n",
    "        df.drop(columns=[f'word_count_shift{gap}' for gap in self.gaps], inplace=True)\n",
    "        \n",
    "        # get aggregate statistical features\n",
    "        print(\"Engineering statistical summaries for features\")\n",
    "        # [(feature name, [ stat summaries to add ])]\n",
    "        percentiles_cols = [percentile(n) for n in self.percentiles]\n",
    "        feats_stat = [\n",
    "            ('action_time',self.percentiles_cols),\n",
    "            ('activity', ['nunique']),\n",
    "            ('down_event', [ 'nunique']),\n",
    "            ('up_event', [ 'nunique']),\n",
    "            ('text_change', [ 'nunique']),\n",
    "            ] \n",
    "\n",
    "        for gap in self.gaps:\n",
    "            feats_stat.extend([\n",
    "                (f'action_time_gap{gap}', ['first','last', 'max', 'min', 'mean', 'std', 'sem', 'skew', pd.DataFrame.kurt]+ percentiles_cols),\n",
    "                (f'cursor_position_change{gap}', ['first','last','max', 'mean', 'std','sem', 'skew', pd.DataFrame.kurt]),\n",
    "                (f'word_count_change{gap}', ['max', 'mean', 'std', 'sum', 'sem', 'skew', pd.DataFrame.kurt] + percentiles_cols),\n",
    "            ])\n",
    "        \n",
    "        pbar = tqdm(feats_stat)\n",
    "        for item in pbar:\n",
    "            colname, methods = item[0], item[1]\n",
    "            for method in methods:\n",
    "                pbar.set_postfix()\n",
    "                if isinstance(method, str):\n",
    "                    method_name = method\n",
    "                else:\n",
    "                    method_name = method.__name__\n",
    "                    \n",
    "                pbar.set_postfix(column=colname, method=method_name)\n",
    "                tmp_df = df.groupby(['id']).agg({colname: method}).reset_index().rename(columns={colname: f'{colname}_{method_name}'})\n",
    "                feats = feats.merge(tmp_df, on='id', how='left') \n",
    "\n",
    "        print(\"Engineering input words data\")\n",
    "        tmp_df = self.get_input_words(df)\n",
    "        feats = pd.merge(feats, tmp_df, on='id', how='left')\n",
    "        \n",
    "        print(\"Done!\")\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e64e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:13.524133Z",
     "iopub.status.busy": "2024-01-05T03:51:13.523845Z",
     "iopub.status.idle": "2024-01-05T03:51:18.296028Z",
     "shell.execute_reply": "2024-01-05T03:51:18.295310Z"
    },
    "papermill": {
     "duration": 4.788093,
     "end_time": "2024-01-05T03:51:18.298459",
     "exception": false,
     "start_time": "2024-01-05T03:51:13.510366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#bruteforce agg\n",
    "train_agg_fe_df = train_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "train_agg_fe_df.columns = ['_'.join(x) for x in train_agg_fe_df.columns]\n",
    "train_agg_fe_df = train_agg_fe_df.add_prefix(\"tmp_\")\n",
    "train_agg_fe_df.reset_index(inplace=True)\n",
    "\n",
    "test_agg_fe_df = test_logs.groupby(\"id\")[['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'last', 'first', 'sem', 'median', 'sum'])\n",
    "test_agg_fe_df.columns = ['_'.join(x) for x in test_agg_fe_df.columns]\n",
    "test_agg_fe_df = test_agg_fe_df.add_prefix(\"tmp_\")\n",
    "test_agg_fe_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae51a597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:18.326438Z",
     "iopub.status.busy": "2024-01-05T03:51:18.326119Z",
     "iopub.status.idle": "2024-01-05T03:51:18.335871Z",
     "shell.execute_reply": "2024-01-05T03:51:18.334981Z"
    },
    "papermill": {
     "duration": 0.025693,
     "end_time": "2024-01-05T03:51:18.337877",
     "exception": false,
     "start_time": "2024-01-05T03:51:18.312184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duration_features(train_logs,test_logs):\n",
    "    tr_logs,te_logs = copy.deepcopy(train_logs),copy.deepcopy(test_logs)\n",
    "    data = []\n",
    "    for logs in [tr_logs, te_logs]:\n",
    "        #logs['up_time_lagged'] = logs.groupby('id')['up_time'].shift(1).fillna(logs['down_time'])\n",
    "        #logs['time_diff'] = abs(logs['down_time'] - logs['up_time_lagged']) / 1000\n",
    "        logs['time_diff'] = logs.groupby('id')['down_time'].diff()\n",
    "        group = logs.groupby('id')['time_diff']\n",
    "        largest_lantency = group.max()\n",
    "        smallest_lantency = group.min()\n",
    "        median_lantency = group.median()\n",
    "        initial_pause = logs.groupby('id')['down_time'].first() / 1000\n",
    "        pauses_half_sec = group.apply(lambda x: ((x > 0.5) & (x < 1)).sum())\n",
    "        pauses_1_sec = group.apply(lambda x: ((x > 1) & (x < 1.5)).sum())\n",
    "        pauses_1_half_sec = group.apply(lambda x: ((x > 1.5) & (x < 2)).sum())\n",
    "        pauses_2_sec = group.apply(lambda x: ((x > 2) & (x < 3)).sum())\n",
    "        pauses_3_sec = group.apply(lambda x: (x > 3).sum())\n",
    "        data.append(pd.DataFrame({\n",
    "            'id': logs['id'].unique(),\n",
    "            'largest_lantency': largest_lantency,\n",
    "            'smallest_lantency': smallest_lantency,\n",
    "            'median_lantency': median_lantency,\n",
    "            'initial_pause': initial_pause,\n",
    "            'pauses_half_sec': pauses_half_sec,\n",
    "            'pauses_1_sec': pauses_1_sec,\n",
    "            'pauses_1_half_sec': pauses_1_half_sec,\n",
    "            'pauses_2_sec': pauses_2_sec,\n",
    "            'pauses_3_sec': pauses_3_sec,\n",
    "        }).reset_index(drop=True))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d35d496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:18.366235Z",
     "iopub.status.busy": "2024-01-05T03:51:18.365912Z",
     "iopub.status.idle": "2024-01-05T03:51:18.376684Z",
     "shell.execute_reply": "2024-01-05T03:51:18.375833Z"
    },
    "papermill": {
     "duration": 0.026808,
     "end_time": "2024-01-05T03:51:18.378670",
     "exception": false,
     "start_time": "2024-01-05T03:51:18.351862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def essay_CountVectorizer(train_essays,test_essays):\n",
    "    train_essaysdf = copy.deepcopy(train_essays['essay'])\n",
    "    test_essaysdf = copy.deepcopy(test_essays['essay'])\n",
    "    train_essaysdf = pd.DataFrame({'id': train_essaysdf.index, 'essay': train_essaysdf.values})\n",
    "    test_essaysdf = pd.DataFrame({'id': test_essaysdf.index, 'essay': test_essaysdf.values})\n",
    "    merged_data = train_essaysdf.merge(train_scores, on='id')\n",
    "\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    X_tokenizer_train = count_vectorizer.fit_transform(merged_data['essay'])\n",
    "    X_tokenizer_test = count_vectorizer.transform(test_essaysdf['essay'])\n",
    "    y = merged_data['score']\n",
    "\n",
    "    X_tokenizer_train = X_tokenizer_train.todense()\n",
    "    X_tokenizer_test = X_tokenizer_test.todense()\n",
    "\n",
    "    train_count_vector = pd.DataFrame()\n",
    "    test_count_vector = pd.DataFrame()\n",
    "    for i in range(X_tokenizer_train.shape[1]) : \n",
    "        L = list(X_tokenizer_train[:,i])\n",
    "        li = [int(x) for x in L ]\n",
    "        train_count_vector[f'feature {i}'] = li\n",
    "    for i in range(X_tokenizer_test.shape[1]): \n",
    "        L = list(X_tokenizer_test[:,i])\n",
    "        li = [int(x) for x in L ]\n",
    "        test_count_vector[f'feature {i}'] = li\n",
    "\n",
    "    df_train_index = train_essaysdf['id']\n",
    "    df_test_index = test_essaysdf['id']\n",
    "    train_count_vector.loc[:, 'id'] = df_train_index\n",
    "    test_count_vector.loc[:, 'id'] = df_test_index\n",
    "\n",
    "    #保留部分特征，保留90%以上的人出现过的特征\n",
    "    save_cols = []\n",
    "    for i in train_count_vector.columns:\n",
    "        if sum(train_count_vector[i]==0)/len(train_count_vector)<0.1:\n",
    "            save_cols.append(i)\n",
    "    return train_count_vector,test_count_vector,save_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7db6025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:18.405987Z",
     "iopub.status.busy": "2024-01-05T03:51:18.405338Z",
     "iopub.status.idle": "2024-01-05T03:51:18.414812Z",
     "shell.execute_reply": "2024-01-05T03:51:18.413957Z"
    },
    "papermill": {
     "duration": 0.025388,
     "end_time": "2024-01-05T03:51:18.416959",
     "exception": false,
     "start_time": "2024-01-05T03:51:18.391571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def other_features(df,method='train'):\n",
    "    a = pd.DataFrame()\n",
    "    a['Input_all_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['activity']!='Input'))/df.groupby(['id']).apply(lambda x:sum(x['activity']=='Input'))\n",
    "    a['all_q_ratio'] = df.groupby(['id']).apply(lambda x:sum(x['down_event']!='q'))/df.groupby(['id']).apply(lambda x:sum(x['down_event']=='q'))\n",
    "    activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "    events_dict = {\n",
    "                    'q':'q', \n",
    "                    'Space':'Space', \n",
    "                    'Backspace':'Backspace', \n",
    "                    'Shift':'Shift', \n",
    "                    'ArrowRight':'ArrowRight', \n",
    "                    'Leftclick':'Leftclick', \n",
    "                    'ArrowLeft':'ArrowLeft', \n",
    "                    '.':'fullstop', \n",
    "                    ',':'comma', \n",
    "                    'ArrowDown':'ArrowDown', \n",
    "                    'ArrowUp':'ArrowUp', \n",
    "                    'Enter':'Enter', \n",
    "                    'CapsLock':'CapsLock', \n",
    "                    \"'\":'single_quote', \n",
    "                    'Delete':'Delete', \n",
    "                    'Unidentified':'Unidentified',\n",
    "                  }\n",
    "    \n",
    "    for i in tqdm(activities):\n",
    "        for j in events_dict:\n",
    "            a[f'{i}_{events_dict[j]}_count'] = df.groupby('id').apply(lambda x:len(x[(x['activity']==i)&(x['down_event']==j)]))\n",
    "    return a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "883fce0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:18.444021Z",
     "iopub.status.busy": "2024-01-05T03:51:18.443728Z",
     "iopub.status.idle": "2024-01-05T03:51:18.478822Z",
     "shell.execute_reply": "2024-01-05T03:51:18.477979Z"
    },
    "papermill": {
     "duration": 0.051126,
     "end_time": "2024-01-05T03:51:18.480834",
     "exception": false,
     "start_time": "2024-01-05T03:51:18.429708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def language_error(df,df_essays):\n",
    "    a = pd.DataFrame()\n",
    "    df['down_event_shift'] = df.groupby('id')['down_event'].shift(-1)\n",
    "    #letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')|((x['down_event']=='Shift')&(x['down_event_shift']=='q'))]))\n",
    "    letter_upper = df.groupby('id').apply(lambda x:len(x[(x['down_event']=='CapsLock')])//2+len(x[(x['down_event']=='Shift')&(x['down_event_shift']=='q')]))\n",
    "    a['letter_big_count'] = letter_upper.values\n",
    "    a['id'] = df['id'].unique()\n",
    "    \n",
    "    essay_df = copy.deepcopy(df_essays)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    \n",
    "    #避免将qqq.).切分成多个句子\n",
    "    #essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'\\.\\]|\\.\\)|\\.\\}|\\?\\]|\\?\\)|\\?\\}|\\!\\]|\\!\\)|\\!\\}','qq',x))\n",
    "    \n",
    "    essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())    \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df['sent_len']!=0]\n",
    "    errors_num = (essay_df.groupby('id').apply(len)-letter_upper).values\n",
    "    a['error_num'] = errors_num                          #如果句子个数大于大写字母按键次数，那么文章会有语法错误\n",
    "    return a\n",
    "def sentence_error(df):\n",
    "    essay_df = copy.deepcopy(df)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    essay_df = essay_df[essay_df['paragraph_len']!=0]\n",
    "    essay_df['only_space'] = essay_df['paragraph'].apply(lambda x:'q' not in x)\n",
    "    essay_df = essay_df[essay_df['only_space']==False]\n",
    "    a = pd.DataFrame()\n",
    "    a['para_error'] = essay_df.groupby('id').apply(lambda x:len(x[x['paragraph_len']<25]))   #一个段落字符过少可能不是完整的一句话，可能存在语法错误\n",
    "\n",
    "    return a.reset_index()\n",
    "\n",
    "def language_error_letter(df):\n",
    "    essay_df = copy.deepcopy(df)\n",
    "    essay_df['id'] = essay_df.index\n",
    "    essay_df['essay'] = essay_df['essay'].apply(lambda x:re.sub(r'q\\.q\\.','qqq',x))\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    essay_df = essay_df.explode('sent')   #explode将列表里的元素展开\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip()) \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    essay_df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    essay_df['language_error_letter'] = essay_df['sent'].apply(lambda x:x[0])\n",
    "    essay_df['if_q'] = essay_df['language_error_letter'].apply(lambda x:x.lower()=='q')\n",
    "    essay_df = essay_df[essay_df['if_q']==True]\n",
    "    a = pd.DataFrame()\n",
    "    a['language_error_letter'] = essay_df.groupby('id').apply(lambda x:len(x[x['language_error_letter']=='q']))\n",
    "    return a.reset_index()\n",
    "\n",
    "\n",
    "punctuations = ['\"', '.', ',', \"'\", '-', ';', ':', '?', '!', '<', '>', '/','@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+']\n",
    "def calculate_pauses(df, pause_threshold=2000):\n",
    "    # Compute IKI within each 'id' group\n",
    "    df['IKI'] = df.groupby('id')['down_time'].diff()\n",
    "\n",
    "    # Define pauses\n",
    "    df['is_pause'] = (df['IKI'] > pause_threshold)\n",
    "\n",
    "    # Compute statistics for IKI\n",
    "    iki_stats = df.groupby('id')['IKI'].agg(['mean', 'median', 'std', 'max']).reset_index().rename(columns={\n",
    "        'mean': 'iki_mean',\n",
    "        'median': 'iki_median',\n",
    "        'std': 'iki_std',\n",
    "        'max': 'iki_max'\n",
    "    })\n",
    "\n",
    "    # Compute pause counts\n",
    "    pause_counts = df.groupby('id')['is_pause'].sum().reset_index(name='pause_count')\n",
    "\n",
    "    # Compute average pause time excluding NaNs\n",
    "    pause_times = df[df['is_pause']].groupby('id')['IKI'].mean().reset_index(name='average_pause_time')\n",
    "\n",
    "    # Compute total pause time for paragraph\n",
    "    para_pause_duration = df.groupby('id').apply(lambda group: group['IKI'].where(group['text_change'] == '\\n').sum()).reset_index(name='para_pause_duration')\n",
    "\n",
    "    # Merge pause features\n",
    "    pause_features = pause_counts.merge(pause_times, on='id', how='left')\n",
    "    pause_features = pause_features.merge(para_pause_duration, on='id', how='left')\n",
    "    pause_features = pause_features.merge(iki_stats, on='id', how='left')\n",
    "\n",
    "    # Compute total IKI time and exclude NaNs\n",
    "    total_time = df.groupby('id')['IKI'].sum().reset_index(name='total_time')\n",
    "\n",
    "    # Merge the total time into pause_features\n",
    "    pause_features = pause_features.merge(total_time, on='id', how='left')\n",
    "\n",
    "    # Calculate pause time ratio\n",
    "    pause_features['pause_time_ratio'] = pause_features['pause_count'] * pause_features['average_pause_time']\n",
    "    pause_features['pause_time_ratio'] = pause_features['pause_time_ratio'] / pause_features['total_time'].replace(0, np.nan)\n",
    "\n",
    "    # Calculate times between sentences within each 'id' group\n",
    "    df['sentence_end_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].isin(['.', '?', '!']))).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Calculate statistics for times between sentences\n",
    "    between_sentences_stats = df.groupby('id')['sentence_end_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_sentences_IKI', 'std': 'std_between_sentences_IKI'})\n",
    "\n",
    "    # Calculate within-word IKI for 'q' characters within each 'id'\n",
    "    df['within_word_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'] == 'q')).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Calculate statistics for within-word IKI\n",
    "    within_word_stats = df.groupby('id')['within_word_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_within_word_IKI', 'std': 'std_within_word_IKI'})\n",
    "\n",
    "    # Calculate between-words IKI for spaces or punctuation followed by 'q'\n",
    "    df['between_words_IKI'] = df.groupby('id').apply(lambda group: group['down_time'].diff().where(group['text_change'].shift().isin([' '] + punctuations) & (group['text_change'] == 'q'))).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Calculate statistics for between-words IKI\n",
    "    between_words_stats = df.groupby('id')['between_words_IKI'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'mean_between_words_IKI', 'std': 'std_between_words_IKI'})\n",
    "\n",
    "    # Combine all the IKI related features into one DataFrame\n",
    "    pause_features = pause_features.merge(between_sentences_stats, on='id', how='left')\n",
    "    pause_features = pause_features.merge(within_word_stats, on='id', how='left')\n",
    "    pause_features = pause_features.merge(between_words_stats, on='id', how='left')\n",
    "\n",
    "    return pause_features\n",
    "\n",
    "\n",
    "def R_burst(df):\n",
    "    a = pd.DataFrame()\n",
    "    df = df[(df['activity']=='Input')|(df['activity']=='Remove/Cut')].reset_index(drop=True)\n",
    "    df['activity_shift'] = df.groupby('id')['activity'].shift().fillna(method='bfill')\n",
    "    df['is_R_burst'] = df['activity'] != df['activity_shift']\n",
    "    a['revision_count'] = df.groupby('id').apply(lambda x:x['is_R_burst'].sum())\n",
    "    df['keystroke_duration'] = df.groupby('id')['down_time'].diff()\n",
    "    df = df[df['keystroke_duration'].notnull()]\n",
    "\n",
    "    a['revision_count_above2s'] = df.groupby('id').apply(lambda x:x[(x['is_R_burst']==True)&(x['keystroke_duration']>2)]['is_R_burst'].sum()).values\n",
    "    Rburst =  df[(df['is_R_burst']==True)&(df['keystroke_duration']>2)]   #&(df['keystroke_duration']>2)\n",
    "    Rburst_statistic = Rburst.groupby('id').agg({'keystroke_duration':['mean','max','sum','median']})\n",
    "    Rburst_statistic.columns = ['_'.join(x) for x in Rburst_statistic.columns]\n",
    "\n",
    "    return a.merge(Rburst_statistic.reset_index(),on='id',how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d48416af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T03:51:18.507848Z",
     "iopub.status.busy": "2024-01-05T03:51:18.507524Z",
     "iopub.status.idle": "2024-01-05T04:04:42.808559Z",
     "shell.execute_reply": "2024-01-05T04:04:42.807628Z"
    },
    "papermill": {
     "duration": 804.317082,
     "end_time": "2024-01-05T04:04:42.810787",
     "exception": false,
     "start_time": "2024-01-05T03:51:18.493705",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to engineer features\n",
      "Getting essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:16<00:00, 151.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting essay aggregations data\n",
      "Getting essay word aggregations data\n",
      "Getting essay sentence aggregations data\n",
      "Getting essay paragraph aggregations data\n",
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 6033.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5765.77it/s]\n",
      "100%|██████████| 2471/2471 [00:00<00:00, 5755.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5905.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2471/2471 [00:00<00:00, 5713.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering space-related data\n",
      "Engineering time data\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering cursor position data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering word count data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [09:27<00:00, 17.73s/it, column=word_count_change100, method=pct_95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Done!\n",
      "Starting to engineer features\n",
      "Getting essays\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1696.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting essay aggregations data\n",
      "Getting essay word aggregations data\n",
      "Getting essay sentence aggregations data\n",
      "Getting essay paragraph aggregations data\n",
      "Engineering activity counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 20393.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering event counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 25115.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 21472.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering text change counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17189.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering punctuation counts data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 24059.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering space-related data\n",
      "Engineering time data\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering cursor position data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering word count data - gaps\n",
      "> for gap 1\n",
      "> for gap 2\n",
      "> for gap 3\n",
      "> for gap 5\n",
      "> for gap 10\n",
      "> for gap 20\n",
      "> for gap 50\n",
      "> for gap 70\n",
      "> for gap 100\n",
      "Engineering statistical summaries for features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00,  9.89it/s, column=word_count_change100, method=pct_95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering input words data\n",
      "Done!\n",
      "<train_feats and test_feats done.>\n",
      "<merge train and test agg done.>\n",
      "<merge duration features|train label done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1306.50it/s]\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector[f'feature {i}'] = li\n",
      "/tmp/ipykernel_26/857373842.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_count_vector.loc[:, 'id'] = df_train_index\n",
      "/tmp/ipykernel_26/857373842.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_count_vector.loc[:, 'id'] = df_test_index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge train and test essay CountVectorizer done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge other featuers done.>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 675.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<merge language errors done.>\n",
      "<merge IKI features done.>\n",
      "<merge R_burst features done.>\n"
     ]
    }
   ],
   "source": [
    "use_pre_fe_data = False\n",
    "if use_pre_fe_data:\n",
    "    pass\n",
    "else:\n",
    "    preprocessor_train = Preprocessor(seed = 42)\n",
    "    train_feats = preprocessor_train.make_feats(train_logs,save_essays_path = 'train_essays.csv')\n",
    "    del preprocessor_train\n",
    "    gc.collect()\n",
    "\n",
    "preprocessor_test = Preprocessor(seed = 42)\n",
    "test_feats = preprocessor_test.make_feats(test_logs,save_essays_path = 'test_essays.csv')\n",
    "print('<train_feats and test_feats done.>')\n",
    "\n",
    "train_feats = train_feats.merge(train_agg_fe_df, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_fe_df, on='id', how='left')\n",
    "print('<merge train and test agg done.>')\n",
    "\n",
    "train_eD592674, test_eD592674 = duration_features(train_logs,test_logs)\n",
    "train_feats = train_feats.merge(train_eD592674, on='id', how='left')\n",
    "test_feats = test_feats.merge(test_eD592674, on='id', how='left')\n",
    "train_feats = train_feats.merge(train_scores, on='id', how='left')\n",
    "print('<merge duration features|train label done.>')\n",
    "\n",
    "test_essays = (EssayConstructor().getEssays(test_logs))\n",
    "test_essays.index = test_essays['id'].values\n",
    "test_essays.drop(columns='id',inplace=True)\n",
    "train_count_vector,test_count_vector,save_cols = essay_CountVectorizer(train_essays,test_essays)\n",
    "train_feats = train_feats.merge(train_count_vector[save_cols], on='id', how='left')\n",
    "test_feats = test_feats.merge(test_count_vector[save_cols], on='id', how='left')\n",
    "print('<merge train and test essay CountVectorizer done.>')\n",
    "\n",
    "if os.path.exists('/kaggle/input/lgbm-and-nn-on-sentences'):  \n",
    "    train_agg_ratio = pd.read_csv('/kaggle/input/lgbm-and-nn-on-sentences/train_agg_ratio.csv')\n",
    "else:\n",
    "    train_agg_ratio = other_features(train_logs)\n",
    "#删除掉全部为0的列\n",
    "save_cols = []\n",
    "for i in train_agg_ratio.columns:\n",
    "    if sum(train_agg_ratio[i]==0)<len(train_agg_ratio):\n",
    "        save_cols.append(i)\n",
    "test_agg_ratio = other_features(test_logs)                          \n",
    "train_feats = train_feats.merge(train_agg_ratio[save_cols], on='id', how='left')\n",
    "test_feats = test_feats.merge(test_agg_ratio[save_cols], on='id', how='left')\n",
    "train_agg_ratio.to_csv('/kaggle/working/train_agg_ratio.csv',index=0)\n",
    "print('<merge other featuers done.>')\n",
    "\n",
    "tr_language_error_agg = language_error(train_logs,train_essays)\n",
    "te_language_error_agg = language_error(test_logs,test_essays)\n",
    "train_feats = train_feats.merge(tr_language_error_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_language_error_agg, on='id', how='left')\n",
    "\n",
    "tr_sentence_error_agg = sentence_error(train_essays)\n",
    "te_sentence_error_agg = sentence_error(test_essays)\n",
    "train_feats = train_feats.merge(tr_sentence_error_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_sentence_error_agg, on='id', how='left')\n",
    "\n",
    "tr_language_error_letter_agg =  language_error_letter(train_essays_with_upper)\n",
    "test_essays_with_upper = getEssays_with_upper(test_logs)\n",
    "te_language_error_letter_agg =  language_error_letter(test_essays_with_upper)\n",
    "train_feats = train_feats.merge(tr_language_error_letter_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_language_error_letter_agg, on='id', how='left')\n",
    "print('<merge language errors done.>')\n",
    "\n",
    "tr_IKI_agg = calculate_pauses(train_logs)\n",
    "te_IKI_agg = calculate_pauses(test_logs)\n",
    "train_feats = train_feats.merge(tr_IKI_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_IKI_agg, on='id', how='left')\n",
    "print('<merge IKI features done.>')\n",
    "\n",
    "tr_rburst_agg = R_burst(train_logs)\n",
    "te_rburst_agg = R_burst(test_logs)\n",
    "train_feats = train_feats.merge(tr_rburst_agg, on='id', how='left')\n",
    "test_feats = test_feats.merge(te_rburst_agg, on='id', how='left')\n",
    "print('<merge R_burst features done.>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece27066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:04:43.112894Z",
     "iopub.status.busy": "2024-01-05T04:04:43.112530Z",
     "iopub.status.idle": "2024-01-05T04:04:43.125149Z",
     "shell.execute_reply": "2024-01-05T04:04:43.124259Z"
    },
    "papermill": {
     "duration": 0.165392,
     "end_time": "2024-01-05T04:04:43.127007",
     "exception": false,
     "start_time": "2024-01-05T04:04:42.961615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['score']\n",
    "drop_cols = ['id']\n",
    "has_inf_cols = ['activity_Remove/Cut_tfidf_count',\n",
    "                 'activity_Replace_tfidf_count',\n",
    "                 'activity_Paste_tfidf_count',\n",
    "                 'activity_move_to_tfidf_count',\n",
    "                 'down_event_Backspace_tfidf_count',\n",
    "                 'down_event_Shift_tfidf_count',\n",
    "                 'down_event_ArrowRight_tfidf_count',\n",
    "                 'down_event_Leftclick_tfidf_count',\n",
    "                 'down_event_ArrowLeft_tfidf_count',\n",
    "                 'down_event_._tfidf_count',\n",
    "                 'down_event_,_tfidf_count',\n",
    "                 'down_event_ArrowDown_tfidf_count',\n",
    "                 'down_event_ArrowUp_tfidf_count',\n",
    "                 'down_event_Enter_tfidf_count',\n",
    "                 'down_event_CapsLock_tfidf_count',\n",
    "                 \"down_event_'_tfidf_count\",\n",
    "                 'down_event_Delete_tfidf_count',\n",
    "                 'down_event_Unidentified_tfidf_count',\n",
    "                 'up_event_Backspace_tfidf_count',\n",
    "                 'up_event_Shift_tfidf_count',\n",
    "                 'up_event_ArrowRight_tfidf_count',\n",
    "                 'up_event_Leftclick_tfidf_count',\n",
    "                 'up_event_ArrowLeft_tfidf_count',\n",
    "                 'up_event_._tfidf_count',\n",
    "                 'up_event_,_tfidf_count',\n",
    "                 'up_event_ArrowDown_tfidf_count',\n",
    "                 'up_event_ArrowUp_tfidf_count',\n",
    "                 'up_event_Enter_tfidf_count',\n",
    "                 'up_event_CapsLock_tfidf_count',\n",
    "                 \"up_event_'_tfidf_count\",\n",
    "                 'up_event_Delete_tfidf_count',\n",
    "                 'up_event_Unidentified_tfidf_count',\n",
    "                 'text_change_comma_tfidf_count',\n",
    "                 'text_change_newline_tfidf_count',\n",
    "                 'text_change_single_quote_tfidf_count',\n",
    "                 'text_change_double_quote_tfidf_count',\n",
    "                 'text_change_dash_tfidf_count',\n",
    "                 'text_change_question_mark_tfidf_count',\n",
    "                 'text_change_semicolon_tfidf_count',\n",
    "                 'text_change_equals_tfidf_count',\n",
    "                 'text_change_slash_tfidf_count',\n",
    "                 'text_change_double_backslash_tfidf_count',\n",
    "                 'text_change_colon_tfidf_count']\n",
    "\n",
    "drop_cols_NN = []\n",
    "for i in train_feats.columns:\n",
    "    if 'action_time' in i:\n",
    "        drop_cols_NN.append(i)\n",
    "    if 'up_time' in i:\n",
    "        drop_cols_NN.append(i)\n",
    "cols = [col for col in train_feats.columns if col not in drop_cols+drop_cols_NN+has_inf_cols]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ff389e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:04:43.426258Z",
     "iopub.status.busy": "2024-01-05T04:04:43.425881Z",
     "iopub.status.idle": "2024-01-05T04:04:43.454112Z",
     "shell.execute_reply": "2024-01-05T04:04:43.453376Z"
    },
    "papermill": {
     "duration": 0.180252,
     "end_time": "2024-01-05T04:04:43.456163",
     "exception": false,
     "start_time": "2024-01-05T04:04:43.275911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK = 'reg'\n",
    "task = Task(TASK)\n",
    "TIMEOUT = 10000\n",
    "N_THREADS = 2\n",
    "TRAIN_BS = 128\n",
    "USE_PLR = True\n",
    "USE_QNT = True\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "ADVANCED_ROLES = False\n",
    "TARGET_NAME = 'score'\n",
    "roles = {'target':TARGET_NAME}\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ba9657",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:04:43.755576Z",
     "iopub.status.busy": "2024-01-05T04:04:43.755161Z",
     "iopub.status.idle": "2024-01-05T04:04:43.762489Z",
     "shell.execute_reply": "2024-01-05T04:04:43.761611Z"
    },
    "papermill": {
     "duration": 0.160175,
     "end_time": "2024-01-05T04:04:43.764343",
     "exception": false,
     "start_time": "2024-01-05T04:04:43.604168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(task, y_true, y_pred):\n",
    "    if task.name == 'binary':\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    elif task.name == 'multiclass':\n",
    "        return log_loss(y_true, y_pred)\n",
    "    elif task.name == 'reg' or task.name == 'multi:reg':\n",
    "        return mean_squared_error(y_true,y_pred,squared=False)\n",
    "    else:\n",
    "        raise 'Task is not correct.'\n",
    "def use_plr(USE_PLR):\n",
    "    if USE_PLR:\n",
    "        return \"plr\"\n",
    "    else:\n",
    "        return \"cont\"\n",
    "def take_pred_from_task(pred, task):\n",
    "    if task.name == 'binary' or task.name == 'reg':\n",
    "        return pred[:, 0]\n",
    "    elif task.name == 'multiclass' or task.name == 'multi:reg':\n",
    "        return pred\n",
    "    else:\n",
    "        raise 'Task is not correct.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ace9745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:04:44.074197Z",
     "iopub.status.busy": "2024-01-05T04:04:44.073339Z",
     "iopub.status.idle": "2024-01-05T04:08:04.202297Z",
     "shell.execute_reply": "2024-01-05T04:08:04.201312Z"
    },
    "papermill": {
     "duration": 200.279333,
     "end_time": "2024-01-05T04:08:04.204628",
     "exception": false,
     "start_time": "2024-01-05T04:04:43.925295",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:04:44] Stdout logging level is INFO3.\n",
      "[04:04:44] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[04:04:44] Task: reg\n",
      "\n",
      "[04:04:44] Start automl preset with listed constraints:\n",
      "[04:04:44] - time: 10000.00 seconds\n",
      "[04:04:44] - CPU: 2 cores\n",
      "[04:04:44] - memory: 16 GB\n",
      "\n",
      "[04:04:44] \u001b[1mTrain data shape: (2471, 519)\u001b[0m\n",
      "\n",
      "[04:04:44] Layer \u001b[1m1\u001b[0m train process start. Time left 9999.31 secs\n",
      "[04:04:46] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[04:04:46] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[04:04:51] Epoch: 0, train loss: 0.9906322956085205, val loss: 0.7656087875366211, val metric: -0.7684003710746765\n",
      "[04:04:56] Epoch: 1, train loss: 0.6874363422393799, val loss: 0.42412498593330383, val metric: -0.42373931407928467\n",
      "[04:05:01] Epoch: 2, train loss: 0.44750791788101196, val loss: 0.31821590662002563, val metric: -0.3170606791973114\n",
      "[04:05:06] Epoch: 3, train loss: 0.3931441009044647, val loss: 0.2995024621486664, val metric: -0.2986217141151428\n",
      "[04:05:10] Epoch: 4, train loss: 0.37190163135528564, val loss: 0.28907328844070435, val metric: -0.28818437457084656\n",
      "[04:05:15] Epoch: 5, train loss: 0.3612576127052307, val loss: 0.2856505811214447, val metric: -0.2849845886230469\n",
      "[04:05:20] Epoch: 6, train loss: 0.34147849678993225, val loss: 0.29077383875846863, val metric: -0.2904459536075592\n",
      "[04:05:24] Epoch: 7, train loss: 0.32820913195610046, val loss: 0.28628265857696533, val metric: -0.28567007184028625\n",
      "[04:05:25] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[04:05:30] Epoch: 0, train loss: 0.9466603994369507, val loss: 0.8969156742095947, val metric: -0.8899596929550171\n",
      "[04:05:34] Epoch: 1, train loss: 0.6243693828582764, val loss: 0.5450395345687866, val metric: -0.5396217107772827\n",
      "[04:05:39] Epoch: 2, train loss: 0.41393470764160156, val loss: 0.4766153395175934, val metric: -0.47237110137939453\n",
      "[04:05:44] Epoch: 3, train loss: 0.3661634922027588, val loss: 0.43971073627471924, val metric: -0.43679529428482056\n",
      "[04:05:49] Epoch: 4, train loss: 0.34351569414138794, val loss: 0.41949498653411865, val metric: -0.41681599617004395\n",
      "[04:05:54] Epoch: 5, train loss: 0.3262133002281189, val loss: 0.4208153784275055, val metric: -0.4180387854576111\n",
      "[04:05:58] Epoch: 6, train loss: 0.31258511543273926, val loss: 0.4153962731361389, val metric: -0.41295650601387024\n",
      "[04:06:03] Epoch: 7, train loss: 0.30239710211753845, val loss: 0.40202319622039795, val metric: -0.40046781301498413\n",
      "[04:06:04] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[04:06:09] Epoch: 0, train loss: 0.9662126898765564, val loss: 0.8285723924636841, val metric: -0.8257289528846741\n",
      "[04:06:13] Epoch: 1, train loss: 0.6623834371566772, val loss: 0.5020843744277954, val metric: -0.503714382648468\n",
      "[04:06:18] Epoch: 2, train loss: 0.4255484938621521, val loss: 0.4120939373970032, val metric: -0.4153701663017273\n",
      "[04:06:23] Epoch: 3, train loss: 0.3851439654827118, val loss: 0.37617719173431396, val metric: -0.37957513332366943\n",
      "[04:06:28] Epoch: 4, train loss: 0.36129143834114075, val loss: 0.3604392409324646, val metric: -0.3635946214199066\n",
      "[04:06:32] Epoch: 5, train loss: 0.33819812536239624, val loss: 0.3562908172607422, val metric: -0.3594229519367218\n",
      "[04:06:37] Epoch: 6, train loss: 0.33131393790245056, val loss: 0.35254809260368347, val metric: -0.35615095496177673\n",
      "[04:06:42] Epoch: 7, train loss: 0.3125486969947815, val loss: 0.3500841557979584, val metric: -0.353730171918869\n",
      "[04:06:42] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[04:06:47] Epoch: 0, train loss: 0.9578157663345337, val loss: 0.869323194026947, val metric: -0.872725784778595\n",
      "[04:06:52] Epoch: 1, train loss: 0.650321900844574, val loss: 0.5066162347793579, val metric: -0.5065152049064636\n",
      "[04:06:57] Epoch: 2, train loss: 0.4186444878578186, val loss: 0.4331698715686798, val metric: -0.4321695566177368\n",
      "[04:07:02] Epoch: 3, train loss: 0.37075579166412354, val loss: 0.4037171006202698, val metric: -0.4030897617340088\n",
      "[04:07:06] Epoch: 4, train loss: 0.34079229831695557, val loss: 0.39677220582962036, val metric: -0.3962593972682953\n",
      "[04:07:11] Epoch: 5, train loss: 0.3338780105113983, val loss: 0.4085199236869812, val metric: -0.40813204646110535\n",
      "[04:07:16] Epoch: 6, train loss: 0.3255263566970825, val loss: 0.3931322693824768, val metric: -0.3928004205226898\n",
      "[04:07:21] Epoch: 7, train loss: 0.3051013946533203, val loss: 0.40667006373405457, val metric: -0.4062795341014862\n",
      "[04:07:21] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[04:07:26] Epoch: 0, train loss: 0.9638460874557495, val loss: 0.8005474805831909, val metric: -0.8044766187667847\n",
      "[04:07:31] Epoch: 1, train loss: 0.6484634876251221, val loss: 0.45334723591804504, val metric: -0.4542974531650543\n",
      "[04:07:36] Epoch: 2, train loss: 0.4207462668418884, val loss: 0.3868902325630188, val metric: -0.3866726756095886\n",
      "[04:07:40] Epoch: 3, train loss: 0.3782512843608856, val loss: 0.3721393346786499, val metric: -0.37222370505332947\n",
      "[04:07:45] Epoch: 4, train loss: 0.3625607490539551, val loss: 0.3510721027851105, val metric: -0.350647896528244\n",
      "[04:07:50] Epoch: 5, train loss: 0.33711522817611694, val loss: 0.3521192967891693, val metric: -0.3514361083507538\n",
      "[04:07:55] Epoch: 6, train loss: 0.3252981901168823, val loss: 0.3537415862083435, val metric: -0.35366347432136536\n",
      "[04:08:00] Epoch: 7, train loss: 0.32104724645614624, val loss: 0.34670189023017883, val metric: -0.34636783599853516\n",
      "[04:08:00] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m-0.3584736094013661\u001b[0m\n",
      "[04:08:00] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[04:08:00] Time left 9803.67 secs\n",
      "\n",
      "[04:08:00] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[04:08:00] \u001b[1mAutoml preset training completed in 196.34 seconds\u001b[0m\n",
      "\n",
      "[04:08:00] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "OOF_PREDS = np.zeros(len(train_feats))\n",
    "algo = \"mlp\"\n",
    "for RANDOM_STATE in range(42,43):\n",
    "    automl = TabularAutoML(\n",
    "        task = task, \n",
    "        timeout = TIMEOUT,\n",
    "        cpu_limit = N_THREADS,\n",
    "        general_params = {\"use_algos\": [[algo]]}, # ['nn', 'mlp', 'dense', 'denselight', 'resnet', 'snn', 'node', 'autoint', 'fttransformer'] or custom torch model\n",
    "        nn_params = {\n",
    "            \"n_epochs\": 8, \n",
    "            \"bs\": TRAIN_BS, \n",
    "            \"num_workers\": 0, \n",
    "            \"path_to_save\": None, \n",
    "            \"freeze_defaults\": True, \n",
    "            \"cont_embedder\": use_plr(USE_PLR), \n",
    "            \"hidden_size\": 32,\n",
    "            'clip_grad': True, \n",
    "            'clip_grad_params': {'max_norm': 1},\n",
    "            'snap_params': {'early_stopping':False,'swa':False},\n",
    "        },\n",
    "        nn_pipeline_params = {\"use_qnt\": USE_QNT, \"use_te\": False},\n",
    "        reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE, 'advanced_roles': ADVANCED_ROLES},\n",
    "    )\n",
    "    \n",
    "    OOF_PREDS += take_pred_from_task((automl.fit_predict(train_data=train_feats[cols],roles = {'target':'score'}, verbose = 3)).data, task)  /len(range(42,43))\n",
    "    joblib.dump(automl,f'MLP_{RANDOM_STATE}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb841bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:04.512180Z",
     "iopub.status.busy": "2024-01-05T04:08:04.511803Z",
     "iopub.status.idle": "2024-01-05T04:08:04.520139Z",
     "shell.execute_reply": "2024-01-05T04:08:04.519266Z"
    },
    "papermill": {
     "duration": 0.163487,
     "end_time": "2024-01-05T04:08:04.521935",
     "exception": false,
     "start_time": "2024-01-05T04:08:04.358448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598726656665098"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.mean_squared_error(train_feats['score'],OOF_PREDS,squared=False)   #10:0.6030168941831987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28d09e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:04.872646Z",
     "iopub.status.busy": "2024-01-05T04:08:04.871807Z",
     "iopub.status.idle": "2024-01-05T04:08:04.879906Z",
     "shell.execute_reply": "2024-01-05T04:08:04.879003Z"
    },
    "papermill": {
     "duration": 0.163127,
     "end_time": "2024-01-05T04:08:04.881779",
     "exception": false,
     "start_time": "2024-01-05T04:08:04.718652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cols = [col for col in train_feats.columns if col not in drop_cols+drop_cols_NN+target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b4d8d04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:05.187520Z",
     "iopub.status.busy": "2024-01-05T04:08:05.186911Z",
     "iopub.status.idle": "2024-01-05T04:08:12.510007Z",
     "shell.execute_reply": "2024-01-05T04:08:12.508946Z"
    },
    "papermill": {
     "duration": 7.478617,
     "end_time": "2024-01-05T04:08:12.512636",
     "exception": false,
     "start_time": "2024-01-05T04:08:05.034019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PREDS = np.zeros(len(test_feats))\n",
    "for i in range(42,43):\n",
    "    automl = joblib.load('/kaggle/working/MLP_{}.pkl'.format(i))\n",
    "    TEST_PREDS += automl.predict(test_feats[test_cols]).data[:, 0] / len(range(42,43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da341d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:12.823783Z",
     "iopub.status.busy": "2024-01-05T04:08:12.822936Z",
     "iopub.status.idle": "2024-01-05T04:08:12.837693Z",
     "shell.execute_reply": "2024-01-05T04:08:12.836881Z"
    },
    "papermill": {
     "duration": 0.16961,
     "end_time": "2024-01-05T04:08:12.839859",
     "exception": false,
     "start_time": "2024-01-05T04:08:12.670249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/3044189139.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_feats['score'] = TEST_PREDS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.480401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>0.606703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>0.533156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.480401\n",
       "1  2222bbbb  0.606703\n",
       "2  4444cccc  0.533156"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats['score'] = TEST_PREDS\n",
    "test_feats[['id', 'score']].to_csv(\"submission.csv\", index=False)\n",
    "test_feats[['id', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237fbd9",
   "metadata": {
    "papermill": {
     "duration": 0.151507,
     "end_time": "2024-01-05T04:08:13.144073",
     "exception": false,
     "start_time": "2024-01-05T04:08:12.992566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d6e58ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:13.451070Z",
     "iopub.status.busy": "2024-01-05T04:08:13.450367Z",
     "iopub.status.idle": "2024-01-05T04:08:13.477567Z",
     "shell.execute_reply": "2024-01-05T04:08:13.476808Z"
    },
    "papermill": {
     "duration": 0.182109,
     "end_time": "2024-01-05T04:08:13.479409",
     "exception": false,
     "start_time": "2024-01-05T04:08:13.297300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "has_inf_cols = []\n",
    "for i in cols:\n",
    "    has_inf = np.isinf(train_feats[i].values).any()\n",
    "    if has_inf:\n",
    "        print(f'feature{i} has inf')\n",
    "        has_inf_cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dc83b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T04:08:13.788265Z",
     "iopub.status.busy": "2024-01-05T04:08:13.787906Z",
     "iopub.status.idle": "2024-01-05T04:08:13.793725Z",
     "shell.execute_reply": "2024-01-05T04:08:13.792824Z"
    },
    "papermill": {
     "duration": 0.164236,
     "end_time": "2024-01-05T04:08:13.795545",
     "exception": false,
     "start_time": "2024-01-05T04:08:13.631309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_inf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646e1f9",
   "metadata": {
    "papermill": {
     "duration": 0.152309,
     "end_time": "2024-01-05T04:08:14.119264",
     "exception": false,
     "start_time": "2024-01-05T04:08:13.966955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8861a4b",
   "metadata": {
    "papermill": {
     "duration": 0.15236,
     "end_time": "2024-01-05T04:08:14.424689",
     "exception": false,
     "start_time": "2024-01-05T04:08:14.272329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    },
    {
     "datasetId": 3949123,
     "sourceId": 6973319,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 150384981,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154091050,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 154250178,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1120.98024,
   "end_time": "2024-01-05T04:08:17.313197",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-05T03:49:36.332957",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
